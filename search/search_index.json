{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GenLM Backend Documentation","text":"<p>GenLM Backend is a high-performance backend for language model probabilistic programs in the GenLM ecosystem. It provides essential tools and functions that serve as building blocks for more complex applications.</p> <p>Key Features:</p> <ul> <li>Asynchronous LLM Interfaces: Asynchronous computation of next-token probabilities with <code>vllm</code> and <code>transformer</code> language models.</li> <li>Tokenizer Vocabulary Decoding: Decoding Hugging Face tokenizer vocabularies into their byte and string representations.</li> <li>Token-Character Tries: Efficient conversion from token distributions to byte-level distributions using a trie datastructure.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<p>Clone the repository: <pre><code>git clone https://github.com/chisym/genlm-backend.git\ncd genlm_backend\n</code></pre> and install with pip: <pre><code>pip install .\n</code></pre> or install with development dependencies: <pre><code>pip install -e \".[test,docs]\"\n</code></pre></p>"},{"location":"#main-components","title":"Main Components","text":""},{"location":"#asynchronous-language-model-backends","title":"Asynchronous Language Model Backends","text":"<p>The <code>genlm_backend.llm</code> module provides asynchronous interfaces for computing next-token probabilities with <code>vllm</code> and <code>transformer</code> language models.</p> <pre><code>from genlm_backend.llm import AsyncVirtualLM\n# Initialize model with vLLM backend from a HuggingFace model name\nllm = AsyncVirtualLM.from_name(\"gpt2\")\n</code></pre> <p>These interfaces enable automatic batching of concurrent requests:</p> <p><pre><code>import time\nimport asyncio\n\nasync def my_model(i):\n    time.sleep(0.01) # Simulate CPU work.\n    # Get log probabilities of next tokens given token_ids.\n    return await llm.next_token_logprobs(token_ids=[i] * 10)\n\n# Both requests will be batched together by the underlying LM.\nouts = asyncio.run(asyncio.gather(*[my_model(0), my_model(1)]))\n</code></pre> as well as automatic output and KV caching, and CPU/GPU parallelization in certain scenarios.</p> <p>This submodule includes three key classes:</p> <ul> <li>AsyncVirtualLM (GPU): vLLM-based backend optimized for next-token probability computations. Fastest and most memory-efficient; requires a GPU. Uses vLLM's prefix caching feature for KV caching.</li> <li>AsyncTransformer (CPU): HuggingFace-based backend for next-token probability computations. Slower and less memory-efficient; for CPU usage. Uses custom KV caching.</li> <li>MockAsyncLM (Testing): Mock implementation for development and testing.</li> </ul> <p>See the LLM Code Reference for detailed API documentation.</p>"},{"location":"#token-character-tries","title":"Token-Character Tries","text":"<p>The <code>genlm_backend.trie</code> module provides an efficient trie data structure for mapping weight distributions over tokens to weight distributions over token prefixes.</p> <pre><code>from genlm_backend.trie import TokenCharacterTrie\n# Initialize TokenCharacterTrie from a byte vocabulary\ntrie = TokenCharacterTrie(decode=[b'cat', b'cats', b'dog', b'dogs'])\ntrie.visualize()\n</code></pre> <p></p> <p>Each node in the trie corresponds to a prefix of one or multiple tokens in the vocabulary. Internal nodes correspond to the incomplete prefixes and leaf nodes to complete tokens. The <code>weight_sum</code> function provides the marginal weight of each prefix (i.e., node) given a distribution on the underlying vocabulary:</p> <pre><code># Get mass at each node given a distribution over the vocab\nws = trie.weight_sum([0.4, 0.1, 0.3, 0.2])\ntrie.visualize(ws)\n</code></pre> <p></p> <p>This submodule includes three key classes:</p> <ul> <li>TokenCharacterTrie (CPU): Base implementation for CPU usage.</li> <li>ParallelTokenCharacterTrie (GPU): GPU-accelerated version which uses sparse matrix operations for mass summing.</li> <li>AsyncTokenCharacterTrie (Async): Asynchronous wrapper for use in asynchronous contexts; enables automatic batching of concurrent requests. This class can wrap either the sequential or parallel trie implementations.</li> </ul> <p>See the Trie Code Reference for detailed API documentation.</p>"},{"location":"#vocabulary-decoding","title":"Vocabulary Decoding","text":"<p>The <code>genlm_backend.tokenization</code> module converts Hugging Face tokenizer vocabularies into byte and string representations, with each token's representation stored at its corresponding token ID in the output lists.</p> <pre><code>from transformers import AutoTokenizer\nfrom genlm_backend.tokenization import decode_vocab\n\n# Load a tokenizer and decode its vocabulary\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nbyte_vocab, str_vocab = decode_vocab(tokenizer)\nbyte_vocab[10] # Byte representation of token with ID 10\n</code></pre> <p>Warning</p> <p>The byte representation (<code>byte_vocab</code>) is the canonical form and should be preferred for reliable token handling. The string representation (<code>str_vocab</code>) is provided for convenience and debugging but may not correctly represent all tokens, especially those containing invalid UTF-8 sequences.</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.10</li> <li>The core dependencies listed in the <code>setup.py</code> file of the repository.</li> </ul> <p>Note</p> <p>vLLM is not supported on macOS. On macOS systems, only CPU-based functionality (<code>AsyncTransformer</code>) will be available. GPU-accelerated features requiring vLLM (<code>AsyncVirtualLM</code>) will not work.</p>"},{"location":"#testing","title":"Testing","text":"<p>When test dependencies are installed, the test suite can be run via: <pre><code>pytest tests\n</code></pre></p>"},{"location":"#performance-benchmarking","title":"Performance Benchmarking","text":"<p>Performance benchmarks comparing different configurations can be found in our benchmarks directory.</p>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you are getting:     <pre><code>A module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy&lt;2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n</code></pre>     then you should downgrade your version of <code>numpy</code> with <code>pip install \"numpy&lt;2\"</code>.</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>genlm_backend<ul> <li>cache</li> <li>llm<ul> <li>base</li> <li>hf</li> <li>vllm</li> <li>vllm_reference</li> </ul> </li> <li>tokenization<ul> <li>bytes</li> <li>vocab</li> </ul> </li> <li>trie<ul> <li>async_impl</li> <li>base</li> <li>parallel</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/genlm_backend/__init__/","title":"genlm_backend","text":""},{"location":"reference/genlm_backend/cache/","title":"cache","text":""},{"location":"reference/genlm_backend/cache/#genlm_backend.cache.OutputCache","title":"<code>OutputCache</code>","text":"<p>A cache for storing tensor outputs with optional CPU offloading.</p> <p>This cache stores tensors along with their original devices and can optionally move tensors to CPU to save GPU memory. When retrieving tensors, they are moved back to their original device.</p> <p>Parameters:</p> Name Type Description Default <code>maxsize</code> <code>int</code> <p>Maximum number of items to store in the cache</p> required <code>move_to_cpu</code> <code>bool</code> <p>If True, tensors will be moved to CPU when cached</p> <code>False</code> Source code in <code>genlm_backend/cache.py</code> <pre><code>class OutputCache:\n    \"\"\"A cache for storing tensor outputs with optional CPU offloading.\n\n    This cache stores tensors along with their original devices and can optionally\n    move tensors to CPU to save GPU memory. When retrieving tensors, they are\n    moved back to their original device.\n\n    Args:\n        maxsize (int): Maximum number of items to store in the cache\n        move_to_cpu (bool): If True, tensors will be moved to CPU when cached\n    \"\"\"\n\n    def __init__(self, maxsize, move_to_cpu=False):\n        self.maxsize = maxsize\n        self.move_to_cpu = move_to_cpu\n        self.cache = OrderedDict()  # stores (device, tensor) tuples\n\n    def __getitem__(self, key):\n        if key in self.cache:\n            device, value = self.cache.pop(key)\n            self.cache[key] = (device, value)\n            return value.to(device) if self.move_to_cpu else value\n        raise KeyError(key)\n\n    def __setitem__(self, key, value):\n        if len(self.cache) &gt;= self.maxsize:\n            old_key, (_, old_tensor) = self.cache.popitem(last=False)\n            del old_tensor\n\n        self.cache[key] = (value.device, value.cpu() if self.move_to_cpu else value)\n\n    def __contains__(self, key):\n        return key in self.cache\n\n    def clear(self):\n        self.cache.clear()\n</code></pre>"},{"location":"reference/genlm_backend/cache/#genlm_backend.cache.TokenTrie","title":"<code>TokenTrie</code>","text":"<p>Class used internally to cache language model results.</p> <p>The TokenTrie maintains a tree of token sequences, storing logits and key-value states for each path.</p> Source code in <code>genlm_backend/cache.py</code> <pre><code>class TokenTrie:\n    \"\"\"Class used internally to cache language model results.\n\n    The TokenTrie maintains a tree of token sequences, storing logits and key-value\n    states for each path.\n    \"\"\"\n\n    # maybe TODO: Implement eviction policy\n\n    # Trie of tokens.\n\n    def __init__(self, parent=None, logprobs=None):\n        self.children = {}  # maps token ID to child\n        self.logprobs = logprobs  # for next token\n        self.past_key_values = None\n\n    def __repr__(self):\n        return (\n            f\"{'*' if self.past_key_values is not None else ''}[\"\n            + \", \".join(\n                [\n                    f\"{node_id}: {node.__repr__()}\"\n                    for (node_id, node) in self.children.items()\n                ]\n            )\n            + \"]\"\n        )\n\n    def clear_kv_cache(self):\n        self.past_key_values = None\n        for child, node in self.children.items():\n            node.clear_kv_cache()\n\n    def has_token(self, token_id):\n        return token_id in self.children\n\n    def get_token(self, token_id):\n        return self.children[token_id]\n\n    def add_token(self, token_id, logprobs=None):\n        self.children[token_id] = TokenTrie(self, logprobs)\n        return self.children[token_id]\n\n    def extend_cache(self, next_token_index, token_ids, logits, base):\n        node = self\n\n        for j in range(next_token_index, len(token_ids)):\n            token_id = token_ids[j]\n            token_logits = logits[j - base]\n            token_logprobs = torch.log_softmax(token_logits, 0)\n\n            node = node.add_token(token_id, token_logprobs.cpu())\n\n        return node\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/","title":"llm","text":""},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncLM","title":"<code>AsyncLM</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for asynchronous language models.</p> <p>This class provides an interface for language models that can generate token probabilities asynchronously. It handles tokenization and vocabulary management.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance compatible with the language model</p> required Source code in <code>genlm_backend/llm/base.py</code> <pre><code>class AsyncLM(ABC):\n    \"\"\"Abstract base class for asynchronous language models.\n\n    This class provides an interface for language models that can generate token probabilities\n    asynchronously. It handles tokenization and vocabulary management.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance compatible with the language model\n    \"\"\"\n\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.byte_vocab, self.str_vocab = decode_vocab(self.tokenizer)\n\n    @abstractmethod\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously.\n\n        Args:\n            token_ids (list[int]): A list of token IDs representing the prompt.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token synchronously.\n\n        Args:\n            token_ids (list[int]): A list of token IDs representing the prompt.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        pass\n\n    async def batch_next_token_logprobs(self, token_ids_list):\n        \"\"\"Batch request log probabilities for multiple token sequences asynchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists.\n\n        Returns:\n            (torch.Tensor): A tensor of log probability tensors.\n        \"\"\"\n        logprobs = await asyncio.gather(\n            *[self.next_token_logprobs(token_ids) for token_ids in token_ids_list]\n        )\n\n        return torch.stack(logprobs)\n\n    def batch_next_token_logprobs_sync(self, token_ids_list):\n        \"\"\"Batch request log probabilities for multiple token sequences synchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists.\n\n        Returns:\n            (torch.Tensor): A tensor of log probability tensors.\n        \"\"\"\n        return torch.stack(\n            [self.next_token_logprobs_sync(token_ids) for token_ids in token_ids_list]\n        )\n\n    def clear_cache(self):\n        \"\"\"Clear any caches used by the language model. No-op in base class.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncLM.batch_next_token_logprobs","title":"<code>batch_next_token_logprobs(token_ids_list)</code>  <code>async</code>","text":"<p>Batch request log probabilities for multiple token sequences asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of log probability tensors.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>async def batch_next_token_logprobs(self, token_ids_list):\n    \"\"\"Batch request log probabilities for multiple token sequences asynchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists.\n\n    Returns:\n        (torch.Tensor): A tensor of log probability tensors.\n    \"\"\"\n    logprobs = await asyncio.gather(\n        *[self.next_token_logprobs(token_ids) for token_ids in token_ids_list]\n    )\n\n    return torch.stack(logprobs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncLM.batch_next_token_logprobs_sync","title":"<code>batch_next_token_logprobs_sync(token_ids_list)</code>","text":"<p>Batch request log probabilities for multiple token sequences synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of log probability tensors.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>def batch_next_token_logprobs_sync(self, token_ids_list):\n    \"\"\"Batch request log probabilities for multiple token sequences synchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists.\n\n    Returns:\n        (torch.Tensor): A tensor of log probability tensors.\n    \"\"\"\n    return torch.stack(\n        [self.next_token_logprobs_sync(token_ids) for token_ids in token_ids_list]\n    )\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear any caches used by the language model. No-op in base class.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear any caches used by the language model. No-op in base class.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Request log probabilities of next token asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>A list of token IDs representing the prompt.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>@abstractmethod\nasync def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token asynchronously.\n\n    Args:\n        token_ids (list[int]): A list of token IDs representing the prompt.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>  <code>abstractmethod</code>","text":"<p>Request log probabilities of next token synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>A list of token IDs representing the prompt.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>@abstractmethod\ndef next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token synchronously.\n\n    Args:\n        token_ids (list[int]): A list of token IDs representing the prompt.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer","title":"<code>AsyncTransformer</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>Asynchronous wrapper around a HuggingFace causal language model with caching support.</p> <p>This class provides an asynchronous interface to HuggingFace language models with automatic batching and caching (output and KV) for improved efficiency.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>class AsyncTransformer(AsyncLM):\n    \"\"\"Asynchronous wrapper around a HuggingFace causal language model with caching support.\n\n    This class provides an asynchronous interface to HuggingFace language models with automatic batching\n    and caching (output and KV) for improved efficiency.\n    \"\"\"\n\n    @classmethod\n    def from_name(cls, model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs):\n        \"\"\"Create an AsyncTransformer instance from a pretrained HuggingFace model.\n\n        Args:\n            model_id (str): Model identifier in HuggingFace's model hub.\n            bitsandbytes_opts (dict, optional): Additional configuration options for bitsandbytes quantization.\n                Defaults to None.\n            hf_opts (dict, optional): Additional configuration options for loading the HuggingFace model.\n                Defaults to None.\n            **kwargs: Additional arguments passed to the `AsyncTransformer` constructor\n\n        Returns:\n            (AsyncTransformer): An initialized `AsyncTransformer` instance.\n        \"\"\"\n        if bitsandbytes_opts:\n            bnb_config = BitsAndBytesConfig(**bitsandbytes_opts)\n        else:\n            bnb_config = None\n\n        _hf_opts = {\n            \"device_map\": \"auto\",\n            \"torch_dtype\": \"auto\",\n        }\n        if hf_opts:\n            _hf_opts.update(hf_opts)\n\n        tok = AutoTokenizer.from_pretrained(model_id)\n        mod = AutoModelForCausalLM.from_pretrained(\n            model_id, quantization_config=bnb_config, **_hf_opts\n        )\n\n        return cls(mod, tok, **kwargs)\n\n    @torch.no_grad()\n    def __init__(self, hf_model, hf_tokenizer, batch_size=20, timeout=0.02):\n        \"\"\"Initialize an AsyncTransformer instance.\n\n        Args:\n            hf_model: A HuggingFace CausalLM model instance.\n            hf_tokenizer: A HuggingFace Tokenizer.\n            batch_size (int, optional): Maximum queries to process in one batch during auto-batching.\n                Defaults to 20.\n            timeout (float, optional): Seconds to wait since last query before processing current batch.\n                Defaults to 0.02.\n        \"\"\"\n        self.model = hf_model\n        self.tokenizer = hf_tokenizer\n        self.device = hf_model.device\n        self.cache = TokenTrie()\n\n        # Queries to be batched. Each query is a sequence of tokens,\n        # and a Future to be called when the query is resolved.\n        self.queries = []\n        self.batch_size = batch_size\n        self.timeout = timeout\n        self.timer = None\n\n        self.model.eval()\n\n        super().__init__(tokenizer=self.tokenizer)\n\n    def clear_cache(self):\n        \"\"\"Clear the cache of log probabilities and key/value pairs.\"\"\"\n        self.cache = TokenTrie(None, self.cache.logprobs)\n\n    def clear_kv_cache(self):\n        \"\"\"Clear any key and value vectors from the cache.\"\"\"\n        self.cache.clear_kv_cache()\n\n    def reset_async_queries(self):\n        \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n        to completion.\"\"\"\n        self.queries = []\n\n    @torch.no_grad()\n    def cache_kv(self, prompt_tokens):\n        \"\"\"Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.\n\n        Args:\n            prompt_tokens (list[int]): token ids for the prompt to cache.\n        \"\"\"\n        result = self.model(torch.tensor([prompt_tokens]).to(self.device))\n        node = self.cache.extend_cache(1, prompt_tokens, result.logits[0], 0)\n        node.past_key_values = result.past_key_values\n\n    @torch.no_grad()\n    def batch_evaluate_queries(self):\n        \"\"\"\n        Process a batch of queued language model queries.\n\n        This method is called internally when the `batch_size` has been met or the `timeout` has expired.\n        \"\"\"\n\n        queries, self.queries = self.queries, []\n        if len(queries) == 0:\n            return\n\n        query_groups = defaultdict(list)\n        for query in queries:\n            key = tuple(query.prompt)  # XXX: cache based on past_len too?\n            query_groups[key].append(query)\n\n        # Use one representative query from each group\n        unique_queries = [group[0] for group in query_groups.values()]\n\n        past_example = next((q.past for q in unique_queries if q.past), False)\n        max_past_length = max(q.past_len for q in unique_queries)\n        max_query_length = max(len(q.prompt) for q in unique_queries)\n\n        padding_token_id = (\n            self.tokenizer.pad_token_id\n            if self.tokenizer.pad_token_id is not None\n            else 0\n        )\n\n        input_ids = torch.tensor(\n            [\n                q.prompt_padded(padding_token_id, max_query_length)\n                for q in unique_queries\n            ]\n        ).to(self.device)\n        attn_masks = torch.tensor(\n            [\n                q.attention_mask(max_past_length, max_query_length)\n                for q in unique_queries\n            ]\n        ).to(self.device)\n        posn_ids = torch.tensor(\n            [q.position_ids(max_past_length, max_query_length) for q in unique_queries]\n        ).to(self.device)\n        if past_example:\n            pasts = [\n                [\n                    torch.cat(\n                        (\n                            *(\n                                q.past_padded(\n                                    layer,\n                                    j,\n                                    max_past_length,\n                                    past_example[0][0].dtype,\n                                    self.device,\n                                    past_example[0][0].shape,\n                                )\n                                for q in unique_queries\n                            ),\n                        ),\n                        dim=0,\n                    )\n                    for j in range(2)\n                ]\n                for layer in range(len(past_example))\n            ]\n        else:\n            pasts = None\n\n        results = self.model(\n            input_ids,\n            attention_mask=attn_masks,\n            position_ids=posn_ids,\n            past_key_values=pasts,\n            use_cache=pasts is not None,\n        )\n\n        assert len(results.logits) == len(unique_queries)\n\n        for i, q in enumerate(unique_queries):\n            result = results.logits[i]\n            for dup_query in query_groups[tuple(q.prompt)]:\n                dup_query.future.set_result(result)\n\n    @torch.no_grad()\n    def add_query(self, query, future, past):\n        \"\"\"Add a query to be evaluated in the next batch.\n\n        This method is called internally when a `next_token_logprobs` request is made.\n\n        Args:\n            query (list[int]): Token IDs representing the query prompt\n            future (asyncio.Future): Future to store the result in\n            past (list[tuple[torch.Tensor]]|None): Past key/value states from previous evaluation,\n                or None if this is a new query\n        \"\"\"\n        self.queries.append(Query(query, future, past))\n\n        if self.timer:\n            self.timer.cancel()\n            self.timer = None\n        if len(self.queries) &gt;= self.batch_size:\n            self.batch_evaluate_queries()\n        else:\n            self.timer = asyncio.get_running_loop().call_later(\n                self.timeout, lambda: self.batch_evaluate_queries()\n            )\n\n    def walk_cache(self, token_ids):\n        \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n        Args:\n            token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n        Returns:\n            tuple:\n                - CacheNode: The deepest node in the cache tree that matches the token sequence\n                - int: Number of tokens matched from the start of token_ids\n                - list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,\n                    or None if no cached states were found\n                - int: Base index indicating where the past states start in token_ids\n        \"\"\"\n        # Walk while tokens can be found\n        node = self.cache\n        next_token_index = 0\n\n        past = None\n        base = 0\n        while next_token_index &lt; len(token_ids):\n            if node.past_key_values is not None:\n                past = node.past_key_values\n                base = next_token_index\n            if node.has_token(token_ids[next_token_index]):\n                node = node.get_token(token_ids[next_token_index])\n                next_token_index += 1\n            else:\n                break\n\n        return node, next_token_index, past, base\n\n    @torch.no_grad()\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        node, next_token_index, past, base = self.walk_cache(token_ids)\n\n        # If we processed all tokens, then we're done.\n        if next_token_index == len(token_ids):\n            return node.logprobs\n\n        # Create a future with the prompt\n        future = asyncio.get_running_loop().create_future()\n        self.add_query(token_ids[base:], future, past)\n        logits = await future\n\n        # Create new nodes\n        node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n        return node.logprobs\n\n    @torch.no_grad()\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        # Walk while tokens can be found\n        node, next_token_index, past, base = self.walk_cache(token_ids)\n\n        if next_token_index == len(token_ids):\n            return node.logprobs\n\n        logits = self.model(\n            torch.tensor([token_ids[base:]]).to(self.device),\n            past_key_values=node.past_key_values,\n            use_cache=node.past_key_values is not None,\n        ).logits[0]\n\n        node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n        return node.logprobs\n\n    def next_token_logprobs_uncached(self, token_ids):\n        \"\"\"Request log probabilities of next token. No KV or output caching, and does not support auto-batching.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        with torch.no_grad():\n            logits = self.model(\n                torch.tensor([token_ids]).to(self.device),\n                past_key_values=None,\n                use_cache=False,\n            ).logits[0]\n            return torch.log_softmax(logits[-1], dim=0)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer.__init__","title":"<code>__init__(hf_model, hf_tokenizer, batch_size=20, timeout=0.02)</code>","text":"<p>Initialize an AsyncTransformer instance.</p> <p>Parameters:</p> Name Type Description Default <code>hf_model</code> <p>A HuggingFace CausalLM model instance.</p> required <code>hf_tokenizer</code> <p>A HuggingFace Tokenizer.</p> required <code>batch_size</code> <code>int</code> <p>Maximum queries to process in one batch during auto-batching. Defaults to 20.</p> <code>20</code> <code>timeout</code> <code>float</code> <p>Seconds to wait since last query before processing current batch. Defaults to 0.02.</p> <code>0.02</code> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef __init__(self, hf_model, hf_tokenizer, batch_size=20, timeout=0.02):\n    \"\"\"Initialize an AsyncTransformer instance.\n\n    Args:\n        hf_model: A HuggingFace CausalLM model instance.\n        hf_tokenizer: A HuggingFace Tokenizer.\n        batch_size (int, optional): Maximum queries to process in one batch during auto-batching.\n            Defaults to 20.\n        timeout (float, optional): Seconds to wait since last query before processing current batch.\n            Defaults to 0.02.\n    \"\"\"\n    self.model = hf_model\n    self.tokenizer = hf_tokenizer\n    self.device = hf_model.device\n    self.cache = TokenTrie()\n\n    # Queries to be batched. Each query is a sequence of tokens,\n    # and a Future to be called when the query is resolved.\n    self.queries = []\n    self.batch_size = batch_size\n    self.timeout = timeout\n    self.timer = None\n\n    self.model.eval()\n\n    super().__init__(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer.add_query","title":"<code>add_query(query, future, past)</code>","text":"<p>Add a query to be evaluated in the next batch.</p> <p>This method is called internally when a <code>next_token_logprobs</code> request is made.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[int]</code> <p>Token IDs representing the query prompt</p> required <code>future</code> <code>Future</code> <p>Future to store the result in</p> required <code>past</code> <code>list[tuple[Tensor]] | None</code> <p>Past key/value states from previous evaluation, or None if this is a new query</p> required Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef add_query(self, query, future, past):\n    \"\"\"Add a query to be evaluated in the next batch.\n\n    This method is called internally when a `next_token_logprobs` request is made.\n\n    Args:\n        query (list[int]): Token IDs representing the query prompt\n        future (asyncio.Future): Future to store the result in\n        past (list[tuple[torch.Tensor]]|None): Past key/value states from previous evaluation,\n            or None if this is a new query\n    \"\"\"\n    self.queries.append(Query(query, future, past))\n\n    if self.timer:\n        self.timer.cancel()\n        self.timer = None\n    if len(self.queries) &gt;= self.batch_size:\n        self.batch_evaluate_queries()\n    else:\n        self.timer = asyncio.get_running_loop().call_later(\n            self.timeout, lambda: self.batch_evaluate_queries()\n        )\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer.batch_evaluate_queries","title":"<code>batch_evaluate_queries()</code>","text":"<p>Process a batch of queued language model queries.</p> <p>This method is called internally when the <code>batch_size</code> has been met or the <code>timeout</code> has expired.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef batch_evaluate_queries(self):\n    \"\"\"\n    Process a batch of queued language model queries.\n\n    This method is called internally when the `batch_size` has been met or the `timeout` has expired.\n    \"\"\"\n\n    queries, self.queries = self.queries, []\n    if len(queries) == 0:\n        return\n\n    query_groups = defaultdict(list)\n    for query in queries:\n        key = tuple(query.prompt)  # XXX: cache based on past_len too?\n        query_groups[key].append(query)\n\n    # Use one representative query from each group\n    unique_queries = [group[0] for group in query_groups.values()]\n\n    past_example = next((q.past for q in unique_queries if q.past), False)\n    max_past_length = max(q.past_len for q in unique_queries)\n    max_query_length = max(len(q.prompt) for q in unique_queries)\n\n    padding_token_id = (\n        self.tokenizer.pad_token_id\n        if self.tokenizer.pad_token_id is not None\n        else 0\n    )\n\n    input_ids = torch.tensor(\n        [\n            q.prompt_padded(padding_token_id, max_query_length)\n            for q in unique_queries\n        ]\n    ).to(self.device)\n    attn_masks = torch.tensor(\n        [\n            q.attention_mask(max_past_length, max_query_length)\n            for q in unique_queries\n        ]\n    ).to(self.device)\n    posn_ids = torch.tensor(\n        [q.position_ids(max_past_length, max_query_length) for q in unique_queries]\n    ).to(self.device)\n    if past_example:\n        pasts = [\n            [\n                torch.cat(\n                    (\n                        *(\n                            q.past_padded(\n                                layer,\n                                j,\n                                max_past_length,\n                                past_example[0][0].dtype,\n                                self.device,\n                                past_example[0][0].shape,\n                            )\n                            for q in unique_queries\n                        ),\n                    ),\n                    dim=0,\n                )\n                for j in range(2)\n            ]\n            for layer in range(len(past_example))\n        ]\n    else:\n        pasts = None\n\n    results = self.model(\n        input_ids,\n        attention_mask=attn_masks,\n        position_ids=posn_ids,\n        past_key_values=pasts,\n        use_cache=pasts is not None,\n    )\n\n    assert len(results.logits) == len(unique_queries)\n\n    for i, q in enumerate(unique_queries):\n        result = results.logits[i]\n        for dup_query in query_groups[tuple(q.prompt)]:\n            dup_query.future.set_result(result)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer.cache_kv","title":"<code>cache_kv(prompt_tokens)</code>","text":"<p>Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_tokens</code> <code>list[int]</code> <p>token ids for the prompt to cache.</p> required Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef cache_kv(self, prompt_tokens):\n    \"\"\"Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.\n\n    Args:\n        prompt_tokens (list[int]): token ids for the prompt to cache.\n    \"\"\"\n    result = self.model(torch.tensor([prompt_tokens]).to(self.device))\n    node = self.cache.extend_cache(1, prompt_tokens, result.logits[0], 0)\n    node.past_key_values = result.past_key_values\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the cache of log probabilities and key/value pairs.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the cache of log probabilities and key/value pairs.\"\"\"\n    self.cache = TokenTrie(None, self.cache.logprobs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer.clear_kv_cache","title":"<code>clear_kv_cache()</code>","text":"<p>Clear any key and value vectors from the cache.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>def clear_kv_cache(self):\n    \"\"\"Clear any key and value vectors from the cache.\"\"\"\n    self.cache.clear_kv_cache()\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer.from_name","title":"<code>from_name(model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an AsyncTransformer instance from a pretrained HuggingFace model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Model identifier in HuggingFace's model hub.</p> required <code>bitsandbytes_opts</code> <code>dict</code> <p>Additional configuration options for bitsandbytes quantization. Defaults to None.</p> <code>None</code> <code>hf_opts</code> <code>dict</code> <p>Additional configuration options for loading the HuggingFace model. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the <code>AsyncTransformer</code> constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTransformer</code> <p>An initialized <code>AsyncTransformer</code> instance.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@classmethod\ndef from_name(cls, model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs):\n    \"\"\"Create an AsyncTransformer instance from a pretrained HuggingFace model.\n\n    Args:\n        model_id (str): Model identifier in HuggingFace's model hub.\n        bitsandbytes_opts (dict, optional): Additional configuration options for bitsandbytes quantization.\n            Defaults to None.\n        hf_opts (dict, optional): Additional configuration options for loading the HuggingFace model.\n            Defaults to None.\n        **kwargs: Additional arguments passed to the `AsyncTransformer` constructor\n\n    Returns:\n        (AsyncTransformer): An initialized `AsyncTransformer` instance.\n    \"\"\"\n    if bitsandbytes_opts:\n        bnb_config = BitsAndBytesConfig(**bitsandbytes_opts)\n    else:\n        bnb_config = None\n\n    _hf_opts = {\n        \"device_map\": \"auto\",\n        \"torch_dtype\": \"auto\",\n    }\n    if hf_opts:\n        _hf_opts.update(hf_opts)\n\n    tok = AutoTokenizer.from_pretrained(model_id)\n    mod = AutoModelForCausalLM.from_pretrained(\n        model_id, quantization_config=bnb_config, **_hf_opts\n    )\n\n    return cls(mod, tok, **kwargs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with <code>await</code>.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@torch.no_grad()\nasync def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    node, next_token_index, past, base = self.walk_cache(token_ids)\n\n    # If we processed all tokens, then we're done.\n    if next_token_index == len(token_ids):\n        return node.logprobs\n\n    # Create a future with the prompt\n    future = asyncio.get_running_loop().create_future()\n    self.add_query(token_ids[base:], future, past)\n    logits = await future\n\n    # Create new nodes\n    node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n    return node.logprobs\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Request log probabilities of next token. Not asynchronous, and does not support auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    # Walk while tokens can be found\n    node, next_token_index, past, base = self.walk_cache(token_ids)\n\n    if next_token_index == len(token_ids):\n        return node.logprobs\n\n    logits = self.model(\n        torch.tensor([token_ids[base:]]).to(self.device),\n        past_key_values=node.past_key_values,\n        use_cache=node.past_key_values is not None,\n    ).logits[0]\n\n    node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n    return node.logprobs\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer.next_token_logprobs_uncached","title":"<code>next_token_logprobs_uncached(token_ids)</code>","text":"<p>Request log probabilities of next token. No KV or output caching, and does not support auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>def next_token_logprobs_uncached(self, token_ids):\n    \"\"\"Request log probabilities of next token. No KV or output caching, and does not support auto-batching.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    with torch.no_grad():\n        logits = self.model(\n            torch.tensor([token_ids]).to(self.device),\n            past_key_values=None,\n            use_cache=False,\n        ).logits[0]\n        return torch.log_softmax(logits[-1], dim=0)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer.reset_async_queries","title":"<code>reset_async_queries()</code>","text":"<p>Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing to completion.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>def reset_async_queries(self):\n    \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n    to completion.\"\"\"\n    self.queries = []\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncTransformer.walk_cache","title":"<code>walk_cache(token_ids)</code>","text":"<p>Walk the cache tree to find the deepest node matching a sequence of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Sequence of token IDs to follow in the cache tree</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <ul> <li>CacheNode: The deepest node in the cache tree that matches the token sequence</li> <li>int: Number of tokens matched from the start of token_ids</li> <li>list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,     or None if no cached states were found</li> <li>int: Base index indicating where the past states start in token_ids</li> </ul> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>def walk_cache(self, token_ids):\n    \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n    Args:\n        token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n    Returns:\n        tuple:\n            - CacheNode: The deepest node in the cache tree that matches the token sequence\n            - int: Number of tokens matched from the start of token_ids\n            - list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,\n                or None if no cached states were found\n            - int: Base index indicating where the past states start in token_ids\n    \"\"\"\n    # Walk while tokens can be found\n    node = self.cache\n    next_token_index = 0\n\n    past = None\n    base = 0\n    while next_token_index &lt; len(token_ids):\n        if node.past_key_values is not None:\n            past = node.past_key_values\n            base = next_token_index\n        if node.has_token(token_ids[next_token_index]):\n            node = node.get_token(token_ids[next_token_index])\n            next_token_index += 1\n        else:\n            break\n\n    return node, next_token_index, past, base\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncVirtualLM","title":"<code>AsyncVirtualLM</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>A wrapper around vLLM's <code>AsyncLLMEngine</code> for asynchronous next token log probability computations.</p> <p>This class provides an asynchronous interface for computing log probabilities using vLLM's engine. It is optimized for next token log probability computations and supports caching of results (outputs and KV).</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>class AsyncVirtualLM(AsyncLM):\n    \"\"\"A wrapper around vLLM's `AsyncLLMEngine` for asynchronous next token log probability computations.\n\n    This class provides an asynchronous interface for computing log probabilities using vLLM's engine.\n    It is optimized for next token log probability computations and supports caching of results (outputs and KV).\n    \"\"\"\n\n    default_params = SamplingParams(\n        max_tokens=1, n=1, logprobs=1, detokenize=False, stop=None, ignore_eos=True\n    )\n\n    def __init__(self, async_llm_engine, cache_size=0, cache_opts={}):\n        \"\"\"Initialize an `AsyncVirtualLM` instance.\n\n        Args:\n            async_llm_engine (AsyncLLMEngine): The async vLLM engine instance.\n            cache_size (int, optional): Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.\n            cache_opts (dict, optional): Additional options to pass to the [`OutputCache`][genlm_backend.cache.OutputCache] constructor. Defaults to {}.\n\n        Note:\n            The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.\n        \"\"\"\n        self.async_llm_engine = async_llm_engine\n        self.tokenizer = async_llm_engine.engine.get_tokenizer()\n        self.request_counter = Counter()\n        self.custom_sampler = DeferredSampler()\n        self.cache = (\n            OutputCache(maxsize=cache_size, **cache_opts)\n            if cache_size &gt; 0\n            else None\n        )\n\n        async_llm_engine.engine.log_stats = False\n\n        super().__init__(tokenizer=self.tokenizer)\n\n    @classmethod\n    def from_name(cls, model_name, engine_opts=None, **kwargs):\n        \"\"\"Create a `AsyncVirtualLM` instance from a model name.\n\n        Args:\n            model_name (str): Name of the model to load.\n            engine_opts (dict): Additional options to pass to the `AsyncLLMEngine`. The engine will be\n                configured with prefix caching enabled and async output processing disabled by default.\n            **kwargs: Additional arguments passed to `AsyncVirtualLM` constructor.\n\n        Returns:\n            (AsyncVirtualLM): An `AsyncVirtualLM` instance.\n        \"\"\"\n        if not HAS_VLLM:\n            raise ImportError(\n                \"vLLM not available. Install vLLM or use AsyncTransformer instead.\"\n            )\n\n        engine_opts = {\n            \"enable_prefix_caching\": True,\n            \"disable_log_requests\": True,\n            \"disable_async_output_proc\": True,\n            **(engine_opts or {}),\n        }\n\n        engine = AsyncLLMEngine.from_engine_args(\n            AsyncEngineArgs(model=model_name, tokenizer=model_name, **engine_opts)\n        )\n\n        return cls(engine, **kwargs)\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously with output caching.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            result (torch.Tensor): Normalized log probability tensor.\n\n        Warning:\n            Do not use `asyncio.run(next_token_logprobs())` as it may interfere with vLLM's background loop.\n            For synchronous usage, use the `next_token_logprobs_sync()` method instead.\n        \"\"\"\n        key = tuple(token_ids)\n\n        if self.cache is not None and key in self.cache:\n            return self.cache[key]\n\n        result = await self._next_token_logprobs(key)\n\n        if self.cache is not None:\n            self.cache[key] = result\n\n        return result\n\n    async def _next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        req_id = str(next(self.request_counter))\n        prompt = TokensPrompt(prompt_token_ids=token_ids)\n\n        outputs = []\n        with self._optimized_sampling_context():\n            async for output in self.async_llm_engine.generate(\n                prompt=prompt,\n                sampling_params=self.default_params,\n                request_id=req_id,\n            ):\n                if output.finished:\n                    outputs.append(output)\n\n        return self._validate_outputs(outputs)\n\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token synchronously.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self.batch_next_token_logprobs_sync([token_ids])[0]\n\n    def batch_next_token_logprobs_sync(self, token_ids_list):\n        \"\"\"\n        Request log probabilities of next tokens in a batch synchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists, each representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): A tensor of normalized log probability tensors, one for each prompt in the input list.\n        \"\"\"\n        req_ids = []\n        for token_ids in token_ids_list:\n            req_id = str(next(self.request_counter))\n            req_ids.append(req_id)\n            self.async_llm_engine.engine.add_request(\n                prompt=TokensPrompt(prompt_token_ids=token_ids),\n                params=self.default_params,\n                request_id=req_id,\n            )\n\n        req_id2outputs = {}\n        with self._optimized_sampling_context():\n            while self.async_llm_engine.engine.has_unfinished_requests():\n                output = self.async_llm_engine.engine.step()\n                for out in output:\n                    if out.finished:\n                        assert out.request_id not in req_id2outputs, (\n                            f\"Duplicate outputs for request {out.request_id}\"\n                        )\n                        assert out.request_id in req_ids, (\n                            f\"{out.request_id} not in requested IDs\"\n                        )\n                        req_id2outputs[out.request_id] = out\n\n        logprobs = [\n            self._validate_outputs([req_id2outputs[req_id]]) for req_id in req_ids\n        ]\n\n        return torch.stack(logprobs)\n\n    @contextmanager\n    def _optimized_sampling_context(self):\n        \"\"\"Context manager for optimized sampling configuration.\"\"\"\n        model = self.async_llm_engine.engine.model_executor.driver_worker.model_runner.model\n        original_sampler = model.sampler\n        try:\n            model.sampler = self.custom_sampler\n            yield\n        finally:\n            model.sampler = original_sampler\n\n    def _validate_outputs(self, outputs):\n        \"\"\"Validate and extract logprobs from a vLLM output.\n\n        Args:\n            outputs: List of sequence group outputs from vLLM generation\n\n        Returns:\n            Tensor of log probabilities for the next token\n\n        Raises:\n            AssertionError: If output structure doesn't match expected format\n        \"\"\"\n        assert len(outputs) == 1, \"Expected exactly one sequence group\"\n        seq_group = outputs[0]\n\n        assert len(seq_group.outputs) == 1, (\n            \"Expected exactly one sequence in output\"\n        )\n        sequence = seq_group.outputs[0]\n\n        assert len(sequence.logprobs) == 1, \"Expected exactly one set of logprobs\"\n        token_logprobs = sequence.logprobs[0].logprobs\n\n        return token_logprobs\n\n    def clear_cache(self):\n        \"\"\"Clear output cache.\"\"\"\n        if self.cache:\n            self.cache.clear()\n\n    def __del__(self):\n        \"\"\"Clean up resources on deletion.\"\"\"\n        self._cleanup_engine()\n\n    def _cleanup_engine(self):\n        \"\"\"Clean up the vLLM engine and associated resources.\"\"\"\n        if async_engine := getattr(self, \"async_llm_engine\", None):\n            async_engine.shutdown_background_loop()\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncVirtualLM.__del__","title":"<code>__del__()</code>","text":"<p>Clean up resources on deletion.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def __del__(self):\n    \"\"\"Clean up resources on deletion.\"\"\"\n    self._cleanup_engine()\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncVirtualLM.__init__","title":"<code>__init__(async_llm_engine, cache_size=0, cache_opts={})</code>","text":"<p>Initialize an <code>AsyncVirtualLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>async_llm_engine</code> <code>AsyncLLMEngine</code> <p>The async vLLM engine instance.</p> required <code>cache_size</code> <code>int</code> <p>Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.</p> <code>0</code> <code>cache_opts</code> <code>dict</code> <p>Additional options to pass to the <code>OutputCache</code> constructor. Defaults to {}.</p> <code>{}</code> Note <p>The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def __init__(self, async_llm_engine, cache_size=0, cache_opts={}):\n    \"\"\"Initialize an `AsyncVirtualLM` instance.\n\n    Args:\n        async_llm_engine (AsyncLLMEngine): The async vLLM engine instance.\n        cache_size (int, optional): Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.\n        cache_opts (dict, optional): Additional options to pass to the [`OutputCache`][genlm_backend.cache.OutputCache] constructor. Defaults to {}.\n\n    Note:\n        The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.\n    \"\"\"\n    self.async_llm_engine = async_llm_engine\n    self.tokenizer = async_llm_engine.engine.get_tokenizer()\n    self.request_counter = Counter()\n    self.custom_sampler = DeferredSampler()\n    self.cache = (\n        OutputCache(maxsize=cache_size, **cache_opts)\n        if cache_size &gt; 0\n        else None\n    )\n\n    async_llm_engine.engine.log_stats = False\n\n    super().__init__(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncVirtualLM._cleanup_engine","title":"<code>_cleanup_engine()</code>","text":"<p>Clean up the vLLM engine and associated resources.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def _cleanup_engine(self):\n    \"\"\"Clean up the vLLM engine and associated resources.\"\"\"\n    if async_engine := getattr(self, \"async_llm_engine\", None):\n        async_engine.shutdown_background_loop()\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncVirtualLM._next_token_logprobs","title":"<code>_next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>async def _next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token asynchronously.\n\n    Args:\n        token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    req_id = str(next(self.request_counter))\n    prompt = TokensPrompt(prompt_token_ids=token_ids)\n\n    outputs = []\n    with self._optimized_sampling_context():\n        async for output in self.async_llm_engine.generate(\n            prompt=prompt,\n            sampling_params=self.default_params,\n            request_id=req_id,\n        ):\n            if output.finished:\n                outputs.append(output)\n\n    return self._validate_outputs(outputs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncVirtualLM._optimized_sampling_context","title":"<code>_optimized_sampling_context()</code>","text":"<p>Context manager for optimized sampling configuration.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>@contextmanager\ndef _optimized_sampling_context(self):\n    \"\"\"Context manager for optimized sampling configuration.\"\"\"\n    model = self.async_llm_engine.engine.model_executor.driver_worker.model_runner.model\n    original_sampler = model.sampler\n    try:\n        model.sampler = self.custom_sampler\n        yield\n    finally:\n        model.sampler = original_sampler\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncVirtualLM._validate_outputs","title":"<code>_validate_outputs(outputs)</code>","text":"<p>Validate and extract logprobs from a vLLM output.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <p>List of sequence group outputs from vLLM generation</p> required <p>Returns:</p> Type Description <p>Tensor of log probabilities for the next token</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If output structure doesn't match expected format</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def _validate_outputs(self, outputs):\n    \"\"\"Validate and extract logprobs from a vLLM output.\n\n    Args:\n        outputs: List of sequence group outputs from vLLM generation\n\n    Returns:\n        Tensor of log probabilities for the next token\n\n    Raises:\n        AssertionError: If output structure doesn't match expected format\n    \"\"\"\n    assert len(outputs) == 1, \"Expected exactly one sequence group\"\n    seq_group = outputs[0]\n\n    assert len(seq_group.outputs) == 1, (\n        \"Expected exactly one sequence in output\"\n    )\n    sequence = seq_group.outputs[0]\n\n    assert len(sequence.logprobs) == 1, \"Expected exactly one set of logprobs\"\n    token_logprobs = sequence.logprobs[0].logprobs\n\n    return token_logprobs\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncVirtualLM.batch_next_token_logprobs_sync","title":"<code>batch_next_token_logprobs_sync(token_ids_list)</code>","text":"<p>Request log probabilities of next tokens in a batch synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists, each representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of normalized log probability tensors, one for each prompt in the input list.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def batch_next_token_logprobs_sync(self, token_ids_list):\n    \"\"\"\n    Request log probabilities of next tokens in a batch synchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists, each representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): A tensor of normalized log probability tensors, one for each prompt in the input list.\n    \"\"\"\n    req_ids = []\n    for token_ids in token_ids_list:\n        req_id = str(next(self.request_counter))\n        req_ids.append(req_id)\n        self.async_llm_engine.engine.add_request(\n            prompt=TokensPrompt(prompt_token_ids=token_ids),\n            params=self.default_params,\n            request_id=req_id,\n        )\n\n    req_id2outputs = {}\n    with self._optimized_sampling_context():\n        while self.async_llm_engine.engine.has_unfinished_requests():\n            output = self.async_llm_engine.engine.step()\n            for out in output:\n                if out.finished:\n                    assert out.request_id not in req_id2outputs, (\n                        f\"Duplicate outputs for request {out.request_id}\"\n                    )\n                    assert out.request_id in req_ids, (\n                        f\"{out.request_id} not in requested IDs\"\n                    )\n                    req_id2outputs[out.request_id] = out\n\n    logprobs = [\n        self._validate_outputs([req_id2outputs[req_id]]) for req_id in req_ids\n    ]\n\n    return torch.stack(logprobs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncVirtualLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear output cache.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear output cache.\"\"\"\n    if self.cache:\n        self.cache.clear()\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncVirtualLM.from_name","title":"<code>from_name(model_name, engine_opts=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a <code>AsyncVirtualLM</code> instance from a model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to load.</p> required <code>engine_opts</code> <code>dict</code> <p>Additional options to pass to the <code>AsyncLLMEngine</code>. The engine will be configured with prefix caching enabled and async output processing disabled by default.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to <code>AsyncVirtualLM</code> constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncVirtualLM</code> <p>An <code>AsyncVirtualLM</code> instance.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>@classmethod\ndef from_name(cls, model_name, engine_opts=None, **kwargs):\n    \"\"\"Create a `AsyncVirtualLM` instance from a model name.\n\n    Args:\n        model_name (str): Name of the model to load.\n        engine_opts (dict): Additional options to pass to the `AsyncLLMEngine`. The engine will be\n            configured with prefix caching enabled and async output processing disabled by default.\n        **kwargs: Additional arguments passed to `AsyncVirtualLM` constructor.\n\n    Returns:\n        (AsyncVirtualLM): An `AsyncVirtualLM` instance.\n    \"\"\"\n    if not HAS_VLLM:\n        raise ImportError(\n            \"vLLM not available. Install vLLM or use AsyncTransformer instead.\"\n        )\n\n    engine_opts = {\n        \"enable_prefix_caching\": True,\n        \"disable_log_requests\": True,\n        \"disable_async_output_proc\": True,\n        **(engine_opts or {}),\n    }\n\n    engine = AsyncLLMEngine.from_engine_args(\n        AsyncEngineArgs(model=model_name, tokenizer=model_name, **engine_opts)\n    )\n\n    return cls(engine, **kwargs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncVirtualLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token asynchronously with output caching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>Tensor</code> <p>Normalized log probability tensor.</p> Warning <p>Do not use <code>asyncio.run(next_token_logprobs())</code> as it may interfere with vLLM's background loop. For synchronous usage, use the <code>next_token_logprobs_sync()</code> method instead.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token asynchronously with output caching.\n\n    Args:\n        token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        result (torch.Tensor): Normalized log probability tensor.\n\n    Warning:\n        Do not use `asyncio.run(next_token_logprobs())` as it may interfere with vLLM's background loop.\n        For synchronous usage, use the `next_token_logprobs_sync()` method instead.\n    \"\"\"\n    key = tuple(token_ids)\n\n    if self.cache is not None and key in self.cache:\n        return self.cache[key]\n\n    result = await self._next_token_logprobs(key)\n\n    if self.cache is not None:\n        self.cache[key] = result\n\n    return result\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.AsyncVirtualLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Request log probabilities of next token synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token synchronously.\n\n    Args:\n        token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self.batch_next_token_logprobs_sync([token_ids])[0]\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.MockAsyncLM","title":"<code>MockAsyncLM</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>Mock implementation of AsyncLM used for testing.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>class MockAsyncLM(AsyncLM):\n    \"\"\"Mock implementation of AsyncLM used for testing.\"\"\"\n\n    def __init__(self, tokenizer):\n        \"\"\"Initialize a `MockAsyncLM` instance.\n\n        Args:\n            tokenizer: Hugging Face tokenizer instance\n        \"\"\"\n        super().__init__(tokenizer)\n        self._rng = np.random.RandomState(42)\n\n    @classmethod\n    def from_name(cls, model_name, **kwargs):\n        \"\"\"Create a MockAsyncLM instance over the vocabulary of the model's tokenizer.\n\n        Args:\n            model_name (str): Name of pretrained model to load tokenizer from\n            **kwargs: Additional arguments passed to `MockAsyncLM` constructor\n\n        Returns:\n            (MockAsyncLM): `MockAsyncLM` instance initialized with tokenizer from `model_name`\n        \"\"\"\n        from transformers import AutoTokenizer\n\n        return cls(AutoTokenizer.from_pretrained(model_name), **kwargs)\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Get next token log probabilities asynchronously.\n\n        Args:\n            token_ids (list[int]): Input token IDs.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self._get_logprobs(token_ids)\n\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Get next token log probabilities synchronously.\n\n        Args:\n            token_ids (list[int]): Input token IDs.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self._get_logprobs(token_ids)\n\n    def _get_logprobs(self, token_ids):\n        \"\"\"Generate random but deterministic log probabilities for given tokens.\n\n        Uses token_ids to seed the random generator, ensuring same inputs produce same outputs.\n\n        Args:\n            token_ids (list[int]): Input token IDs.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        seed = sum([(i + 1) * t for i, t in enumerate(token_ids)])\n        self._rng.seed(seed)\n        logits = torch.from_numpy(\n            self._rng.rand(len(self.tokenizer)).astype(np.float32)\n        )\n        return torch.log_softmax(logits, dim=-1)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.MockAsyncLM.__init__","title":"<code>__init__(tokenizer)</code>","text":"<p>Initialize a <code>MockAsyncLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>Hugging Face tokenizer instance</p> required Source code in <code>genlm_backend/llm/base.py</code> <pre><code>def __init__(self, tokenizer):\n    \"\"\"Initialize a `MockAsyncLM` instance.\n\n    Args:\n        tokenizer: Hugging Face tokenizer instance\n    \"\"\"\n    super().__init__(tokenizer)\n    self._rng = np.random.RandomState(42)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.MockAsyncLM._get_logprobs","title":"<code>_get_logprobs(token_ids)</code>","text":"<p>Generate random but deterministic log probabilities for given tokens.</p> <p>Uses token_ids to seed the random generator, ensuring same inputs produce same outputs.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Input token IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>def _get_logprobs(self, token_ids):\n    \"\"\"Generate random but deterministic log probabilities for given tokens.\n\n    Uses token_ids to seed the random generator, ensuring same inputs produce same outputs.\n\n    Args:\n        token_ids (list[int]): Input token IDs.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    seed = sum([(i + 1) * t for i, t in enumerate(token_ids)])\n    self._rng.seed(seed)\n    logits = torch.from_numpy(\n        self._rng.rand(len(self.tokenizer)).astype(np.float32)\n    )\n    return torch.log_softmax(logits, dim=-1)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.MockAsyncLM.from_name","title":"<code>from_name(model_name, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a MockAsyncLM instance over the vocabulary of the model's tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of pretrained model to load tokenizer from</p> required <code>**kwargs</code> <p>Additional arguments passed to <code>MockAsyncLM</code> constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>MockAsyncLM</code> <p><code>MockAsyncLM</code> instance initialized with tokenizer from <code>model_name</code></p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>@classmethod\ndef from_name(cls, model_name, **kwargs):\n    \"\"\"Create a MockAsyncLM instance over the vocabulary of the model's tokenizer.\n\n    Args:\n        model_name (str): Name of pretrained model to load tokenizer from\n        **kwargs: Additional arguments passed to `MockAsyncLM` constructor\n\n    Returns:\n        (MockAsyncLM): `MockAsyncLM` instance initialized with tokenizer from `model_name`\n    \"\"\"\n    from transformers import AutoTokenizer\n\n    return cls(AutoTokenizer.from_pretrained(model_name), **kwargs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.MockAsyncLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Get next token log probabilities asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Input token IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Get next token log probabilities asynchronously.\n\n    Args:\n        token_ids (list[int]): Input token IDs.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self._get_logprobs(token_ids)\n</code></pre>"},{"location":"reference/genlm_backend/llm/__init__/#genlm_backend.llm.MockAsyncLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Get next token log probabilities synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Input token IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>def next_token_logprobs_sync(self, token_ids):\n    \"\"\"Get next token log probabilities synchronously.\n\n    Args:\n        token_ids (list[int]): Input token IDs.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self._get_logprobs(token_ids)\n</code></pre>"},{"location":"reference/genlm_backend/llm/base/","title":"base","text":""},{"location":"reference/genlm_backend/llm/base/#genlm_backend.llm.base.AsyncLM","title":"<code>AsyncLM</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for asynchronous language models.</p> <p>This class provides an interface for language models that can generate token probabilities asynchronously. It handles tokenization and vocabulary management.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance compatible with the language model</p> required Source code in <code>genlm_backend/llm/base.py</code> <pre><code>class AsyncLM(ABC):\n    \"\"\"Abstract base class for asynchronous language models.\n\n    This class provides an interface for language models that can generate token probabilities\n    asynchronously. It handles tokenization and vocabulary management.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance compatible with the language model\n    \"\"\"\n\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.byte_vocab, self.str_vocab = decode_vocab(self.tokenizer)\n\n    @abstractmethod\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously.\n\n        Args:\n            token_ids (list[int]): A list of token IDs representing the prompt.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token synchronously.\n\n        Args:\n            token_ids (list[int]): A list of token IDs representing the prompt.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        pass\n\n    async def batch_next_token_logprobs(self, token_ids_list):\n        \"\"\"Batch request log probabilities for multiple token sequences asynchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists.\n\n        Returns:\n            (torch.Tensor): A tensor of log probability tensors.\n        \"\"\"\n        logprobs = await asyncio.gather(\n            *[self.next_token_logprobs(token_ids) for token_ids in token_ids_list]\n        )\n\n        return torch.stack(logprobs)\n\n    def batch_next_token_logprobs_sync(self, token_ids_list):\n        \"\"\"Batch request log probabilities for multiple token sequences synchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists.\n\n        Returns:\n            (torch.Tensor): A tensor of log probability tensors.\n        \"\"\"\n        return torch.stack(\n            [self.next_token_logprobs_sync(token_ids) for token_ids in token_ids_list]\n        )\n\n    def clear_cache(self):\n        \"\"\"Clear any caches used by the language model. No-op in base class.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/genlm_backend/llm/base/#genlm_backend.llm.base.AsyncLM.batch_next_token_logprobs","title":"<code>batch_next_token_logprobs(token_ids_list)</code>  <code>async</code>","text":"<p>Batch request log probabilities for multiple token sequences asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of log probability tensors.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>async def batch_next_token_logprobs(self, token_ids_list):\n    \"\"\"Batch request log probabilities for multiple token sequences asynchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists.\n\n    Returns:\n        (torch.Tensor): A tensor of log probability tensors.\n    \"\"\"\n    logprobs = await asyncio.gather(\n        *[self.next_token_logprobs(token_ids) for token_ids in token_ids_list]\n    )\n\n    return torch.stack(logprobs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/base/#genlm_backend.llm.base.AsyncLM.batch_next_token_logprobs_sync","title":"<code>batch_next_token_logprobs_sync(token_ids_list)</code>","text":"<p>Batch request log probabilities for multiple token sequences synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of log probability tensors.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>def batch_next_token_logprobs_sync(self, token_ids_list):\n    \"\"\"Batch request log probabilities for multiple token sequences synchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists.\n\n    Returns:\n        (torch.Tensor): A tensor of log probability tensors.\n    \"\"\"\n    return torch.stack(\n        [self.next_token_logprobs_sync(token_ids) for token_ids in token_ids_list]\n    )\n</code></pre>"},{"location":"reference/genlm_backend/llm/base/#genlm_backend.llm.base.AsyncLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear any caches used by the language model. No-op in base class.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear any caches used by the language model. No-op in base class.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm_backend/llm/base/#genlm_backend.llm.base.AsyncLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Request log probabilities of next token asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>A list of token IDs representing the prompt.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>@abstractmethod\nasync def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token asynchronously.\n\n    Args:\n        token_ids (list[int]): A list of token IDs representing the prompt.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm_backend/llm/base/#genlm_backend.llm.base.AsyncLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>  <code>abstractmethod</code>","text":"<p>Request log probabilities of next token synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>A list of token IDs representing the prompt.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>@abstractmethod\ndef next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token synchronously.\n\n    Args:\n        token_ids (list[int]): A list of token IDs representing the prompt.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm_backend/llm/base/#genlm_backend.llm.base.MockAsyncLM","title":"<code>MockAsyncLM</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>Mock implementation of AsyncLM used for testing.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>class MockAsyncLM(AsyncLM):\n    \"\"\"Mock implementation of AsyncLM used for testing.\"\"\"\n\n    def __init__(self, tokenizer):\n        \"\"\"Initialize a `MockAsyncLM` instance.\n\n        Args:\n            tokenizer: Hugging Face tokenizer instance\n        \"\"\"\n        super().__init__(tokenizer)\n        self._rng = np.random.RandomState(42)\n\n    @classmethod\n    def from_name(cls, model_name, **kwargs):\n        \"\"\"Create a MockAsyncLM instance over the vocabulary of the model's tokenizer.\n\n        Args:\n            model_name (str): Name of pretrained model to load tokenizer from\n            **kwargs: Additional arguments passed to `MockAsyncLM` constructor\n\n        Returns:\n            (MockAsyncLM): `MockAsyncLM` instance initialized with tokenizer from `model_name`\n        \"\"\"\n        from transformers import AutoTokenizer\n\n        return cls(AutoTokenizer.from_pretrained(model_name), **kwargs)\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Get next token log probabilities asynchronously.\n\n        Args:\n            token_ids (list[int]): Input token IDs.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self._get_logprobs(token_ids)\n\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Get next token log probabilities synchronously.\n\n        Args:\n            token_ids (list[int]): Input token IDs.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self._get_logprobs(token_ids)\n\n    def _get_logprobs(self, token_ids):\n        \"\"\"Generate random but deterministic log probabilities for given tokens.\n\n        Uses token_ids to seed the random generator, ensuring same inputs produce same outputs.\n\n        Args:\n            token_ids (list[int]): Input token IDs.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        seed = sum([(i + 1) * t for i, t in enumerate(token_ids)])\n        self._rng.seed(seed)\n        logits = torch.from_numpy(\n            self._rng.rand(len(self.tokenizer)).astype(np.float32)\n        )\n        return torch.log_softmax(logits, dim=-1)\n</code></pre>"},{"location":"reference/genlm_backend/llm/base/#genlm_backend.llm.base.MockAsyncLM.__init__","title":"<code>__init__(tokenizer)</code>","text":"<p>Initialize a <code>MockAsyncLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>Hugging Face tokenizer instance</p> required Source code in <code>genlm_backend/llm/base.py</code> <pre><code>def __init__(self, tokenizer):\n    \"\"\"Initialize a `MockAsyncLM` instance.\n\n    Args:\n        tokenizer: Hugging Face tokenizer instance\n    \"\"\"\n    super().__init__(tokenizer)\n    self._rng = np.random.RandomState(42)\n</code></pre>"},{"location":"reference/genlm_backend/llm/base/#genlm_backend.llm.base.MockAsyncLM._get_logprobs","title":"<code>_get_logprobs(token_ids)</code>","text":"<p>Generate random but deterministic log probabilities for given tokens.</p> <p>Uses token_ids to seed the random generator, ensuring same inputs produce same outputs.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Input token IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>def _get_logprobs(self, token_ids):\n    \"\"\"Generate random but deterministic log probabilities for given tokens.\n\n    Uses token_ids to seed the random generator, ensuring same inputs produce same outputs.\n\n    Args:\n        token_ids (list[int]): Input token IDs.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    seed = sum([(i + 1) * t for i, t in enumerate(token_ids)])\n    self._rng.seed(seed)\n    logits = torch.from_numpy(\n        self._rng.rand(len(self.tokenizer)).astype(np.float32)\n    )\n    return torch.log_softmax(logits, dim=-1)\n</code></pre>"},{"location":"reference/genlm_backend/llm/base/#genlm_backend.llm.base.MockAsyncLM.from_name","title":"<code>from_name(model_name, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a MockAsyncLM instance over the vocabulary of the model's tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of pretrained model to load tokenizer from</p> required <code>**kwargs</code> <p>Additional arguments passed to <code>MockAsyncLM</code> constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>MockAsyncLM</code> <p><code>MockAsyncLM</code> instance initialized with tokenizer from <code>model_name</code></p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>@classmethod\ndef from_name(cls, model_name, **kwargs):\n    \"\"\"Create a MockAsyncLM instance over the vocabulary of the model's tokenizer.\n\n    Args:\n        model_name (str): Name of pretrained model to load tokenizer from\n        **kwargs: Additional arguments passed to `MockAsyncLM` constructor\n\n    Returns:\n        (MockAsyncLM): `MockAsyncLM` instance initialized with tokenizer from `model_name`\n    \"\"\"\n    from transformers import AutoTokenizer\n\n    return cls(AutoTokenizer.from_pretrained(model_name), **kwargs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/base/#genlm_backend.llm.base.MockAsyncLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Get next token log probabilities asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Input token IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Get next token log probabilities asynchronously.\n\n    Args:\n        token_ids (list[int]): Input token IDs.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self._get_logprobs(token_ids)\n</code></pre>"},{"location":"reference/genlm_backend/llm/base/#genlm_backend.llm.base.MockAsyncLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Get next token log probabilities synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Input token IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/base.py</code> <pre><code>def next_token_logprobs_sync(self, token_ids):\n    \"\"\"Get next token log probabilities synchronously.\n\n    Args:\n        token_ids (list[int]): Input token IDs.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self._get_logprobs(token_ids)\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/","title":"hf","text":""},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer","title":"<code>AsyncTransformer</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>Asynchronous wrapper around a HuggingFace causal language model with caching support.</p> <p>This class provides an asynchronous interface to HuggingFace language models with automatic batching and caching (output and KV) for improved efficiency.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>class AsyncTransformer(AsyncLM):\n    \"\"\"Asynchronous wrapper around a HuggingFace causal language model with caching support.\n\n    This class provides an asynchronous interface to HuggingFace language models with automatic batching\n    and caching (output and KV) for improved efficiency.\n    \"\"\"\n\n    @classmethod\n    def from_name(cls, model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs):\n        \"\"\"Create an AsyncTransformer instance from a pretrained HuggingFace model.\n\n        Args:\n            model_id (str): Model identifier in HuggingFace's model hub.\n            bitsandbytes_opts (dict, optional): Additional configuration options for bitsandbytes quantization.\n                Defaults to None.\n            hf_opts (dict, optional): Additional configuration options for loading the HuggingFace model.\n                Defaults to None.\n            **kwargs: Additional arguments passed to the `AsyncTransformer` constructor\n\n        Returns:\n            (AsyncTransformer): An initialized `AsyncTransformer` instance.\n        \"\"\"\n        if bitsandbytes_opts:\n            bnb_config = BitsAndBytesConfig(**bitsandbytes_opts)\n        else:\n            bnb_config = None\n\n        _hf_opts = {\n            \"device_map\": \"auto\",\n            \"torch_dtype\": \"auto\",\n        }\n        if hf_opts:\n            _hf_opts.update(hf_opts)\n\n        tok = AutoTokenizer.from_pretrained(model_id)\n        mod = AutoModelForCausalLM.from_pretrained(\n            model_id, quantization_config=bnb_config, **_hf_opts\n        )\n\n        return cls(mod, tok, **kwargs)\n\n    @torch.no_grad()\n    def __init__(self, hf_model, hf_tokenizer, batch_size=20, timeout=0.02):\n        \"\"\"Initialize an AsyncTransformer instance.\n\n        Args:\n            hf_model: A HuggingFace CausalLM model instance.\n            hf_tokenizer: A HuggingFace Tokenizer.\n            batch_size (int, optional): Maximum queries to process in one batch during auto-batching.\n                Defaults to 20.\n            timeout (float, optional): Seconds to wait since last query before processing current batch.\n                Defaults to 0.02.\n        \"\"\"\n        self.model = hf_model\n        self.tokenizer = hf_tokenizer\n        self.device = hf_model.device\n        self.cache = TokenTrie()\n\n        # Queries to be batched. Each query is a sequence of tokens,\n        # and a Future to be called when the query is resolved.\n        self.queries = []\n        self.batch_size = batch_size\n        self.timeout = timeout\n        self.timer = None\n\n        self.model.eval()\n\n        super().__init__(tokenizer=self.tokenizer)\n\n    def clear_cache(self):\n        \"\"\"Clear the cache of log probabilities and key/value pairs.\"\"\"\n        self.cache = TokenTrie(None, self.cache.logprobs)\n\n    def clear_kv_cache(self):\n        \"\"\"Clear any key and value vectors from the cache.\"\"\"\n        self.cache.clear_kv_cache()\n\n    def reset_async_queries(self):\n        \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n        to completion.\"\"\"\n        self.queries = []\n\n    @torch.no_grad()\n    def cache_kv(self, prompt_tokens):\n        \"\"\"Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.\n\n        Args:\n            prompt_tokens (list[int]): token ids for the prompt to cache.\n        \"\"\"\n        result = self.model(torch.tensor([prompt_tokens]).to(self.device))\n        node = self.cache.extend_cache(1, prompt_tokens, result.logits[0], 0)\n        node.past_key_values = result.past_key_values\n\n    @torch.no_grad()\n    def batch_evaluate_queries(self):\n        \"\"\"\n        Process a batch of queued language model queries.\n\n        This method is called internally when the `batch_size` has been met or the `timeout` has expired.\n        \"\"\"\n\n        queries, self.queries = self.queries, []\n        if len(queries) == 0:\n            return\n\n        query_groups = defaultdict(list)\n        for query in queries:\n            key = tuple(query.prompt)  # XXX: cache based on past_len too?\n            query_groups[key].append(query)\n\n        # Use one representative query from each group\n        unique_queries = [group[0] for group in query_groups.values()]\n\n        past_example = next((q.past for q in unique_queries if q.past), False)\n        max_past_length = max(q.past_len for q in unique_queries)\n        max_query_length = max(len(q.prompt) for q in unique_queries)\n\n        padding_token_id = (\n            self.tokenizer.pad_token_id\n            if self.tokenizer.pad_token_id is not None\n            else 0\n        )\n\n        input_ids = torch.tensor(\n            [\n                q.prompt_padded(padding_token_id, max_query_length)\n                for q in unique_queries\n            ]\n        ).to(self.device)\n        attn_masks = torch.tensor(\n            [\n                q.attention_mask(max_past_length, max_query_length)\n                for q in unique_queries\n            ]\n        ).to(self.device)\n        posn_ids = torch.tensor(\n            [q.position_ids(max_past_length, max_query_length) for q in unique_queries]\n        ).to(self.device)\n        if past_example:\n            pasts = [\n                [\n                    torch.cat(\n                        (\n                            *(\n                                q.past_padded(\n                                    layer,\n                                    j,\n                                    max_past_length,\n                                    past_example[0][0].dtype,\n                                    self.device,\n                                    past_example[0][0].shape,\n                                )\n                                for q in unique_queries\n                            ),\n                        ),\n                        dim=0,\n                    )\n                    for j in range(2)\n                ]\n                for layer in range(len(past_example))\n            ]\n        else:\n            pasts = None\n\n        results = self.model(\n            input_ids,\n            attention_mask=attn_masks,\n            position_ids=posn_ids,\n            past_key_values=pasts,\n            use_cache=pasts is not None,\n        )\n\n        assert len(results.logits) == len(unique_queries)\n\n        for i, q in enumerate(unique_queries):\n            result = results.logits[i]\n            for dup_query in query_groups[tuple(q.prompt)]:\n                dup_query.future.set_result(result)\n\n    @torch.no_grad()\n    def add_query(self, query, future, past):\n        \"\"\"Add a query to be evaluated in the next batch.\n\n        This method is called internally when a `next_token_logprobs` request is made.\n\n        Args:\n            query (list[int]): Token IDs representing the query prompt\n            future (asyncio.Future): Future to store the result in\n            past (list[tuple[torch.Tensor]]|None): Past key/value states from previous evaluation,\n                or None if this is a new query\n        \"\"\"\n        self.queries.append(Query(query, future, past))\n\n        if self.timer:\n            self.timer.cancel()\n            self.timer = None\n        if len(self.queries) &gt;= self.batch_size:\n            self.batch_evaluate_queries()\n        else:\n            self.timer = asyncio.get_running_loop().call_later(\n                self.timeout, lambda: self.batch_evaluate_queries()\n            )\n\n    def walk_cache(self, token_ids):\n        \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n        Args:\n            token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n        Returns:\n            tuple:\n                - CacheNode: The deepest node in the cache tree that matches the token sequence\n                - int: Number of tokens matched from the start of token_ids\n                - list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,\n                    or None if no cached states were found\n                - int: Base index indicating where the past states start in token_ids\n        \"\"\"\n        # Walk while tokens can be found\n        node = self.cache\n        next_token_index = 0\n\n        past = None\n        base = 0\n        while next_token_index &lt; len(token_ids):\n            if node.past_key_values is not None:\n                past = node.past_key_values\n                base = next_token_index\n            if node.has_token(token_ids[next_token_index]):\n                node = node.get_token(token_ids[next_token_index])\n                next_token_index += 1\n            else:\n                break\n\n        return node, next_token_index, past, base\n\n    @torch.no_grad()\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        node, next_token_index, past, base = self.walk_cache(token_ids)\n\n        # If we processed all tokens, then we're done.\n        if next_token_index == len(token_ids):\n            return node.logprobs\n\n        # Create a future with the prompt\n        future = asyncio.get_running_loop().create_future()\n        self.add_query(token_ids[base:], future, past)\n        logits = await future\n\n        # Create new nodes\n        node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n        return node.logprobs\n\n    @torch.no_grad()\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        # Walk while tokens can be found\n        node, next_token_index, past, base = self.walk_cache(token_ids)\n\n        if next_token_index == len(token_ids):\n            return node.logprobs\n\n        logits = self.model(\n            torch.tensor([token_ids[base:]]).to(self.device),\n            past_key_values=node.past_key_values,\n            use_cache=node.past_key_values is not None,\n        ).logits[0]\n\n        node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n        return node.logprobs\n\n    def next_token_logprobs_uncached(self, token_ids):\n        \"\"\"Request log probabilities of next token. No KV or output caching, and does not support auto-batching.\n\n        Args:\n            token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n        Returns:\n            logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n        \"\"\"\n        if not token_ids:\n            raise ValueError(\"Token ids must not be empty\")\n\n        with torch.no_grad():\n            logits = self.model(\n                torch.tensor([token_ids]).to(self.device),\n                past_key_values=None,\n                use_cache=False,\n            ).logits[0]\n            return torch.log_softmax(logits[-1], dim=0)\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer.__init__","title":"<code>__init__(hf_model, hf_tokenizer, batch_size=20, timeout=0.02)</code>","text":"<p>Initialize an AsyncTransformer instance.</p> <p>Parameters:</p> Name Type Description Default <code>hf_model</code> <p>A HuggingFace CausalLM model instance.</p> required <code>hf_tokenizer</code> <p>A HuggingFace Tokenizer.</p> required <code>batch_size</code> <code>int</code> <p>Maximum queries to process in one batch during auto-batching. Defaults to 20.</p> <code>20</code> <code>timeout</code> <code>float</code> <p>Seconds to wait since last query before processing current batch. Defaults to 0.02.</p> <code>0.02</code> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef __init__(self, hf_model, hf_tokenizer, batch_size=20, timeout=0.02):\n    \"\"\"Initialize an AsyncTransformer instance.\n\n    Args:\n        hf_model: A HuggingFace CausalLM model instance.\n        hf_tokenizer: A HuggingFace Tokenizer.\n        batch_size (int, optional): Maximum queries to process in one batch during auto-batching.\n            Defaults to 20.\n        timeout (float, optional): Seconds to wait since last query before processing current batch.\n            Defaults to 0.02.\n    \"\"\"\n    self.model = hf_model\n    self.tokenizer = hf_tokenizer\n    self.device = hf_model.device\n    self.cache = TokenTrie()\n\n    # Queries to be batched. Each query is a sequence of tokens,\n    # and a Future to be called when the query is resolved.\n    self.queries = []\n    self.batch_size = batch_size\n    self.timeout = timeout\n    self.timer = None\n\n    self.model.eval()\n\n    super().__init__(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer.add_query","title":"<code>add_query(query, future, past)</code>","text":"<p>Add a query to be evaluated in the next batch.</p> <p>This method is called internally when a <code>next_token_logprobs</code> request is made.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[int]</code> <p>Token IDs representing the query prompt</p> required <code>future</code> <code>Future</code> <p>Future to store the result in</p> required <code>past</code> <code>list[tuple[Tensor]] | None</code> <p>Past key/value states from previous evaluation, or None if this is a new query</p> required Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef add_query(self, query, future, past):\n    \"\"\"Add a query to be evaluated in the next batch.\n\n    This method is called internally when a `next_token_logprobs` request is made.\n\n    Args:\n        query (list[int]): Token IDs representing the query prompt\n        future (asyncio.Future): Future to store the result in\n        past (list[tuple[torch.Tensor]]|None): Past key/value states from previous evaluation,\n            or None if this is a new query\n    \"\"\"\n    self.queries.append(Query(query, future, past))\n\n    if self.timer:\n        self.timer.cancel()\n        self.timer = None\n    if len(self.queries) &gt;= self.batch_size:\n        self.batch_evaluate_queries()\n    else:\n        self.timer = asyncio.get_running_loop().call_later(\n            self.timeout, lambda: self.batch_evaluate_queries()\n        )\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer.batch_evaluate_queries","title":"<code>batch_evaluate_queries()</code>","text":"<p>Process a batch of queued language model queries.</p> <p>This method is called internally when the <code>batch_size</code> has been met or the <code>timeout</code> has expired.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef batch_evaluate_queries(self):\n    \"\"\"\n    Process a batch of queued language model queries.\n\n    This method is called internally when the `batch_size` has been met or the `timeout` has expired.\n    \"\"\"\n\n    queries, self.queries = self.queries, []\n    if len(queries) == 0:\n        return\n\n    query_groups = defaultdict(list)\n    for query in queries:\n        key = tuple(query.prompt)  # XXX: cache based on past_len too?\n        query_groups[key].append(query)\n\n    # Use one representative query from each group\n    unique_queries = [group[0] for group in query_groups.values()]\n\n    past_example = next((q.past for q in unique_queries if q.past), False)\n    max_past_length = max(q.past_len for q in unique_queries)\n    max_query_length = max(len(q.prompt) for q in unique_queries)\n\n    padding_token_id = (\n        self.tokenizer.pad_token_id\n        if self.tokenizer.pad_token_id is not None\n        else 0\n    )\n\n    input_ids = torch.tensor(\n        [\n            q.prompt_padded(padding_token_id, max_query_length)\n            for q in unique_queries\n        ]\n    ).to(self.device)\n    attn_masks = torch.tensor(\n        [\n            q.attention_mask(max_past_length, max_query_length)\n            for q in unique_queries\n        ]\n    ).to(self.device)\n    posn_ids = torch.tensor(\n        [q.position_ids(max_past_length, max_query_length) for q in unique_queries]\n    ).to(self.device)\n    if past_example:\n        pasts = [\n            [\n                torch.cat(\n                    (\n                        *(\n                            q.past_padded(\n                                layer,\n                                j,\n                                max_past_length,\n                                past_example[0][0].dtype,\n                                self.device,\n                                past_example[0][0].shape,\n                            )\n                            for q in unique_queries\n                        ),\n                    ),\n                    dim=0,\n                )\n                for j in range(2)\n            ]\n            for layer in range(len(past_example))\n        ]\n    else:\n        pasts = None\n\n    results = self.model(\n        input_ids,\n        attention_mask=attn_masks,\n        position_ids=posn_ids,\n        past_key_values=pasts,\n        use_cache=pasts is not None,\n    )\n\n    assert len(results.logits) == len(unique_queries)\n\n    for i, q in enumerate(unique_queries):\n        result = results.logits[i]\n        for dup_query in query_groups[tuple(q.prompt)]:\n            dup_query.future.set_result(result)\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer.cache_kv","title":"<code>cache_kv(prompt_tokens)</code>","text":"<p>Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_tokens</code> <code>list[int]</code> <p>token ids for the prompt to cache.</p> required Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef cache_kv(self, prompt_tokens):\n    \"\"\"Cache the key and value vectors for a prompt. Future queries that have this prompt as a prefix will only run the LLM on new tokens.\n\n    Args:\n        prompt_tokens (list[int]): token ids for the prompt to cache.\n    \"\"\"\n    result = self.model(torch.tensor([prompt_tokens]).to(self.device))\n    node = self.cache.extend_cache(1, prompt_tokens, result.logits[0], 0)\n    node.past_key_values = result.past_key_values\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the cache of log probabilities and key/value pairs.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the cache of log probabilities and key/value pairs.\"\"\"\n    self.cache = TokenTrie(None, self.cache.logprobs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer.clear_kv_cache","title":"<code>clear_kv_cache()</code>","text":"<p>Clear any key and value vectors from the cache.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>def clear_kv_cache(self):\n    \"\"\"Clear any key and value vectors from the cache.\"\"\"\n    self.cache.clear_kv_cache()\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer.from_name","title":"<code>from_name(model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create an AsyncTransformer instance from a pretrained HuggingFace model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Model identifier in HuggingFace's model hub.</p> required <code>bitsandbytes_opts</code> <code>dict</code> <p>Additional configuration options for bitsandbytes quantization. Defaults to None.</p> <code>None</code> <code>hf_opts</code> <code>dict</code> <p>Additional configuration options for loading the HuggingFace model. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the <code>AsyncTransformer</code> constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTransformer</code> <p>An initialized <code>AsyncTransformer</code> instance.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@classmethod\ndef from_name(cls, model_id, bitsandbytes_opts=None, hf_opts=None, **kwargs):\n    \"\"\"Create an AsyncTransformer instance from a pretrained HuggingFace model.\n\n    Args:\n        model_id (str): Model identifier in HuggingFace's model hub.\n        bitsandbytes_opts (dict, optional): Additional configuration options for bitsandbytes quantization.\n            Defaults to None.\n        hf_opts (dict, optional): Additional configuration options for loading the HuggingFace model.\n            Defaults to None.\n        **kwargs: Additional arguments passed to the `AsyncTransformer` constructor\n\n    Returns:\n        (AsyncTransformer): An initialized `AsyncTransformer` instance.\n    \"\"\"\n    if bitsandbytes_opts:\n        bnb_config = BitsAndBytesConfig(**bitsandbytes_opts)\n    else:\n        bnb_config = None\n\n    _hf_opts = {\n        \"device_map\": \"auto\",\n        \"torch_dtype\": \"auto\",\n    }\n    if hf_opts:\n        _hf_opts.update(hf_opts)\n\n    tok = AutoTokenizer.from_pretrained(model_id)\n    mod = AutoModelForCausalLM.from_pretrained(\n        model_id, quantization_config=bnb_config, **_hf_opts\n    )\n\n    return cls(mod, tok, **kwargs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with <code>await</code>.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@torch.no_grad()\nasync def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token. This version is asynchronous because it automatically batches concurrent requests; use with `await`.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor of with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    node, next_token_index, past, base = self.walk_cache(token_ids)\n\n    # If we processed all tokens, then we're done.\n    if next_token_index == len(token_ids):\n        return node.logprobs\n\n    # Create a future with the prompt\n    future = asyncio.get_running_loop().create_future()\n    self.add_query(token_ids[base:], future, past)\n    logits = await future\n\n    # Create new nodes\n    node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n    return node.logprobs\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Request log probabilities of next token. Not asynchronous, and does not support auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>@torch.no_grad()\ndef next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token. Not asynchronous, and does not support auto-batching.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    # Walk while tokens can be found\n    node, next_token_index, past, base = self.walk_cache(token_ids)\n\n    if next_token_index == len(token_ids):\n        return node.logprobs\n\n    logits = self.model(\n        torch.tensor([token_ids[base:]]).to(self.device),\n        past_key_values=node.past_key_values,\n        use_cache=node.past_key_values is not None,\n    ).logits[0]\n\n    node = node.extend_cache(next_token_index, token_ids, logits, base)\n\n    return node.logprobs\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer.next_token_logprobs_uncached","title":"<code>next_token_logprobs_uncached(token_ids)</code>","text":"<p>Request log probabilities of next token. No KV or output caching, and does not support auto-batching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>a list of token ids, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>logprobs</code> <code>Tensor</code> <p>a tensor with the language model's log (normalized) probabilities for the next token following the prompt.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>def next_token_logprobs_uncached(self, token_ids):\n    \"\"\"Request log probabilities of next token. No KV or output caching, and does not support auto-batching.\n\n    Args:\n        token_ids (list[int]): a list of token ids, representing a prompt to the language model.\n\n    Returns:\n        logprobs (torch.Tensor): a tensor with the language model's log (normalized) probabilities for the next token following the prompt.\n    \"\"\"\n    if not token_ids:\n        raise ValueError(\"Token ids must not be empty\")\n\n    with torch.no_grad():\n        logits = self.model(\n            torch.tensor([token_ids]).to(self.device),\n            past_key_values=None,\n            use_cache=False,\n        ).logits[0]\n        return torch.log_softmax(logits[-1], dim=0)\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer.reset_async_queries","title":"<code>reset_async_queries()</code>","text":"<p>Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing to completion.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>def reset_async_queries(self):\n    \"\"\"Clear any pending language model queries from the queue. Use this method when an exception prevented an inference algorithm from executing\n    to completion.\"\"\"\n    self.queries = []\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.AsyncTransformer.walk_cache","title":"<code>walk_cache(token_ids)</code>","text":"<p>Walk the cache tree to find the deepest node matching a sequence of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>list[int]</code> <p>Sequence of token IDs to follow in the cache tree</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <ul> <li>CacheNode: The deepest node in the cache tree that matches the token sequence</li> <li>int: Number of tokens matched from the start of token_ids</li> <li>list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,     or None if no cached states were found</li> <li>int: Base index indicating where the past states start in token_ids</li> </ul> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>def walk_cache(self, token_ids):\n    \"\"\"Walk the cache tree to find the deepest node matching a sequence of tokens.\n\n    Args:\n        token_ids (list[int]): Sequence of token IDs to follow in the cache tree\n\n    Returns:\n        tuple:\n            - CacheNode: The deepest node in the cache tree that matches the token sequence\n            - int: Number of tokens matched from the start of token_ids\n            - list[tuple[torch.Tensor]]|None: Past key/value states from the deepest cached node,\n                or None if no cached states were found\n            - int: Base index indicating where the past states start in token_ids\n    \"\"\"\n    # Walk while tokens can be found\n    node = self.cache\n    next_token_index = 0\n\n    past = None\n    base = 0\n    while next_token_index &lt; len(token_ids):\n        if node.past_key_values is not None:\n            past = node.past_key_values\n            base = next_token_index\n        if node.has_token(token_ids[next_token_index]):\n            node = node.get_token(token_ids[next_token_index])\n            next_token_index += 1\n        else:\n            break\n\n    return node, next_token_index, past, base\n</code></pre>"},{"location":"reference/genlm_backend/llm/hf/#genlm_backend.llm.hf.Query","title":"<code>Query</code>","text":"<p>A query to a language model, waiting to be batched.</p> Source code in <code>genlm_backend/llm/hf.py</code> <pre><code>class Query:\n    \"\"\"A query to a language model, waiting to be batched.\"\"\"\n\n    def __init__(self, prompt, future, past=None):\n        self.prompt = prompt\n        self.future = future\n        self.past = past\n\n        if self.past is not None:\n            self.past_len = past[\n                0\n            ][\n                0\n            ].shape[\n                2\n            ]  # layers, key or value, batch size, num heads, num tokens, head repr length\n        else:\n            self.past_len = 0\n\n    @torch.no_grad()\n    def past_padded(self, layer, j, to_length, dtype, device, past_shape):\n        if self.past is not None:\n            return torch.cat(\n                (\n                    self.past[layer][j],\n                    torch.zeros(\n                        1,\n                        past_shape[1],\n                        to_length - self.past_len,\n                        past_shape[3],\n                        dtype=dtype,\n                        device=device,\n                    ),\n                ),\n                dim=2,\n            )\n        else:\n            return torch.zeros(\n                1, past_shape[1], to_length, past_shape[3], dtype=dtype, device=device\n            )\n\n    def prompt_padded(self, pad_token, to_length):\n        return [*self.prompt, *[pad_token for _ in range(to_length - len(self.prompt))]]\n\n    def attention_mask(self, total_past_length, total_seq_length):\n        return [\n            *[1 for _ in range(self.past_len)],\n            *[0 for _ in range(total_past_length - self.past_len)],\n            *[1 for _ in range(len(self.prompt))],\n            *[0 for _ in range(total_seq_length - len(self.prompt))],\n        ]\n\n    def position_ids(self, total_past_length, total_seq_length):\n        return [\n            *range(self.past_len, self.past_len + len(self.prompt)),\n            *[0 for _ in range(total_seq_length - len(self.prompt))],\n        ]\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/","title":"vllm","text":""},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.AsyncVirtualLM","title":"<code>AsyncVirtualLM</code>","text":"<p>               Bases: <code>AsyncLM</code></p> <p>A wrapper around vLLM's <code>AsyncLLMEngine</code> for asynchronous next token log probability computations.</p> <p>This class provides an asynchronous interface for computing log probabilities using vLLM's engine. It is optimized for next token log probability computations and supports caching of results (outputs and KV).</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>class AsyncVirtualLM(AsyncLM):\n    \"\"\"A wrapper around vLLM's `AsyncLLMEngine` for asynchronous next token log probability computations.\n\n    This class provides an asynchronous interface for computing log probabilities using vLLM's engine.\n    It is optimized for next token log probability computations and supports caching of results (outputs and KV).\n    \"\"\"\n\n    default_params = SamplingParams(\n        max_tokens=1, n=1, logprobs=1, detokenize=False, stop=None, ignore_eos=True\n    )\n\n    def __init__(self, async_llm_engine, cache_size=0, cache_opts={}):\n        \"\"\"Initialize an `AsyncVirtualLM` instance.\n\n        Args:\n            async_llm_engine (AsyncLLMEngine): The async vLLM engine instance.\n            cache_size (int, optional): Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.\n            cache_opts (dict, optional): Additional options to pass to the [`OutputCache`][genlm_backend.cache.OutputCache] constructor. Defaults to {}.\n\n        Note:\n            The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.\n        \"\"\"\n        self.async_llm_engine = async_llm_engine\n        self.tokenizer = async_llm_engine.engine.get_tokenizer()\n        self.request_counter = Counter()\n        self.custom_sampler = DeferredSampler()\n        self.cache = (\n            OutputCache(maxsize=cache_size, **cache_opts)\n            if cache_size &gt; 0\n            else None\n        )\n\n        async_llm_engine.engine.log_stats = False\n\n        super().__init__(tokenizer=self.tokenizer)\n\n    @classmethod\n    def from_name(cls, model_name, engine_opts=None, **kwargs):\n        \"\"\"Create a `AsyncVirtualLM` instance from a model name.\n\n        Args:\n            model_name (str): Name of the model to load.\n            engine_opts (dict): Additional options to pass to the `AsyncLLMEngine`. The engine will be\n                configured with prefix caching enabled and async output processing disabled by default.\n            **kwargs: Additional arguments passed to `AsyncVirtualLM` constructor.\n\n        Returns:\n            (AsyncVirtualLM): An `AsyncVirtualLM` instance.\n        \"\"\"\n        if not HAS_VLLM:\n            raise ImportError(\n                \"vLLM not available. Install vLLM or use AsyncTransformer instead.\"\n            )\n\n        engine_opts = {\n            \"enable_prefix_caching\": True,\n            \"disable_log_requests\": True,\n            \"disable_async_output_proc\": True,\n            **(engine_opts or {}),\n        }\n\n        engine = AsyncLLMEngine.from_engine_args(\n            AsyncEngineArgs(model=model_name, tokenizer=model_name, **engine_opts)\n        )\n\n        return cls(engine, **kwargs)\n\n    async def next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously with output caching.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            result (torch.Tensor): Normalized log probability tensor.\n\n        Warning:\n            Do not use `asyncio.run(next_token_logprobs())` as it may interfere with vLLM's background loop.\n            For synchronous usage, use the `next_token_logprobs_sync()` method instead.\n        \"\"\"\n        key = tuple(token_ids)\n\n        if self.cache is not None and key in self.cache:\n            return self.cache[key]\n\n        result = await self._next_token_logprobs(key)\n\n        if self.cache is not None:\n            self.cache[key] = result\n\n        return result\n\n    async def _next_token_logprobs(self, token_ids):\n        \"\"\"Request log probabilities of next token asynchronously.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        req_id = str(next(self.request_counter))\n        prompt = TokensPrompt(prompt_token_ids=token_ids)\n\n        outputs = []\n        with self._optimized_sampling_context():\n            async for output in self.async_llm_engine.generate(\n                prompt=prompt,\n                sampling_params=self.default_params,\n                request_id=req_id,\n            ):\n                if output.finished:\n                    outputs.append(output)\n\n        return self._validate_outputs(outputs)\n\n    def next_token_logprobs_sync(self, token_ids):\n        \"\"\"Request log probabilities of next token synchronously.\n\n        Args:\n            token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): Normalized log probability tensor.\n        \"\"\"\n        return self.batch_next_token_logprobs_sync([token_ids])[0]\n\n    def batch_next_token_logprobs_sync(self, token_ids_list):\n        \"\"\"\n        Request log probabilities of next tokens in a batch synchronously.\n\n        Args:\n            token_ids_list (list[list[int]]): A list of token ID lists, each representing a prompt to the language model.\n\n        Returns:\n            (torch.Tensor): A tensor of normalized log probability tensors, one for each prompt in the input list.\n        \"\"\"\n        req_ids = []\n        for token_ids in token_ids_list:\n            req_id = str(next(self.request_counter))\n            req_ids.append(req_id)\n            self.async_llm_engine.engine.add_request(\n                prompt=TokensPrompt(prompt_token_ids=token_ids),\n                params=self.default_params,\n                request_id=req_id,\n            )\n\n        req_id2outputs = {}\n        with self._optimized_sampling_context():\n            while self.async_llm_engine.engine.has_unfinished_requests():\n                output = self.async_llm_engine.engine.step()\n                for out in output:\n                    if out.finished:\n                        assert out.request_id not in req_id2outputs, (\n                            f\"Duplicate outputs for request {out.request_id}\"\n                        )\n                        assert out.request_id in req_ids, (\n                            f\"{out.request_id} not in requested IDs\"\n                        )\n                        req_id2outputs[out.request_id] = out\n\n        logprobs = [\n            self._validate_outputs([req_id2outputs[req_id]]) for req_id in req_ids\n        ]\n\n        return torch.stack(logprobs)\n\n    @contextmanager\n    def _optimized_sampling_context(self):\n        \"\"\"Context manager for optimized sampling configuration.\"\"\"\n        model = self.async_llm_engine.engine.model_executor.driver_worker.model_runner.model\n        original_sampler = model.sampler\n        try:\n            model.sampler = self.custom_sampler\n            yield\n        finally:\n            model.sampler = original_sampler\n\n    def _validate_outputs(self, outputs):\n        \"\"\"Validate and extract logprobs from a vLLM output.\n\n        Args:\n            outputs: List of sequence group outputs from vLLM generation\n\n        Returns:\n            Tensor of log probabilities for the next token\n\n        Raises:\n            AssertionError: If output structure doesn't match expected format\n        \"\"\"\n        assert len(outputs) == 1, \"Expected exactly one sequence group\"\n        seq_group = outputs[0]\n\n        assert len(seq_group.outputs) == 1, (\n            \"Expected exactly one sequence in output\"\n        )\n        sequence = seq_group.outputs[0]\n\n        assert len(sequence.logprobs) == 1, \"Expected exactly one set of logprobs\"\n        token_logprobs = sequence.logprobs[0].logprobs\n\n        return token_logprobs\n\n    def clear_cache(self):\n        \"\"\"Clear output cache.\"\"\"\n        if self.cache:\n            self.cache.clear()\n\n    def __del__(self):\n        \"\"\"Clean up resources on deletion.\"\"\"\n        self._cleanup_engine()\n\n    def _cleanup_engine(self):\n        \"\"\"Clean up the vLLM engine and associated resources.\"\"\"\n        if async_engine := getattr(self, \"async_llm_engine\", None):\n            async_engine.shutdown_background_loop()\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.AsyncVirtualLM.__del__","title":"<code>__del__()</code>","text":"<p>Clean up resources on deletion.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def __del__(self):\n    \"\"\"Clean up resources on deletion.\"\"\"\n    self._cleanup_engine()\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.AsyncVirtualLM.__init__","title":"<code>__init__(async_llm_engine, cache_size=0, cache_opts={})</code>","text":"<p>Initialize an <code>AsyncVirtualLM</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>async_llm_engine</code> <code>AsyncLLMEngine</code> <p>The async vLLM engine instance.</p> required <code>cache_size</code> <code>int</code> <p>Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.</p> <code>0</code> <code>cache_opts</code> <code>dict</code> <p>Additional options to pass to the <code>OutputCache</code> constructor. Defaults to {}.</p> <code>{}</code> Note <p>The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def __init__(self, async_llm_engine, cache_size=0, cache_opts={}):\n    \"\"\"Initialize an `AsyncVirtualLM` instance.\n\n    Args:\n        async_llm_engine (AsyncLLMEngine): The async vLLM engine instance.\n        cache_size (int, optional): Maximum size of the output cache. If 0, caching is disabled. Defaults to 0.\n        cache_opts (dict, optional): Additional options to pass to the [`OutputCache`][genlm_backend.cache.OutputCache] constructor. Defaults to {}.\n\n    Note:\n        The cache stores the log probabilities for previously seen token sequences to avoid redundant requests. KV caching is handled internally by the vLLM engine.\n    \"\"\"\n    self.async_llm_engine = async_llm_engine\n    self.tokenizer = async_llm_engine.engine.get_tokenizer()\n    self.request_counter = Counter()\n    self.custom_sampler = DeferredSampler()\n    self.cache = (\n        OutputCache(maxsize=cache_size, **cache_opts)\n        if cache_size &gt; 0\n        else None\n    )\n\n    async_llm_engine.engine.log_stats = False\n\n    super().__init__(tokenizer=self.tokenizer)\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.AsyncVirtualLM._cleanup_engine","title":"<code>_cleanup_engine()</code>","text":"<p>Clean up the vLLM engine and associated resources.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def _cleanup_engine(self):\n    \"\"\"Clean up the vLLM engine and associated resources.\"\"\"\n    if async_engine := getattr(self, \"async_llm_engine\", None):\n        async_engine.shutdown_background_loop()\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.AsyncVirtualLM._next_token_logprobs","title":"<code>_next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>async def _next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token asynchronously.\n\n    Args:\n        token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    req_id = str(next(self.request_counter))\n    prompt = TokensPrompt(prompt_token_ids=token_ids)\n\n    outputs = []\n    with self._optimized_sampling_context():\n        async for output in self.async_llm_engine.generate(\n            prompt=prompt,\n            sampling_params=self.default_params,\n            request_id=req_id,\n        ):\n            if output.finished:\n                outputs.append(output)\n\n    return self._validate_outputs(outputs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.AsyncVirtualLM._optimized_sampling_context","title":"<code>_optimized_sampling_context()</code>","text":"<p>Context manager for optimized sampling configuration.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>@contextmanager\ndef _optimized_sampling_context(self):\n    \"\"\"Context manager for optimized sampling configuration.\"\"\"\n    model = self.async_llm_engine.engine.model_executor.driver_worker.model_runner.model\n    original_sampler = model.sampler\n    try:\n        model.sampler = self.custom_sampler\n        yield\n    finally:\n        model.sampler = original_sampler\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.AsyncVirtualLM._validate_outputs","title":"<code>_validate_outputs(outputs)</code>","text":"<p>Validate and extract logprobs from a vLLM output.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <p>List of sequence group outputs from vLLM generation</p> required <p>Returns:</p> Type Description <p>Tensor of log probabilities for the next token</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If output structure doesn't match expected format</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def _validate_outputs(self, outputs):\n    \"\"\"Validate and extract logprobs from a vLLM output.\n\n    Args:\n        outputs: List of sequence group outputs from vLLM generation\n\n    Returns:\n        Tensor of log probabilities for the next token\n\n    Raises:\n        AssertionError: If output structure doesn't match expected format\n    \"\"\"\n    assert len(outputs) == 1, \"Expected exactly one sequence group\"\n    seq_group = outputs[0]\n\n    assert len(seq_group.outputs) == 1, (\n        \"Expected exactly one sequence in output\"\n    )\n    sequence = seq_group.outputs[0]\n\n    assert len(sequence.logprobs) == 1, \"Expected exactly one set of logprobs\"\n    token_logprobs = sequence.logprobs[0].logprobs\n\n    return token_logprobs\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.AsyncVirtualLM.batch_next_token_logprobs_sync","title":"<code>batch_next_token_logprobs_sync(token_ids_list)</code>","text":"<p>Request log probabilities of next tokens in a batch synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[list[int]]</code> <p>A list of token ID lists, each representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of normalized log probability tensors, one for each prompt in the input list.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def batch_next_token_logprobs_sync(self, token_ids_list):\n    \"\"\"\n    Request log probabilities of next tokens in a batch synchronously.\n\n    Args:\n        token_ids_list (list[list[int]]): A list of token ID lists, each representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): A tensor of normalized log probability tensors, one for each prompt in the input list.\n    \"\"\"\n    req_ids = []\n    for token_ids in token_ids_list:\n        req_id = str(next(self.request_counter))\n        req_ids.append(req_id)\n        self.async_llm_engine.engine.add_request(\n            prompt=TokensPrompt(prompt_token_ids=token_ids),\n            params=self.default_params,\n            request_id=req_id,\n        )\n\n    req_id2outputs = {}\n    with self._optimized_sampling_context():\n        while self.async_llm_engine.engine.has_unfinished_requests():\n            output = self.async_llm_engine.engine.step()\n            for out in output:\n                if out.finished:\n                    assert out.request_id not in req_id2outputs, (\n                        f\"Duplicate outputs for request {out.request_id}\"\n                    )\n                    assert out.request_id in req_ids, (\n                        f\"{out.request_id} not in requested IDs\"\n                    )\n                    req_id2outputs[out.request_id] = out\n\n    logprobs = [\n        self._validate_outputs([req_id2outputs[req_id]]) for req_id in req_ids\n    ]\n\n    return torch.stack(logprobs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.AsyncVirtualLM.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear output cache.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear output cache.\"\"\"\n    if self.cache:\n        self.cache.clear()\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.AsyncVirtualLM.from_name","title":"<code>from_name(model_name, engine_opts=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a <code>AsyncVirtualLM</code> instance from a model name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to load.</p> required <code>engine_opts</code> <code>dict</code> <p>Additional options to pass to the <code>AsyncLLMEngine</code>. The engine will be configured with prefix caching enabled and async output processing disabled by default.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to <code>AsyncVirtualLM</code> constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncVirtualLM</code> <p>An <code>AsyncVirtualLM</code> instance.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>@classmethod\ndef from_name(cls, model_name, engine_opts=None, **kwargs):\n    \"\"\"Create a `AsyncVirtualLM` instance from a model name.\n\n    Args:\n        model_name (str): Name of the model to load.\n        engine_opts (dict): Additional options to pass to the `AsyncLLMEngine`. The engine will be\n            configured with prefix caching enabled and async output processing disabled by default.\n        **kwargs: Additional arguments passed to `AsyncVirtualLM` constructor.\n\n    Returns:\n        (AsyncVirtualLM): An `AsyncVirtualLM` instance.\n    \"\"\"\n    if not HAS_VLLM:\n        raise ImportError(\n            \"vLLM not available. Install vLLM or use AsyncTransformer instead.\"\n        )\n\n    engine_opts = {\n        \"enable_prefix_caching\": True,\n        \"disable_log_requests\": True,\n        \"disable_async_output_proc\": True,\n        **(engine_opts or {}),\n    }\n\n    engine = AsyncLLMEngine.from_engine_args(\n        AsyncEngineArgs(model=model_name, tokenizer=model_name, **engine_opts)\n    )\n\n    return cls(engine, **kwargs)\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.AsyncVirtualLM.next_token_logprobs","title":"<code>next_token_logprobs(token_ids)</code>  <code>async</code>","text":"<p>Request log probabilities of next token asynchronously with output caching.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>Tensor</code> <p>Normalized log probability tensor.</p> Warning <p>Do not use <code>asyncio.run(next_token_logprobs())</code> as it may interfere with vLLM's background loop. For synchronous usage, use the <code>next_token_logprobs_sync()</code> method instead.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>async def next_token_logprobs(self, token_ids):\n    \"\"\"Request log probabilities of next token asynchronously with output caching.\n\n    Args:\n        token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        result (torch.Tensor): Normalized log probability tensor.\n\n    Warning:\n        Do not use `asyncio.run(next_token_logprobs())` as it may interfere with vLLM's background loop.\n        For synchronous usage, use the `next_token_logprobs_sync()` method instead.\n    \"\"\"\n    key = tuple(token_ids)\n\n    if self.cache is not None and key in self.cache:\n        return self.cache[key]\n\n    result = await self._next_token_logprobs(key)\n\n    if self.cache is not None:\n        self.cache[key] = result\n\n    return result\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.AsyncVirtualLM.next_token_logprobs_sync","title":"<code>next_token_logprobs_sync(token_ids)</code>","text":"<p>Request log probabilities of next token synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids_list</code> <code>list[int]</code> <p>A list of token IDs, representing a prompt to the language model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized log probability tensor.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def next_token_logprobs_sync(self, token_ids):\n    \"\"\"Request log probabilities of next token synchronously.\n\n    Args:\n        token_ids_list (list[int]): A list of token IDs, representing a prompt to the language model.\n\n    Returns:\n        (torch.Tensor): Normalized log probability tensor.\n    \"\"\"\n    return self.batch_next_token_logprobs_sync([token_ids])[0]\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.DeferredSampler","title":"<code>DeferredSampler</code>","text":"<p>               Bases: <code>Module</code></p> <p>A custom vLLM sampler optimized for efficient next-token probability calculations.</p> <p>This sampler replaces vLLM's default sampling mechanism to optimize for scenarios where we only need the next token probabilities without actually sampling tokens.</p> Note <p>While this sampler implements vLLM's expected interface, it intentionally avoids actual token sampling to optimize for probability calculation use cases. It should not be used in scenarios where actual token generation is needed.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>class DeferredSampler(torch.nn.Module):\n    \"\"\"A custom vLLM sampler optimized for efficient next-token probability calculations.\n\n    This sampler replaces vLLM's default sampling mechanism to optimize for scenarios\n    where we only need the next token probabilities without actually sampling tokens.\n\n    Note:\n        While this sampler implements vLLM's expected interface, it intentionally\n        avoids actual token sampling to optimize for probability calculation use cases.\n        It should not be used in scenarios where actual token generation is needed.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, logits, sampling_metadata):\n        \"\"\"Process model logits to create vLLM-compatible sampling outputs.\n\n        This method implements the required vLLM sampler interface but optimizes for\n        probability requests.\n\n        Args:\n            logits (torch.Tensor): Raw model logits with shape (num_tokens, vocab_size).\n            sampling_metadata: vLLM metadata containing sequence grouping information.\n\n        Returns:\n            SamplerOutput: A vLLM-compatible output structure containing:\n                - Sequence group outputs with lazy probability dictionaries\n                - Placeholder values for unused sampling fields\n                - No actual sampled tokens (uses dummy token_id=0)\n\n        Note:\n            The sampler uses token_id=0 as a placeholder.\n        \"\"\"\n        assert logits is not None\n\n        logprobs = logits.log_softmax(dim=-1, dtype=torch.float)\n\n        sample_idx = 0\n        sampler_output = []\n        for seq_group in sampling_metadata.seq_groups:\n            seq_ids = seq_group.seq_ids\n            num_parent_seqs = len(seq_ids)\n            logprobs_by_seq = logprobs[sample_idx : sample_idx + num_parent_seqs]\n\n            assert len(logprobs_by_seq) == len(seq_ids)\n\n            seq_outputs = []\n            for seq_id, seq_logprobs in zip(seq_ids, logprobs_by_seq):\n                seq_outputs.append(\n                    SequenceOutput(seq_id, 0, LazyLogprobDict(seq_logprobs))\n                )\n\n            sampler_output.append(\n                CompletionSequenceGroupOutput(samples=seq_outputs, prompt_logprobs=[])\n            )\n\n            sample_idx += 1\n\n        sampler_outputs = SamplerOutput(\n            outputs=sampler_output,\n            sampled_token_probs=None,\n            sampled_token_ids=None,\n            logprobs=None,\n            deferred_sample_results_args=None,\n        )\n\n        return sampler_outputs\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.DeferredSampler.forward","title":"<code>forward(logits, sampling_metadata)</code>","text":"<p>Process model logits to create vLLM-compatible sampling outputs.</p> <p>This method implements the required vLLM sampler interface but optimizes for probability requests.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Raw model logits with shape (num_tokens, vocab_size).</p> required <code>sampling_metadata</code> <p>vLLM metadata containing sequence grouping information.</p> required <p>Returns:</p> Name Type Description <code>SamplerOutput</code> <p>A vLLM-compatible output structure containing: - Sequence group outputs with lazy probability dictionaries - Placeholder values for unused sampling fields - No actual sampled tokens (uses dummy token_id=0)</p> Note <p>The sampler uses token_id=0 as a placeholder.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>def forward(self, logits, sampling_metadata):\n    \"\"\"Process model logits to create vLLM-compatible sampling outputs.\n\n    This method implements the required vLLM sampler interface but optimizes for\n    probability requests.\n\n    Args:\n        logits (torch.Tensor): Raw model logits with shape (num_tokens, vocab_size).\n        sampling_metadata: vLLM metadata containing sequence grouping information.\n\n    Returns:\n        SamplerOutput: A vLLM-compatible output structure containing:\n            - Sequence group outputs with lazy probability dictionaries\n            - Placeholder values for unused sampling fields\n            - No actual sampled tokens (uses dummy token_id=0)\n\n    Note:\n        The sampler uses token_id=0 as a placeholder.\n    \"\"\"\n    assert logits is not None\n\n    logprobs = logits.log_softmax(dim=-1, dtype=torch.float)\n\n    sample_idx = 0\n    sampler_output = []\n    for seq_group in sampling_metadata.seq_groups:\n        seq_ids = seq_group.seq_ids\n        num_parent_seqs = len(seq_ids)\n        logprobs_by_seq = logprobs[sample_idx : sample_idx + num_parent_seqs]\n\n        assert len(logprobs_by_seq) == len(seq_ids)\n\n        seq_outputs = []\n        for seq_id, seq_logprobs in zip(seq_ids, logprobs_by_seq):\n            seq_outputs.append(\n                SequenceOutput(seq_id, 0, LazyLogprobDict(seq_logprobs))\n            )\n\n        sampler_output.append(\n            CompletionSequenceGroupOutput(samples=seq_outputs, prompt_logprobs=[])\n        )\n\n        sample_idx += 1\n\n    sampler_outputs = SamplerOutput(\n        outputs=sampler_output,\n        sampled_token_probs=None,\n        sampled_token_ids=None,\n        logprobs=None,\n        deferred_sample_results_args=None,\n    )\n\n    return sampler_outputs\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm/#genlm_backend.llm.vllm.LazyLogprobDict","title":"<code>LazyLogprobDict</code>","text":"<p>An efficient dictionary-like interface required by vLLM's output processing.</p> <p>vLLM's output processor expects token probabilities to be provided as a dictionary mapping token IDs to Logprob objects. However, creating this full dictionary is computationally expensive, especially when dealing with large vocabulary sizes (often 50k+ tokens).</p> <p>This class provides a compatible interface that satisfies vLLM's requirements while avoiding the overhead.</p> Source code in <code>genlm_backend/llm/vllm.py</code> <pre><code>class LazyLogprobDict:\n    \"\"\"An efficient dictionary-like interface required by vLLM's output processing.\n\n    vLLM's output processor expects token probabilities to be provided as a dictionary\n    mapping token IDs to Logprob objects. However, creating this full dictionary is\n    computationally expensive, especially when dealing with large vocabulary sizes\n    (often 50k+ tokens).\n\n    This class provides a compatible interface that satisfies vLLM's requirements while\n    avoiding the overhead.\n    \"\"\"\n\n    def __init__(self, logprobs):\n        self.logprobs = logprobs\n\n    def __getitem__(self, key):\n        if 0 &lt;= key &lt; len(self.logprobs):\n            return Logprob(self.logprobs[key])\n        raise KeyError(key)\n\n    def __contains__(self, key):\n        return 0 &lt;= key &lt; len(self.logprobs)\n\n    def __len__(self):\n        return len(self.logprobs)\n\n    def items(self):\n        return ((i, Logprob(prob)) for i, prob in enumerate(self.logprobs))\n\n    def keys(self):\n        return range(len(self.logprobs))\n\n    def values(self):\n        return iter(map(Logprob, self.logprobs))\n\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default\n</code></pre>"},{"location":"reference/genlm_backend/llm/vllm_reference/","title":"vllm_reference","text":""},{"location":"reference/genlm_backend/llm/vllm_reference/#genlm_backend.llm.vllm_reference.ReferenceVirtualLM","title":"<code>ReferenceVirtualLM</code>","text":"<p>Reference vLLM implementation used for testing. Synchronous and significantly slower than AsyncVirtualLM (~15x slower).</p> Source code in <code>genlm_backend/llm/vllm_reference.py</code> <pre><code>class ReferenceVirtualLM:\n    \"\"\"Reference vLLM implementation used for testing. Synchronous and significantly slower than AsyncVirtualLM (~15x slower).\"\"\"\n\n    def __init__(self, llm):\n        self.llm = llm\n        self.tokenizer = llm.llm_engine.get_tokenizer()\n        self.byte_vocab, self.str_vocab = decode_vocab(self.tokenizer)\n        self.vocab_length = len(self.byte_vocab)\n        self.llm.llm_engine.get_model_config().max_logprobs = self.vocab_length\n        self.DEFAULT_SAMPLING_PARAMS = SamplingParams(\n            max_tokens=1,\n            n=1,\n            logprobs=self.vocab_length,\n            detokenize=False,\n            stop=None,\n            ignore_eos=True,\n        )\n\n        self.llm.llm_engine.log_stats = False\n\n    @classmethod\n    def from_name(cls, model_name, llm_opts=None):\n        if not HAS_VLLM:\n            raise ImportError(\"vLLM not installed.\")\n        llm_opts = {\n            \"enable_prefix_caching\": True,\n            \"disable_log_stats\": True,\n            **(llm_opts or {}),\n        }\n        llm = LLM(model=model_name, tokenizer=model_name, **llm_opts)\n        return cls(llm)\n\n    def next_token_logprobs_sync(self, token_ids):\n        outputs = self.llm.generate(\n            prompts=TokensPrompt(prompt_token_ids=token_ids),\n            sampling_params=self.DEFAULT_SAMPLING_PARAMS,\n            use_tqdm=False,\n        )\n        logprobs = np.array(\n            [\n                outputs[0].outputs[0].logprobs[0][i].logprob\n                for i in range(self.vocab_length)\n            ]\n        )\n        return logprobs\n\n    async def next_token_logprobs(self, token_ids):\n        # Note: async method only to support protocol, actual implementation is synchronous\n        return self.next_token_logprobs_sync(token_ids)\n\n    async def batch_next_token_logprobs(self, token_ids_list):\n        # Note: async method only to support protocol, actual implementation is synchronous\n        prompts = [\n            TokensPrompt(prompt_token_ids=token_ids) for token_ids in token_ids_list\n        ]\n        outputs = self.llm.generate(\n            prompts=prompts,\n            sampling_params=self.DEFAULT_SAMPLING_PARAMS,\n            use_tqdm=False,\n        )\n        logprobs = np.array(\n            [\n                [\n                    out.outputs[0].logprobs[0][i].logprob\n                    for i in range(self.vocab_length)\n                ]\n                for out in outputs\n            ]\n        )\n        return logprobs\n\n    def __del__(self):\n        if llm_engine := getattr(self.llm, \"llm_engine\"):\n            if executor := getattr(llm_engine, \"model_executor\"):\n                destroy_model_parallel()\n                destroy_distributed_environment()\n                del executor\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/__init__/","title":"tokenization","text":""},{"location":"reference/genlm_backend/tokenization/__init__/#genlm_backend.tokenization.decode_vocab","title":"<code>decode_vocab(tokenizer, byte2str_fallback='tokenizer')</code>","text":"<p>Convert tokenizer vocabulary into byte and string representations.</p> Warning <p>The byte representation is the canonical form. The string representation is provided for convenience but may not decode properly for all tokens, especially those containing invalid UTF-8 sequences.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte2str_fallback</code> <code>str</code> <p>Strategy for converting invalid UTF-8 bytes to strings. Options:</p> <ul> <li>'tokenizer': Use tokenizer's <code>convert_ids_to_tokens</code> (default)</li> <li>'latin1': Decode using latin1 encoding</li> <li>'replace': Use Unicode replacement character '\ufffd'</li> </ul> <code>'tokenizer'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(byte_vocab, str_vocab)</p> Source code in <code>genlm_backend/tokenization/vocab.py</code> <pre><code>def decode_vocab(tokenizer, byte2str_fallback=\"tokenizer\"):\n    \"\"\"Convert tokenizer vocabulary into byte and string representations.\n\n    Warning:\n        The byte representation is the canonical form. The string representation is provided for\n        convenience but may not decode properly for all tokens, especially those containing invalid UTF-8 sequences.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte2str_fallback (str): Strategy for converting invalid UTF-8 bytes to strings. Options:\\n\n            - 'tokenizer': Use tokenizer's `convert_ids_to_tokens` (default)\n            - 'latin1': Decode using latin1 encoding\n            - 'replace': Use Unicode replacement character '\ufffd'\n\n    Returns:\n        (tuple): (byte_vocab, str_vocab)\n    \"\"\"\n    if byte2str_fallback not in [\"latin1\", \"tokenizer\", \"replace\"]:\n        raise ValueError(f\"Unknown byte2str_fallback strategy: {byte2str_fallback}\")\n\n    if tokenizer.is_fast:\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer.name_or_path, use_fast=False\n        )\n\n    # Try slow tokenizer.\n    try:\n        byte_vocab = get_byte_vocab(tokenizer)\n    except ByteVocabError:\n        # warnings.warn(\"Could not decode vocabulary from slow tokenizer. Trying using fast tokenizer.\")\n\n        # Try fast tokenizer.\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer.name_or_path, use_fast=True)\n        try:\n            byte_vocab = get_byte_vocab(tokenizer)\n        except ByteVocabError as e:\n            raise ValueError(\n                f\"Could not decode byte representation of token vocabuary from tokenizer {tokenizer.name_or_path}\"\n            ) from e\n\n    str_vocab = bytes_to_strs(tokenizer, byte_vocab, byte2str_fallback)\n\n    return byte_vocab, str_vocab\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/bytes/","title":"bytes","text":"<p>Functions to get the byte vocabulary from a HuggingFace tokenizer</p>"},{"location":"reference/genlm_backend/tokenization/bytes/#genlm_backend.tokenization.bytes._bytes_to_unicode","title":"<code>_bytes_to_unicode()</code>","text":"<p>Create a mapping from bytes to Unicode characters.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Mapping from byte values to Unicode characters</p> Source code in <code>genlm_backend/tokenization/bytes.py</code> <pre><code>def _bytes_to_unicode():\n    \"\"\"Create a mapping from bytes to Unicode characters.\n\n    Returns:\n        (dict): Mapping from byte values to Unicode characters\n    \"\"\"\n    bs = (\n        list(range(ord(\"!\"), ord(\"~\") + 1))\n        + list(range(ord(\"\u00a1\"), ord(\"\u00ac\") + 1))\n        + list(range(ord(\"\u00ae\"), ord(\"\u00ff\") + 1))\n    )\n    cs = bs[:]\n    n = 0\n    for b in range(256):\n        if b not in bs:\n            bs.append(b)\n            cs.append(256 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/bytes/#genlm_backend.tokenization.bytes._check_byte_decoder_has_all_bytes","title":"<code>_check_byte_decoder_has_all_bytes(tokenizer, byte_decoder)</code>","text":"<p>Verify byte decoder contains mappings for all bytes in vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte_decoder</code> <code>dict</code> <p>Dictionary mapping characters to bytes</p> required <p>Raises:</p> Type Description <code>ByteDecoderError</code> <p>If byte decoder is missing required bytes</p> Source code in <code>genlm_backend/tokenization/bytes.py</code> <pre><code>def _check_byte_decoder_has_all_bytes(tokenizer, byte_decoder):\n    \"\"\"Verify byte decoder contains mappings for all bytes in vocabulary.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte_decoder (dict): Dictionary mapping characters to bytes\n\n    Raises:\n        ByteDecoderError: If byte decoder is missing required bytes\n    \"\"\"\n    all_bytes = set()\n    for x in tokenizer.get_vocab().keys():\n        for y in x:\n            all_bytes.add(y)\n    if not set(byte_decoder.keys()) &gt;= all_bytes:\n        raise ByteDecoderError(\n            f\"Byte decoder is missing bytes: {all_bytes - set(byte_decoder.keys())}\"\n        )\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/bytes/#genlm_backend.tokenization.bytes._check_complex_roundtrip","title":"<code>_check_complex_roundtrip(tokenizer, byte_decoder)</code>","text":"<p>Test byte decoder by round-trip encoding/decoding complex characters.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte_decoder</code> <code>dict</code> <p>Dictionary mapping characters to bytes</p> required <p>Raises:</p> Type Description <code>ByteDecoderError</code> <p>If round-trip conversion fails</p> Source code in <code>genlm_backend/tokenization/bytes.py</code> <pre><code>def _check_complex_roundtrip(tokenizer, byte_decoder):\n    \"\"\"Test byte decoder by round-trip encoding/decoding complex characters.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte_decoder (dict): Dictionary mapping characters to bytes\n\n    Raises:\n        ByteDecoderError: If round-trip conversion fails\n    \"\"\"\n    s = \"\u2019\u2022\u00b6\u2202\u0192\u02d9\u2206\u00a3\u0126\u7228\u0d60\u1158\u2230\u1368\"\n    reconstructed = b\"\"\n    try:\n        input_ids = tokenizer(s)[\"input_ids\"]\n        for i in input_ids:\n            nxt_bytes = []\n            token_str = tokenizer.convert_ids_to_tokens(i)\n            for c in token_str:\n                nxt_bytes.append(byte_decoder[c])\n            reconstructed += bytes(nxt_bytes)\n\n        if (\n            hasattr(tokenizer, \"bos_token\")\n            and tokenizer.bos_token\n            and reconstructed.startswith(tokenizer.bos_token.encode())\n        ):\n            reconstructed = reconstructed[len(tokenizer.bos_token) :]\n    except Exception as e:\n        raise ByteDecoderError(\n            f\"The tokenizer being used is unable to convert a special character in {s}.\"\n        ) from e\n\n    if reconstructed.decode() != s:\n        raise ByteDecoderError(\n            f\"Failed to reconstruct the string {s} from the tokenizer's byte_decoder: {reconstructed.decode()!r} != {s!r}\"\n        )\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/bytes/#genlm_backend.tokenization.bytes._get_default_byte_decoder","title":"<code>_get_default_byte_decoder()</code>","text":"<p>Get the default GPT-2 byte decoder with additional special character mappings.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Mapping from characters to bytes including special characters</p> Source code in <code>genlm_backend/tokenization/bytes.py</code> <pre><code>def _get_default_byte_decoder():\n    \"\"\"Get the default GPT-2 byte decoder with additional special character mappings.\n\n    Returns:\n        (dict): Mapping from characters to bytes including special characters\n    \"\"\"\n    byte_decoder = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=False).byte_decoder\n    byte_decoder.update(\n        {\n            \" \": 32,\n            \"\\n\": 10,\n            \"\\r\": 13,\n            \"\\t\": 9,\n            \"\u2581\": 32,\n        }\n    )\n    return byte_decoder\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/bytes/#genlm_backend.tokenization.bytes.check_byte_decoder","title":"<code>check_byte_decoder(tokenizer, byte_decoder)</code>","text":"<p>Verify that a byte decoder can properly handle all tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte_decoder</code> <code>dict</code> <p>Dictionary mapping characters to bytes</p> required <p>Raises:</p> Type Description <code>ByteDecoderError</code> <p>If byte decoder fails validation checks</p> Source code in <code>genlm_backend/tokenization/bytes.py</code> <pre><code>def check_byte_decoder(tokenizer, byte_decoder):\n    \"\"\"Verify that a byte decoder can properly handle all tokens.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte_decoder (dict): Dictionary mapping characters to bytes\n\n    Raises:\n        ByteDecoderError: If byte decoder fails validation checks\n    \"\"\"\n    _check_byte_decoder_has_all_bytes(tokenizer, byte_decoder)\n    _check_complex_roundtrip(tokenizer, byte_decoder)\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/bytes/#genlm_backend.tokenization.bytes.get_byte_tokens_by_encoding_token_strings","title":"<code>get_byte_tokens_by_encoding_token_strings(tokenizer)</code>","text":"<p>Convert tokens to bytes by encoding token strings directly.</p> <p>This function attempts to convert each token in the vocabulary to its byte representation by directly encoding the token strings. It handles special tokens separately and has multiple fallback strategies for encoding regular tokens:</p> <ol> <li>For special tokens, uses the string representation from the tokenizer's added vocab</li> <li>For regular tokens:     a. If the token is already bytes, uses it directly     b. If the token is a string and the tokenizer has convert_tokens_to_string:         - Converts single token to string         - Verifies roundtrip encoding matches original token ID         - Falls back to byte decoder if roundtrip fails     c. If the token is a string without convert_tokens_to_string:         - Directly encodes the token string</li> </ol> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance.</p> required <p>Returns:</p> Name Type Description <code>byte_tokens</code> <code>list[byte]</code> <p>List of byte representations for each token in the vocabulary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If token encoding fails (roundtrip produces multiple tokens), or if        a token has an unexpected type (not str or bytes).</p> Source code in <code>genlm_backend/tokenization/bytes.py</code> <pre><code>def get_byte_tokens_by_encoding_token_strings(tokenizer):\n    \"\"\"Convert tokens to bytes by encoding token strings directly.\n\n    This function attempts to convert each token in the vocabulary to its byte representation\n    by directly encoding the token strings. It handles special tokens separately and has\n    multiple fallback strategies for encoding regular tokens:\n\n    1. For special tokens, uses the string representation from the tokenizer's added vocab\n    2. For regular tokens:\n        a. If the token is already bytes, uses it directly\n        b. If the token is a string and the tokenizer has convert_tokens_to_string:\n            - Converts single token to string\n            - Verifies roundtrip encoding matches original token ID\n            - Falls back to byte decoder if roundtrip fails\n        c. If the token is a string without convert_tokens_to_string:\n            - Directly encodes the token string\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance.\n\n    Returns:\n        byte_tokens (list[byte]): List of byte representations for each token in the vocabulary.\n\n    Raises:\n        ValueError: If token encoding fails (roundtrip produces multiple tokens), or if\n                   a token has an unexpected type (not str or bytes).\n    \"\"\"\n    byte_tokens = [b\"\"] * len(tokenizer)\n    special_tokens_map = {\n        id: token for token, id in tokenizer.get_added_vocab().items()\n    }\n    byte_encoder = _bytes_to_unicode()\n    byte_decoder = {v: k for k, v in byte_encoder.items()}\n\n    for i in range(len(tokenizer)):\n        if i in special_tokens_map:\n            byte_coded = special_tokens_map[i].encode()\n        else:\n            token = tokenizer.convert_ids_to_tokens(i)\n            if isinstance(token, bytes):\n                byte_coded = token\n            elif isinstance(token, str):\n                if hasattr(tokenizer, \"convert_tokens_to_string\"):\n                    token_str = tokenizer.convert_tokens_to_string([token])\n                    encoded_str = tokenizer.encode(token_str)\n                    if len(encoded_str) != 1:\n                        raise ValueError(\n                            f\"Round-trip encoding of tokens [{token}] failed! Got {encoded_str}\"\n                        )\n                    roundtrip_id = encoded_str[0]\n                    if roundtrip_id == i:\n                        byte_coded = token_str.encode()\n                    else:\n                        byte_coded = bytes([byte_decoder[c] for c in token])\n                else:\n                    byte_coded = token.encode()\n            else:\n                raise ValueError(f\"Unexpected token type: {type(token)}\")\n        byte_tokens[i] = byte_coded\n\n    return byte_tokens\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/bytes/#genlm_backend.tokenization.bytes.get_byte_tokens_from_byte_decoder","title":"<code>get_byte_tokens_from_byte_decoder(tokenizer, byte_decoder)</code>","text":"<p>Convert tokens to bytes using a byte decoder mapping.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte_decoder</code> <code>dict</code> <p>Dictionary mapping characters to bytes</p> required <p>Returns:</p> Name Type Description <code>byte_tokens</code> <code>list[byte]</code> <p>List of byte representations for each token</p> Source code in <code>genlm_backend/tokenization/bytes.py</code> <pre><code>def get_byte_tokens_from_byte_decoder(tokenizer, byte_decoder):\n    \"\"\"Convert tokens to bytes using a byte decoder mapping.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte_decoder (dict): Dictionary mapping characters to bytes\n\n    Returns:\n        byte_tokens (list[byte]): List of byte representations for each token\n    \"\"\"\n    byte_tokens = [\n        bytes([byte_decoder[b] for b in tokenizer.convert_ids_to_tokens(i)])\n        for i in range(len(tokenizer))\n    ]\n    return byte_tokens\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/bytes/#genlm_backend.tokenization.bytes.get_byte_tokens_from_sp","title":"<code>get_byte_tokens_from_sp(tokenizer)</code>","text":"<p>Convert tokens to their byte representations using a SentencePiece model.</p> <p>Uses the SentencePiece model's id_to_piece method to get the raw byte representation of each token, handling special tokens separately. Converts any hex-encoded bytes (in &lt;0xXX&gt; format) to their actual byte values and replaces the SentencePiece prefix space marker with a regular space.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance with a SentencePiece model</p> required <p>Returns:</p> Name Type Description <code>byte_tokens</code> <code>list[byte]</code> <p>List of byte representations for each token in the vocabulary</p> Note <p>Special tokens are handled by directly encoding their string representation, while normal tokens go through the SentencePiece conversion process.</p> Source code in <code>genlm_backend/tokenization/bytes.py</code> <pre><code>def get_byte_tokens_from_sp(tokenizer):\n    \"\"\"Convert tokens to their byte representations using a SentencePiece model.\n\n    Uses the SentencePiece model's id_to_piece method to get the raw byte representation\n    of each token, handling special tokens separately. Converts any hex-encoded bytes\n    (in &lt;0xXX&gt; format) to their actual byte values and replaces the SentencePiece\n    prefix space marker with a regular space.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance with a SentencePiece model\n\n    Returns:\n        byte_tokens (list[byte]): List of byte representations for each token in the vocabulary\n\n    Note:\n        Special tokens are handled by directly encoding their string representation,\n        while normal tokens go through the SentencePiece conversion process.\n    \"\"\"\n    special_tokens_map = {\n        token_id: token for token, token_id in tokenizer.get_added_vocab().items()\n    }\n    byte_tokens = [b\"\"] * len(tokenizer)\n    prefix_space = \"\u2581\".encode()\n    for i in range(len(tokenizer)):\n        if i in special_tokens_map:\n            byte_coded = special_tokens_map[i].encode()\n        else:\n            byte_coded = re.sub(\n                rb\"&lt;0x(..)&gt;\",\n                lambda x: bytes.fromhex(x[1].decode()),\n                tokenizer.sp_model.id_to_piece(i).encode(),\n            )\n        byte_tokens[i] = byte_coded.replace(prefix_space, b\" \")\n    return byte_tokens\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/bytes/#genlm_backend.tokenization.bytes.get_byte_vocab","title":"<code>get_byte_vocab(tokenizer)</code>","text":"<p>Extract byte vocabulary from a tokenizer using various methods.</p> <p>This function attempts to extract the byte representation of each token in the vocabulary using multiple methods, trying each in sequence until one succeeds:</p> <ol> <li>If the tokenizer has a byte_decoder attribute, attempt to use that directly</li> <li>If the tokenizer has an sp_model (SentencePiece) attribute, use that</li> <li>Try encoding the token strings directly</li> <li>Fall back to using the default GPT2 byte decoder</li> </ol> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance.</p> required <p>Returns:</p> Type Description <code>list[byte]</code> <p>List of byte representations of tokens.</p> <p>Raises:</p> Type Description <code>ByteVocabError</code> <p>If vocabulary cannot be decoded using any of the available methods.</p> Source code in <code>genlm_backend/tokenization/bytes.py</code> <pre><code>def get_byte_vocab(tokenizer):\n    \"\"\"Extract byte vocabulary from a tokenizer using various methods.\n\n    This function attempts to extract the byte representation of each token in the vocabulary\n    using multiple methods, trying each in sequence until one succeeds:\n\n    1. If the tokenizer has a byte_decoder attribute, attempt to use that directly\n    2. If the tokenizer has an sp_model (SentencePiece) attribute, use that\n    3. Try encoding the token strings directly\n    4. Fall back to using the default GPT2 byte decoder\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance.\n\n    Returns:\n        (list[byte]): List of byte representations of tokens.\n\n    Raises:\n        ByteVocabError: If vocabulary cannot be decoded using any of the available methods.\n    \"\"\"\n    # Try byte decoder.\n    if hasattr(tokenizer, \"byte_decoder\"):\n        try:\n            byte_decoder = tokenizer.byte_decoder\n            check_byte_decoder(tokenizer, byte_decoder)\n            return get_byte_tokens_from_byte_decoder(tokenizer, byte_decoder)\n        except ByteDecoderError:\n            pass\n            # warnings.warn(f\"Could not decode vocabulary using byte_decoder: {e!r}\")\n\n    # Try SentencePiece model.\n    if hasattr(tokenizer, \"sp_model\"):\n        return get_byte_tokens_from_sp(tokenizer)\n\n    # Try through token encoding.\n    try:\n        return get_byte_tokens_by_encoding_token_strings(tokenizer)\n    except Exception:\n        # warnings.warn(f\"Could not decode vocabulary through string encoding: {e!r}\")\n        pass\n\n    # Try using GPT2 byte decoder.\n    try:\n        byte_decoder = _get_default_byte_decoder()\n        check_byte_decoder(tokenizer, byte_decoder)\n        return get_byte_tokens_from_byte_decoder(tokenizer, byte_decoder)\n    except ByteDecoderError as e:\n        raise ByteVocabError(\n            \"Could not decode vocabulary by falling back to GPT2 byte decoder.\"\n        ) from e\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/vocab/","title":"vocab","text":"<p>Functions to get and check HuggingFace tokenizer vocabularies</p>"},{"location":"reference/genlm_backend/tokenization/vocab/#genlm_backend.tokenization.vocab.assert_roundtrip","title":"<code>assert_roundtrip(test_case, tokenizer, vocab, vocab_type)</code>","text":"<p>Assert that encoding and decoding a test case matches the tokenizer's output.</p> <p>A unified function that handles both string and byte vocabularies.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>str</code> <p>String to test encoding/decoding roundtrip</p> required <code>tokenizer</code> <p>Hugging Face tokenizer instance</p> required <code>vocab</code> <code>list</code> <p>List of token representations (either strings or bytes)</p> required <code>vocab_type</code> <code>str</code> <p>Type of vocabulary - either 'str' or 'byte'</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the roundtrip result doesn't match tokenizer's direct decoding</p> <code>ValueError</code> <p>If vocab_type is not 'str' or 'byte'</p> Source code in <code>genlm_backend/tokenization/vocab.py</code> <pre><code>def assert_roundtrip(test_case, tokenizer, vocab, vocab_type):\n    \"\"\"Assert that encoding and decoding a test case matches the tokenizer's output.\n\n    A unified function that handles both string and byte vocabularies.\n\n    Args:\n        test_case (str): String to test encoding/decoding roundtrip\n        tokenizer: Hugging Face tokenizer instance\n        vocab (list): List of token representations (either strings or bytes)\n        vocab_type (str): Type of vocabulary - either 'str' or 'byte'\n\n    Raises:\n        AssertionError: If the roundtrip result doesn't match tokenizer's direct decoding\n        ValueError: If vocab_type is not 'str' or 'byte'\n    \"\"\"\n    with turn_off_space_cleaning(tokenizer):\n        encd = tokenizer.encode(test_case)\n\n        if vocab_type == \"str\":\n            have = \"\".join([vocab[i] for i in encd])\n        elif vocab_type == \"byte\":\n            have = b\"\".join([vocab[i] for i in encd]).decode(\"utf-8\")\n        else:\n            raise ValueError(\n                f\"Invalid vocab_type: {vocab_type}. Must be 'str' or 'byte'.\"\n            )\n\n        want = tokenizer.decode(encd)\n\n        if have != want:\n            pos = next(\n                (i for i in range(min(len(have), len(want))) if have[i] != want[i]),\n                min(len(have), len(want)),\n            )\n            context = 20\n\n            error_msg = (\n                f\"\\nRoundtrip assertion failed for {vocab_type} vocabulary:\"\n                f\"\\nMismatch at position {pos}\"\n                f\"\\nHave: ...{repr(have[max(0, pos - context) : pos + context])}...\"\n                f\"\\nWant: ...{repr(want[max(0, pos - context) : pos + context])}...\"\n            )\n\n            raise AssertionError(error_msg)\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/vocab/#genlm_backend.tokenization.vocab.assert_roundtrip_bytes","title":"<code>assert_roundtrip_bytes(test_case, tokenizer, byte_vocab)</code>","text":"<p>Assert that encoding and decoding a test case using byte vocabulary matches the tokenizer's output.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>str</code> <p>String to test encoding/decoding roundtrip</p> required <code>tokenizer</code> <p>Hugging Face tokenizer instance</p> required <code>byte_vocab</code> <code>list</code> <p>List of byte representations of tokens</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the roundtrip result doesn't match tokenizer's direct decoding</p> Source code in <code>genlm_backend/tokenization/vocab.py</code> <pre><code>def assert_roundtrip_bytes(test_case, tokenizer, byte_vocab):\n    \"\"\"Assert that encoding and decoding a test case using byte vocabulary matches the tokenizer's output.\n\n    Args:\n        test_case (str): String to test encoding/decoding roundtrip\n        tokenizer: Hugging Face tokenizer instance\n        byte_vocab (list): List of byte representations of tokens\n\n    Raises:\n        AssertionError: If the roundtrip result doesn't match tokenizer's direct decoding\n    \"\"\"\n    return assert_roundtrip(test_case, tokenizer, byte_vocab, vocab_type=\"byte\")\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/vocab/#genlm_backend.tokenization.vocab.assert_roundtrip_strs","title":"<code>assert_roundtrip_strs(test_case, tokenizer, str_vocab)</code>","text":"<p>Assert that encoding and decoding a test case using string vocabulary matches the tokenizer's output.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>str</code> <p>String to test encoding/decoding roundtrip</p> required <code>tokenizer</code> <p>Hugging Face tokenizer instance</p> required <code>str_vocab</code> <code>list</code> <p>List of string representations of tokens</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the roundtrip result doesn't match tokenizer's direct decoding</p> Source code in <code>genlm_backend/tokenization/vocab.py</code> <pre><code>def assert_roundtrip_strs(test_case, tokenizer, str_vocab):\n    \"\"\"Assert that encoding and decoding a test case using string vocabulary matches the tokenizer's output.\n\n    Args:\n        test_case (str): String to test encoding/decoding roundtrip\n        tokenizer: Hugging Face tokenizer instance\n        str_vocab (list): List of string representations of tokens\n\n    Raises:\n        AssertionError: If the roundtrip result doesn't match tokenizer's direct decoding\n    \"\"\"\n    return assert_roundtrip(test_case, tokenizer, str_vocab, vocab_type=\"str\")\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/vocab/#genlm_backend.tokenization.vocab.bytes_to_strs","title":"<code>bytes_to_strs(tokenizer, byte_vocab, byte2str_fallback)</code>","text":"<p>Convert byte representations to UTF-8 strings.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte_vocab</code> <code>list[bytes]</code> <p>List of byte representations of tokens</p> required <code>byte2str_fallback</code> <code>str</code> <p>Strategy for converting invalid UTF-8 bytes to strings: - 'tokenizer': Use tokenizer's convert_ids_to_tokens (default) - 'latin1': Decode using latin1 encoding - 'replace': Use Unicode replacement character '\ufffd'</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of string representations of tokens</p> Note <p>May produce duplicate strings for different token IDs. A warning is issued if duplicates are found.</p> Source code in <code>genlm_backend/tokenization/vocab.py</code> <pre><code>def bytes_to_strs(tokenizer, byte_vocab, byte2str_fallback):\n    \"\"\"Convert byte representations to UTF-8 strings.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte_vocab (list[bytes]): List of byte representations of tokens\n        byte2str_fallback (str): Strategy for converting invalid UTF-8 bytes to strings:\n            - 'tokenizer': Use tokenizer's convert_ids_to_tokens (default)\n            - 'latin1': Decode using latin1 encoding\n            - 'replace': Use Unicode replacement character '\ufffd'\n\n    Returns:\n        (list[str]): List of string representations of tokens\n\n    Note:\n        May produce duplicate strings for different token IDs. A warning is issued if duplicates are found.\n    \"\"\"\n    str_vocab = []\n    seen_tokens = {}\n    for token_id, raw_token in enumerate(byte_vocab):\n        try:\n            token = raw_token.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            if byte2str_fallback == \"latin1\":\n                try:\n                    token = raw_token.decode(\"latin1\")\n                except UnicodeDecodeError:\n                    token = tokenizer.convert_ids_to_tokens(token_id)\n            elif byte2str_fallback == \"tokenizer\":\n                token = tokenizer.convert_ids_to_tokens(token_id)\n            elif byte2str_fallback == \"replace\":\n                token = raw_token.decode(\"utf-8\", errors=\"replace\")\n\n        if token in seen_tokens:\n            seen_tokens[token].append(token_id)\n        else:\n            seen_tokens[token] = [token_id]\n\n        str_vocab.append(token)\n\n    duplicates = {\n        token: indices for token, indices in seen_tokens.items() if len(indices) &gt; 1\n    }\n    if duplicates:\n        warnings.warn(\n            \"Duplicate tokens found in string vocabulary. \"\n            \"This may lead to downstream issues with the string vocabulary; we recommend using the byte vocabulary.\"\n        )\n\n    return str_vocab\n</code></pre>"},{"location":"reference/genlm_backend/tokenization/vocab/#genlm_backend.tokenization.vocab.decode_vocab","title":"<code>decode_vocab(tokenizer, byte2str_fallback='tokenizer')</code>","text":"<p>Convert tokenizer vocabulary into byte and string representations.</p> Warning <p>The byte representation is the canonical form. The string representation is provided for convenience but may not decode properly for all tokens, especially those containing invalid UTF-8 sequences.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A Hugging Face tokenizer instance</p> required <code>byte2str_fallback</code> <code>str</code> <p>Strategy for converting invalid UTF-8 bytes to strings. Options:</p> <ul> <li>'tokenizer': Use tokenizer's <code>convert_ids_to_tokens</code> (default)</li> <li>'latin1': Decode using latin1 encoding</li> <li>'replace': Use Unicode replacement character '\ufffd'</li> </ul> <code>'tokenizer'</code> <p>Returns:</p> Type Description <code>tuple</code> <p>(byte_vocab, str_vocab)</p> Source code in <code>genlm_backend/tokenization/vocab.py</code> <pre><code>def decode_vocab(tokenizer, byte2str_fallback=\"tokenizer\"):\n    \"\"\"Convert tokenizer vocabulary into byte and string representations.\n\n    Warning:\n        The byte representation is the canonical form. The string representation is provided for\n        convenience but may not decode properly for all tokens, especially those containing invalid UTF-8 sequences.\n\n    Args:\n        tokenizer: A Hugging Face tokenizer instance\n        byte2str_fallback (str): Strategy for converting invalid UTF-8 bytes to strings. Options:\\n\n            - 'tokenizer': Use tokenizer's `convert_ids_to_tokens` (default)\n            - 'latin1': Decode using latin1 encoding\n            - 'replace': Use Unicode replacement character '\ufffd'\n\n    Returns:\n        (tuple): (byte_vocab, str_vocab)\n    \"\"\"\n    if byte2str_fallback not in [\"latin1\", \"tokenizer\", \"replace\"]:\n        raise ValueError(f\"Unknown byte2str_fallback strategy: {byte2str_fallback}\")\n\n    if tokenizer.is_fast:\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer.name_or_path, use_fast=False\n        )\n\n    # Try slow tokenizer.\n    try:\n        byte_vocab = get_byte_vocab(tokenizer)\n    except ByteVocabError:\n        # warnings.warn(\"Could not decode vocabulary from slow tokenizer. Trying using fast tokenizer.\")\n\n        # Try fast tokenizer.\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer.name_or_path, use_fast=True)\n        try:\n            byte_vocab = get_byte_vocab(tokenizer)\n        except ByteVocabError as e:\n            raise ValueError(\n                f\"Could not decode byte representation of token vocabuary from tokenizer {tokenizer.name_or_path}\"\n            ) from e\n\n    str_vocab = bytes_to_strs(tokenizer, byte_vocab, byte2str_fallback)\n\n    return byte_vocab, str_vocab\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/","title":"trie","text":""},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.AsyncTokenCharacterTrie","title":"<code>AsyncTokenCharacterTrie</code>","text":"<p>An asynchronous wrapper for TokenCharacterTrie implementations that provides automatic request batching.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>class AsyncTokenCharacterTrie:\n    \"\"\"An asynchronous wrapper for TokenCharacterTrie implementations that provides automatic request batching.\"\"\"\n\n    def __init__(self, trie):\n        \"\"\"Initialize an `AsyncTokenCharacterTrie`.\n\n        Args:\n            trie (TokenCharacterTrie|ParallelTokenCharacterTrie): The underlying `TokenCharacterTrie` or `ParallelTokenCharacterTrie` instance\n        \"\"\"\n        self.trie = trie\n        self._queue = None\n        self._task = None\n\n    @classmethod\n    def from_vocab(cls, vocab, backend=\"parallel\", **kwargs):\n        \"\"\"Creates an `AsyncTokenCharacterTrie` from a vocabulary.\n\n        Args:\n            vocab (list): The vocabulary over which the trie will be defined.\n            backend (str, optional): The trie implementation to use - either 'sequential' or 'parallel'.\n                    Defaults to 'parallel' which uses GPU acceleration when available.\n            **kwargs: Additional arguments passed to the trie constructor\n\n        Returns:\n            (AsyncTokenCharacterTrie): The initialized asynchronous trie instance.\n        \"\"\"\n        if backend == \"sequential\":\n            trie = TokenCharacterTrie(decode=vocab, **kwargs)\n        elif backend == \"parallel\":\n            trie = ParallelTokenCharacterTrie(decode=vocab, **kwargs)\n        else:\n            raise ValueError(\n                f\"Unknown backend: {backend}. Must be one of ['sequential', 'parallel']\"\n            )\n        return cls(trie)\n\n    async def _queue_request(self, request, op):\n        if not self._task or self._task.done():\n            self.start()\n\n        future = asyncio.Future()\n        await self._queue.put((request, future, op))\n        return future\n\n    async def weight_sum(self, ws):\n        \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n        together.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n        Returns:\n            (np.ndarray): The calculated mass sums for the given distribution.\n        \"\"\"\n        future = await self._queue_request(ws, \"sum\")\n        result = await future\n        return result\n\n    async def weight_max(self, ws):\n        \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n        together.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n        Returns:\n            (np.ndarray): The calculated max weights for the given distribution.\n        \"\"\"\n        future = await self._queue_request(ws, \"max\")\n        result = await future\n        return result\n\n    def start(self):\n        \"\"\"Start the background processing task if not already running.\"\"\"\n        if not self._task or self._task.done():\n            self._queue = (\n                asyncio.Queue()\n            )  # Create a new queue so that it is bound to the current event loop\n            self._task = asyncio.create_task(self._background_loop())\n\n    def _do_weight_sums(self, batch_weights):\n        return self.trie.batch_weight_sum(batch_weights)\n\n    def _do_weight_maxs(self, batch_weights):\n        return self.trie.batch_weight_max(batch_weights)\n\n    async def _background_loop(self):\n        \"\"\"Background task that processes queued weight sum and max requests.\n\n        Continuously monitors the queue for new requests and processes them in batches\n        using the underlying trie implementation.\n\n        Raises:\n            Exception: If any error occurs during processing, it is propagated to all\n                      pending futures in the current batch.\n        \"\"\"\n        while True:\n            try:\n                op_groups = defaultdict(list)\n\n                request, future, op = await self._queue.get()\n                op_groups[op].append((request, future))\n\n                while not self._queue.empty():\n                    request, future, op = await self._queue.get()\n                    op_groups[op].append((request, future))\n\n                for op, group in op_groups.items():\n                    requests, futures = zip(*group)\n\n                    if op == \"sum\":\n                        logger.debug(f\"processing {len(requests)} sum requests\")\n                        results = self._do_weight_sums(requests)\n                    elif op == \"max\":\n                        logger.debug(f\"processing {len(requests)} max requests\")\n                        results = self._do_weight_maxs(requests)\n                    else:\n                        raise ValueError(f\"Unknown operation: {op}\")\n\n                    for future, result in zip(futures, results):\n                        future.set_result(result)\n\n            except Exception as e:\n                for group in op_groups.values():\n                    for _, future in group:\n                        if not future.done():\n                            future.set_exception(e)\n                raise\n\n    async def cleanup(self):\n        \"\"\"Async cleanup - preferred method\"\"\"\n        if self._task and not self._task.done():\n            self._task.cancel()\n            try:\n                await self._task\n            except asyncio.CancelledError:\n                pass\n            self._task = None\n\n    def shutdown(self):\n        \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n        if self._task is not None:\n            try:\n                self._task.cancel()\n            except RuntimeError:\n                # Ignore runtime errors that might occur if event loop is closed\n                pass\n            self._task = None\n\n    def __del__(self):\n        self.shutdown()\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.AsyncTokenCharacterTrie.__init__","title":"<code>__init__(trie)</code>","text":"<p>Initialize an <code>AsyncTokenCharacterTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>trie</code> <code>TokenCharacterTrie | ParallelTokenCharacterTrie</code> <p>The underlying <code>TokenCharacterTrie</code> or <code>ParallelTokenCharacterTrie</code> instance</p> required Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>def __init__(self, trie):\n    \"\"\"Initialize an `AsyncTokenCharacterTrie`.\n\n    Args:\n        trie (TokenCharacterTrie|ParallelTokenCharacterTrie): The underlying `TokenCharacterTrie` or `ParallelTokenCharacterTrie` instance\n    \"\"\"\n    self.trie = trie\n    self._queue = None\n    self._task = None\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.AsyncTokenCharacterTrie._background_loop","title":"<code>_background_loop()</code>  <code>async</code>","text":"<p>Background task that processes queued weight sum and max requests.</p> <p>Continuously monitors the queue for new requests and processes them in batches using the underlying trie implementation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during processing, it is propagated to all       pending futures in the current batch.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>async def _background_loop(self):\n    \"\"\"Background task that processes queued weight sum and max requests.\n\n    Continuously monitors the queue for new requests and processes them in batches\n    using the underlying trie implementation.\n\n    Raises:\n        Exception: If any error occurs during processing, it is propagated to all\n                  pending futures in the current batch.\n    \"\"\"\n    while True:\n        try:\n            op_groups = defaultdict(list)\n\n            request, future, op = await self._queue.get()\n            op_groups[op].append((request, future))\n\n            while not self._queue.empty():\n                request, future, op = await self._queue.get()\n                op_groups[op].append((request, future))\n\n            for op, group in op_groups.items():\n                requests, futures = zip(*group)\n\n                if op == \"sum\":\n                    logger.debug(f\"processing {len(requests)} sum requests\")\n                    results = self._do_weight_sums(requests)\n                elif op == \"max\":\n                    logger.debug(f\"processing {len(requests)} max requests\")\n                    results = self._do_weight_maxs(requests)\n                else:\n                    raise ValueError(f\"Unknown operation: {op}\")\n\n                for future, result in zip(futures, results):\n                    future.set_result(result)\n\n        except Exception as e:\n            for group in op_groups.values():\n                for _, future in group:\n                    if not future.done():\n                        future.set_exception(e)\n            raise\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.AsyncTokenCharacterTrie.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Async cleanup - preferred method</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Async cleanup - preferred method\"\"\"\n    if self._task and not self._task.done():\n        self._task.cancel()\n        try:\n            await self._task\n        except asyncio.CancelledError:\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.AsyncTokenCharacterTrie.from_vocab","title":"<code>from_vocab(vocab, backend='parallel', **kwargs)</code>  <code>classmethod</code>","text":"<p>Creates an <code>AsyncTokenCharacterTrie</code> from a vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>list</code> <p>The vocabulary over which the trie will be defined.</p> required <code>backend</code> <code>str</code> <p>The trie implementation to use - either 'sequential' or 'parallel'.     Defaults to 'parallel' which uses GPU acceleration when available.</p> <code>'parallel'</code> <code>**kwargs</code> <p>Additional arguments passed to the trie constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTokenCharacterTrie</code> <p>The initialized asynchronous trie instance.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>@classmethod\ndef from_vocab(cls, vocab, backend=\"parallel\", **kwargs):\n    \"\"\"Creates an `AsyncTokenCharacterTrie` from a vocabulary.\n\n    Args:\n        vocab (list): The vocabulary over which the trie will be defined.\n        backend (str, optional): The trie implementation to use - either 'sequential' or 'parallel'.\n                Defaults to 'parallel' which uses GPU acceleration when available.\n        **kwargs: Additional arguments passed to the trie constructor\n\n    Returns:\n        (AsyncTokenCharacterTrie): The initialized asynchronous trie instance.\n    \"\"\"\n    if backend == \"sequential\":\n        trie = TokenCharacterTrie(decode=vocab, **kwargs)\n    elif backend == \"parallel\":\n        trie = ParallelTokenCharacterTrie(decode=vocab, **kwargs)\n    else:\n        raise ValueError(\n            f\"Unknown backend: {backend}. Must be one of ['sequential', 'parallel']\"\n        )\n    return cls(trie)\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.AsyncTokenCharacterTrie.shutdown","title":"<code>shutdown()</code>","text":"<p>Stop the background processing task and cleanup resources.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>def shutdown(self):\n    \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n    if self._task is not None:\n        try:\n            self._task.cancel()\n        except RuntimeError:\n            # Ignore runtime errors that might occur if event loop is closed\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.AsyncTokenCharacterTrie.start","title":"<code>start()</code>","text":"<p>Start the background processing task if not already running.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>def start(self):\n    \"\"\"Start the background processing task if not already running.\"\"\"\n    if not self._task or self._task.done():\n        self._queue = (\n            asyncio.Queue()\n        )  # Create a new queue so that it is bound to the current event loop\n        self._task = asyncio.create_task(self._background_loop())\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.AsyncTokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>  <code>async</code>","text":"<p>Queue a <code>weight_max</code> request. Multiple concurrent calls will be automatically batched together.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated max weights for the given distribution.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>async def weight_max(self, ws):\n    \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n    together.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n    Returns:\n        (np.ndarray): The calculated max weights for the given distribution.\n    \"\"\"\n    future = await self._queue_request(ws, \"max\")\n    result = await future\n    return result\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.AsyncTokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>  <code>async</code>","text":"<p>Queue a <code>weight_sum</code> request. Multiple concurrent calls will be automatically batched together.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated mass sums for the given distribution.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>async def weight_sum(self, ws):\n    \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n    together.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n    Returns:\n        (np.ndarray): The calculated mass sums for the given distribution.\n    \"\"\"\n    future = await self._queue_request(ws, \"sum\")\n    result = await future\n    return result\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.ParallelTokenCharacterTrie","title":"<code>ParallelTokenCharacterTrie</code>","text":"<p>               Bases: <code>TokenCharacterTrie</code></p> <p>A GPU-optimized version of <code>TokenCharacterTrie</code> that performs weight sum and max operations in parallel.</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>class ParallelTokenCharacterTrie(TokenCharacterTrie):\n    \"\"\"A GPU-optimized version of `TokenCharacterTrie` that performs weight sum and max operations in parallel.\"\"\"\n\n    def __init__(self, decode, device=None, **kwargs):\n        super().__init__(decode, **kwargs)\n\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        if self.device not in [\"cpu\", \"cuda\"]:\n            raise ValueError(f\"Invalid device: {device}. Must be 'cpu', 'cuda' or None\")\n\n        self._build_reachability_matrix()\n        self.token_ids = torch.tensor(\n            self.token_id_to_leaf[:, 0], dtype=torch.long, device=self.device\n        )\n\n    def _build_parent_map(self):\n        \"\"\"Builds a mapping from each node to its parent node in the trie.\n\n        Returns:\n            (dict): A dictionary where keys are child nodes and values are their parent nodes.\n        \"\"\"\n        parent = {}\n        for node in range(len(self.children)):\n            for child in self.jump[node]:\n                parent[child] = node\n        return parent\n\n    def _build_reachability_matrix(self):\n        \"\"\"Constructs a sparse reachability matrix for efficient weight propagation.\n\n        The matrix M is constructed such that M[i,j] = 1 if node j is either:\n        - The leaf node i itself (self-connection)\n        - An ancestor of leaf node i in the trie\n        \"\"\"\n        leaf_indices = self.token_id_to_leaf[:, 1]\n        parent = self._build_parent_map()\n\n        rows, cols = [], []\n        for i, node in enumerate(leaf_indices):\n            # self connections\n            rows.append(i)\n            cols.append(node)\n\n            current = node\n            while current in parent:  # Walk up to root\n                ancestor = parent[current]\n                rows.append(i)\n                cols.append(ancestor)\n                current = ancestor\n\n        self.src_indices = torch.tensor(rows, dtype=torch.long, device=self.device)\n        self.dst_indices = torch.tensor(cols, dtype=torch.long, device=self.device)\n\n        indices = torch.tensor([rows, cols], dtype=torch.long, device=self.device)\n        values = torch.ones(len(rows), device=self.device)\n\n        self.M = torch.sparse_coo_tensor(\n            indices, values, (len(leaf_indices), len(self.children))\n        ).to_sparse_csr()\n\n    def _preprocess_ws(self, batch_ws):\n        processed_batch_ws = []\n        for ws in batch_ws:\n            if not isinstance(ws, torch.Tensor):\n                ws = torch.tensor(ws, device=self.device, dtype=torch.float32)\n            elif ws.device != self.device or ws.dtype != torch.float32:\n                ws = ws.to(device=self.device, dtype=torch.float32)\n            assert ws.shape[0] == len(self.decode), [ws.shape[0], len(self.decode)]\n            processed_batch_ws.append(ws)\n        return torch.stack(processed_batch_ws)\n\n    def weight_sum(self, ws):\n        \"\"\"Computes weight sums given token weights.\n\n        For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n        that are descendants of that node. This is efficiently implemented using sparse matrix multiplication\n        with a pre-computed reachability matrix.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n        Returns:\n            (numpy.ndarray): Summed weights for each node in the trie, shape (`len(self.decode)`,).\n        \"\"\"\n        return self.batch_weight_sum(self._preprocess_ws([ws]))[0]\n\n    def batch_weight_sum(self, ws):\n        \"\"\"Batch version of `weight_sum`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n        Returns:\n            numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        masses = torch.sparse.mm(ws[:, self.token_ids], self.M)\n        return masses.cpu().numpy()\n\n    def weight_max(self, ws):\n        \"\"\"Computes the max weights given the token weights.\n\n        For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n        that are descendants of that node. This is efficiently implemented using parallel scatter_reduce\n        operations on GPU.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (`len(self.decode)`,).\n        \"\"\"\n        return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n\n    def batch_weight_max(self, ws):\n        \"\"\"Batch version of `weight_max`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n\n        # Get leaf weights\n        leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n        batch_size = leaf_weights.shape[0]\n\n        # Use scatter_reduce to propagate maximum values in parallel\n        result = torch.zeros((batch_size, len(self.children)), device=self.device)\n        result.scatter_reduce_(\n            dim=1,\n            index=self.dst_indices.expand(batch_size, -1),\n            src=leaf_weights[:, self.src_indices],\n            reduce=\"amax\",\n            include_self=False,\n        )\n\n        return result.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.ParallelTokenCharacterTrie._build_parent_map","title":"<code>_build_parent_map()</code>","text":"<p>Builds a mapping from each node to its parent node in the trie.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where keys are child nodes and values are their parent nodes.</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>def _build_parent_map(self):\n    \"\"\"Builds a mapping from each node to its parent node in the trie.\n\n    Returns:\n        (dict): A dictionary where keys are child nodes and values are their parent nodes.\n    \"\"\"\n    parent = {}\n    for node in range(len(self.children)):\n        for child in self.jump[node]:\n            parent[child] = node\n    return parent\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.ParallelTokenCharacterTrie._build_reachability_matrix","title":"<code>_build_reachability_matrix()</code>","text":"<p>Constructs a sparse reachability matrix for efficient weight propagation.</p> <p>The matrix M is constructed such that M[i,j] = 1 if node j is either: - The leaf node i itself (self-connection) - An ancestor of leaf node i in the trie</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>def _build_reachability_matrix(self):\n    \"\"\"Constructs a sparse reachability matrix for efficient weight propagation.\n\n    The matrix M is constructed such that M[i,j] = 1 if node j is either:\n    - The leaf node i itself (self-connection)\n    - An ancestor of leaf node i in the trie\n    \"\"\"\n    leaf_indices = self.token_id_to_leaf[:, 1]\n    parent = self._build_parent_map()\n\n    rows, cols = [], []\n    for i, node in enumerate(leaf_indices):\n        # self connections\n        rows.append(i)\n        cols.append(node)\n\n        current = node\n        while current in parent:  # Walk up to root\n            ancestor = parent[current]\n            rows.append(i)\n            cols.append(ancestor)\n            current = ancestor\n\n    self.src_indices = torch.tensor(rows, dtype=torch.long, device=self.device)\n    self.dst_indices = torch.tensor(cols, dtype=torch.long, device=self.device)\n\n    indices = torch.tensor([rows, cols], dtype=torch.long, device=self.device)\n    values = torch.ones(len(rows), device=self.device)\n\n    self.M = torch.sparse_coo_tensor(\n        indices, values, (len(leaf_indices), len(self.children))\n    ).to_sparse_csr()\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.ParallelTokenCharacterTrie.batch_weight_max","title":"<code>batch_weight_max(ws)</code>","text":"<p>Batch version of <code>weight_max</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>def batch_weight_max(self, ws):\n    \"\"\"Batch version of `weight_max`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n\n    # Get leaf weights\n    leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n    batch_size = leaf_weights.shape[0]\n\n    # Use scatter_reduce to propagate maximum values in parallel\n    result = torch.zeros((batch_size, len(self.children)), device=self.device)\n    result.scatter_reduce_(\n        dim=1,\n        index=self.dst_indices.expand(batch_size, -1),\n        src=leaf_weights[:, self.src_indices],\n        reduce=\"amax\",\n        include_self=False,\n    )\n\n    return result.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.ParallelTokenCharacterTrie.batch_weight_sum","title":"<code>batch_weight_sum(ws)</code>","text":"<p>Batch version of <code>weight_sum</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>def batch_weight_sum(self, ws):\n    \"\"\"Batch version of `weight_sum`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n    Returns:\n        numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    masses = torch.sparse.mm(ws[:, self.token_ids], self.M)\n    return masses.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.ParallelTokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>","text":"<p>Computes the max weights given the token weights.</p> <p>For each node in the trie, this computes the maximum weight among all leaf nodes (tokens) that are descendants of that node. This is efficiently implemented using parallel scatter_reduce operations on GPU.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (<code>len(self.decode)</code>,).</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>def weight_max(self, ws):\n    \"\"\"Computes the max weights given the token weights.\n\n    For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n    that are descendants of that node. This is efficiently implemented using parallel scatter_reduce\n    operations on GPU.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (`len(self.decode)`,).\n    \"\"\"\n    return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.ParallelTokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>","text":"<p>Computes weight sums given token weights.</p> <p>For each node in the trie, this computes the sum of weights of all leaf nodes (tokens) that are descendants of that node. This is efficiently implemented using sparse matrix multiplication with a pre-computed reachability matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie, shape (<code>len(self.decode)</code>,).</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>def weight_sum(self, ws):\n    \"\"\"Computes weight sums given token weights.\n\n    For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n    that are descendants of that node. This is efficiently implemented using sparse matrix multiplication\n    with a pre-computed reachability matrix.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n    Returns:\n        (numpy.ndarray): Summed weights for each node in the trie, shape (`len(self.decode)`,).\n    \"\"\"\n    return self.batch_weight_sum(self._preprocess_ws([ws]))[0]\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.TokenCharacterTrie","title":"<code>TokenCharacterTrie</code>","text":"<p>A trie data structure for efficient token-to-character mapping.</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>class TokenCharacterTrie:\n    \"\"\"A trie data structure for efficient token-to-character mapping.\"\"\"\n\n    def __init__(self, decode):\n        \"\"\"Initialize a `TokenCharacterTrie`.\n\n        Args:\n            decode (list): List representing the token vocabulary.\n                Each element of the list must be iterable.\n        \"\"\"\n        self.decode = decode\n        self.word2leaf = {}\n        self.children = [{}]  # First node is root\n        self.root = 0\n        self.token_id_to_leaf = []\n\n        for token_id, word in enumerate(self.decode):\n            curr = self.root\n            for letter in word:\n                if letter not in self.children[curr]:\n                    self.children[curr][letter] = len(self.children)\n                    self.children.append({})\n                curr = self.children[curr][letter]\n\n            self.children[curr][None] = last = len(self.children)\n            self.children.append({})\n            assert word not in self.word2leaf, (\n                \"Can't have duplicate words in vocabulary\"\n            )\n            self.word2leaf[word] = last\n\n            self.token_id_to_leaf.append((token_id, last))\n\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n        self.jump = List(\n            [np.array(sorted(x.values()), dtype=np.int32) for x in self.children]\n        )\n        self.ordering = np.array(list(self._order(self.root)), np.int32)\n\n        # Renumber the states of the trie so that they are named by a contiguous\n        # range of integers and those integers respect the are topologically\n        # ordering of the trie topology.  This improves the efficiency of the\n        # updating the trie as it improves memory locality.\n        ordering = {}\n        for i, x in enumerate(self._order_full(self.root)):\n            ordering[x] = i\n        self._rename(f=lambda x: ordering[x])\n\n        node2prefix = {self.root: []}\n        for x in reversed(range(len(self.children))):\n            for letter, y in self.children[x].items():\n                if letter is None:\n                    node2prefix[y] = node2prefix[x]\n                else:\n                    node2prefix[y] = node2prefix[x] + [letter]\n        self.node2prefix = node2prefix\n\n    def _rename(self, f):\n        \"\"\"Rename all node indices in the trie using the provided mapping function.\n\n        Args:\n            f (callable): Function that maps old node indices to new node indices\n        \"\"\"\n        N = len(self.children)\n\n        new_children = [{} for _ in range(N)]\n        nodes = range(N)\n\n        for x in nodes:\n            for letter, y in self.children[x].items():\n                new_children[f(x)][letter] = f(y)\n\n        self.root = f(self.root)\n        self.children = new_children\n        self.word2leaf = {w: f(x) for w, x in self.word2leaf.items()}\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n\n        self.token_id_to_leaf = np.array(\n            [(i, f(x)) for i, x in self.token_id_to_leaf], dtype=np.int32\n        )\n\n        self.ordering = np.array([f(x) for x in self.ordering])\n        self.jump = List(\n            [np.array(sorted(x.values()), dtype=np.int32) for x in new_children]\n        )\n\n    def _alloc_weights(self):\n        \"\"\"Allocate an array to store weight values for all nodes.\n\n        Returns:\n            np.ndarray: Zero-initialized array for storing weight values\n        \"\"\"\n        return np.zeros(len(self.children), dtype=np.float64)\n\n    def _preprocess_ws(self, ws):\n        \"\"\"Preprocess the weight vector to ensure it is a numpy array and on the correct device.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Weight vector\n        \"\"\"\n        if isinstance(ws, torch.Tensor):\n            if ws.device.type != \"cpu\":\n                ws = ws.cpu()\n            ws = ws.numpy()\n        return ws\n\n    def weight_sum(self, ws):\n        \"\"\"Compute weight sum for each node in the trie.\n\n        For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n        that are descendants of that node.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Summed weights for each node in the trie.\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        node_ws = self._alloc_weights()\n        _update_trie_numba_sum(\n            node_ws=node_ws,\n            ws=ws,\n            token_id_to_leaf=self.token_id_to_leaf,\n            jump=self.jump,\n            ordering=self.ordering,\n        )\n        return node_ws\n\n    def weight_max(self, ws):\n        \"\"\"Compute weight max for each node in the trie.\n\n        For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n        that are descendants of that node.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Weight max values for each node in the trie.\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        node_ws = self._alloc_weights()\n        _update_trie_numba_max(\n            node_ws=node_ws,\n            ws=ws,\n            token_id_to_leaf=self.token_id_to_leaf,\n            jump=self.jump,\n            ordering=self.ordering,\n        )\n        return node_ws\n\n    def batch_weight_sum(self, ws):\n        \"\"\"Batched equivalent of `weight_sum`.\n\n        Args:\n            ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Batch of weight values of `len(ws)` for each node in the trie\n        \"\"\"\n        return np.array([self.weight_sum(ws) for ws in ws])\n\n    def batch_weight_max(self, ws):\n        \"\"\"Batched equivalent of `weight_max`.\n\n        Args:\n            ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Batch of weight max values of `len(ws)` for each node in the trie\n        \"\"\"\n        return np.array([self.weight_max(ws) for ws in ws])\n\n    def _order(self, node):\n        \"\"\"Generate a topological ordering of nodes beneath the given node.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            int: Node indices in topological order\n        \"\"\"\n        for a in self.children[node]:\n            if a is None:\n                pass\n            else:\n                yield from self._order(self.children[node][a])\n        yield node\n\n    def _order_full(self, node):\n        \"\"\"Generate a complete topological ordering including all child nodes.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            (int): Node indices in complete topological order\n        \"\"\"\n        for a in self.children[node]:\n            yield from self._order_full(self.children[node][a])\n        yield node\n\n    def visualize(self, ws=None):\n        \"\"\"Visualize the trie structure using Graphviz.\n\n        Args:\n            ws (np.ndarray|None): Optional weight vector to display at each node.\n                                Should be of length `len(self.children)`.\n\n        Returns:\n            (graphviz.Digraph): The generated graph object\n        \"\"\"\n        try:\n            import graphviz\n        except ImportError:\n            raise ImportError(\"Please install graphviz: pip install graphviz\")\n\n        if ws is not None and len(ws) != len(self.children):\n            raise ValueError(\n                f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n            )\n\n        dot = graphviz.Digraph(comment=\"Token Character Trie\")\n        dot.attr(rankdir=\"LR\")\n\n        # Create a subgraph for the legend\n        with dot.subgraph(name=\"cluster_legend\") as legend:\n            legend.attr(label=\"Legend\", fontsize=\"10\")\n            legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n            # Example internal node\n            legend.node(\n                \"legend_internal\",\n                \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n                shape=\"circle\",\n            )\n\n            # Example leaf node\n            legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n            legend.edge(\n                \"legend_internal\",\n                \"legend_leaf\",\n                label=\"Token item\",\n                fontsize=\"10\",\n            )\n\n            # Align legend horizontally\n            legend.attr(rankdir=\"TB\")\n            legend.attr(rank=\"same\")\n\n        # Add the main trie nodes and edges\n        for node_id in range(len(self.children)):\n            prefix = self.node2prefix[node_id]\n\n            if ws is not None:\n                label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n            else:\n                label = f\"{node_id}\\n'{prefix}'\"\n\n            # Color nodes based on mass if provided\n            if ws is not None:\n                max_ws = ws.max()\n                if max_ws &gt; 0:\n                    intensity = int(255 * (1 - ws[node_id] / max_ws))\n                    color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n                else:\n                    color = \"#ffffff\"  # white for zero mass\n            else:\n                color = \"#ffffff\"  # default white\n\n            if node_id in self.leaf2word:\n                dot.node(\n                    str(node_id),\n                    label,\n                    shape=\"doublecircle\",\n                    style=\"filled\",\n                    fillcolor=color,\n                )\n            else:\n                dot.node(\n                    str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n                )\n\n        for node_id, children in enumerate(self.children):\n            for char, child_id in children.items():\n                if char is not None:\n                    edge_label = str(char)\n                else:\n                    edge_label = \"End-of-Token\"\n\n                dot.edge(str(node_id), str(child_id), label=edge_label)\n\n        return dot\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.TokenCharacterTrie.__init__","title":"<code>__init__(decode)</code>","text":"<p>Initialize a <code>TokenCharacterTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>decode</code> <code>list</code> <p>List representing the token vocabulary. Each element of the list must be iterable.</p> required Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def __init__(self, decode):\n    \"\"\"Initialize a `TokenCharacterTrie`.\n\n    Args:\n        decode (list): List representing the token vocabulary.\n            Each element of the list must be iterable.\n    \"\"\"\n    self.decode = decode\n    self.word2leaf = {}\n    self.children = [{}]  # First node is root\n    self.root = 0\n    self.token_id_to_leaf = []\n\n    for token_id, word in enumerate(self.decode):\n        curr = self.root\n        for letter in word:\n            if letter not in self.children[curr]:\n                self.children[curr][letter] = len(self.children)\n                self.children.append({})\n            curr = self.children[curr][letter]\n\n        self.children[curr][None] = last = len(self.children)\n        self.children.append({})\n        assert word not in self.word2leaf, (\n            \"Can't have duplicate words in vocabulary\"\n        )\n        self.word2leaf[word] = last\n\n        self.token_id_to_leaf.append((token_id, last))\n\n    self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n    self.jump = List(\n        [np.array(sorted(x.values()), dtype=np.int32) for x in self.children]\n    )\n    self.ordering = np.array(list(self._order(self.root)), np.int32)\n\n    # Renumber the states of the trie so that they are named by a contiguous\n    # range of integers and those integers respect the are topologically\n    # ordering of the trie topology.  This improves the efficiency of the\n    # updating the trie as it improves memory locality.\n    ordering = {}\n    for i, x in enumerate(self._order_full(self.root)):\n        ordering[x] = i\n    self._rename(f=lambda x: ordering[x])\n\n    node2prefix = {self.root: []}\n    for x in reversed(range(len(self.children))):\n        for letter, y in self.children[x].items():\n            if letter is None:\n                node2prefix[y] = node2prefix[x]\n            else:\n                node2prefix[y] = node2prefix[x] + [letter]\n    self.node2prefix = node2prefix\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.TokenCharacterTrie._alloc_weights","title":"<code>_alloc_weights()</code>","text":"<p>Allocate an array to store weight values for all nodes.</p> <p>Returns:</p> Type Description <p>np.ndarray: Zero-initialized array for storing weight values</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def _alloc_weights(self):\n    \"\"\"Allocate an array to store weight values for all nodes.\n\n    Returns:\n        np.ndarray: Zero-initialized array for storing weight values\n    \"\"\"\n    return np.zeros(len(self.children), dtype=np.float64)\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.TokenCharacterTrie._order","title":"<code>_order(node)</code>","text":"<p>Generate a topological ordering of nodes beneath the given node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>int</code> <p>Starting node index</p> required <p>Yields:</p> Name Type Description <code>int</code> <p>Node indices in topological order</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def _order(self, node):\n    \"\"\"Generate a topological ordering of nodes beneath the given node.\n\n    Args:\n        node (int): Starting node index\n\n    Yields:\n        int: Node indices in topological order\n    \"\"\"\n    for a in self.children[node]:\n        if a is None:\n            pass\n        else:\n            yield from self._order(self.children[node][a])\n    yield node\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.TokenCharacterTrie._order_full","title":"<code>_order_full(node)</code>","text":"<p>Generate a complete topological ordering including all child nodes.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>int</code> <p>Starting node index</p> required <p>Yields:</p> Type Description <code>int</code> <p>Node indices in complete topological order</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def _order_full(self, node):\n    \"\"\"Generate a complete topological ordering including all child nodes.\n\n    Args:\n        node (int): Starting node index\n\n    Yields:\n        (int): Node indices in complete topological order\n    \"\"\"\n    for a in self.children[node]:\n        yield from self._order_full(self.children[node][a])\n    yield node\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.TokenCharacterTrie._preprocess_ws","title":"<code>_preprocess_ws(ws)</code>","text":"<p>Preprocess the weight vector to ensure it is a numpy array and on the correct device.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor | ndarray</code> <p>Token weights over the vocabulary of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Weight vector</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def _preprocess_ws(self, ws):\n    \"\"\"Preprocess the weight vector to ensure it is a numpy array and on the correct device.\n\n    Args:\n        ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Weight vector\n    \"\"\"\n    if isinstance(ws, torch.Tensor):\n        if ws.device.type != \"cpu\":\n            ws = ws.cpu()\n        ws = ws.numpy()\n    return ws\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.TokenCharacterTrie._rename","title":"<code>_rename(f)</code>","text":"<p>Rename all node indices in the trie using the provided mapping function.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>callable</code> <p>Function that maps old node indices to new node indices</p> required Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def _rename(self, f):\n    \"\"\"Rename all node indices in the trie using the provided mapping function.\n\n    Args:\n        f (callable): Function that maps old node indices to new node indices\n    \"\"\"\n    N = len(self.children)\n\n    new_children = [{} for _ in range(N)]\n    nodes = range(N)\n\n    for x in nodes:\n        for letter, y in self.children[x].items():\n            new_children[f(x)][letter] = f(y)\n\n    self.root = f(self.root)\n    self.children = new_children\n    self.word2leaf = {w: f(x) for w, x in self.word2leaf.items()}\n    self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n\n    self.token_id_to_leaf = np.array(\n        [(i, f(x)) for i, x in self.token_id_to_leaf], dtype=np.int32\n    )\n\n    self.ordering = np.array([f(x) for x in self.ordering])\n    self.jump = List(\n        [np.array(sorted(x.values()), dtype=np.int32) for x in new_children]\n    )\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.TokenCharacterTrie.batch_weight_max","title":"<code>batch_weight_max(ws)</code>","text":"<p>Batched equivalent of <code>weight_max</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>list[Tensor | ndarray]</code> <p>Batch of token weights, each of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Batch of weight max values of <code>len(ws)</code> for each node in the trie</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def batch_weight_max(self, ws):\n    \"\"\"Batched equivalent of `weight_max`.\n\n    Args:\n        ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Batch of weight max values of `len(ws)` for each node in the trie\n    \"\"\"\n    return np.array([self.weight_max(ws) for ws in ws])\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.TokenCharacterTrie.batch_weight_sum","title":"<code>batch_weight_sum(ws)</code>","text":"<p>Batched equivalent of <code>weight_sum</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>list[Tensor | ndarray]</code> <p>Batch of token weights, each of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Batch of weight values of <code>len(ws)</code> for each node in the trie</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def batch_weight_sum(self, ws):\n    \"\"\"Batched equivalent of `weight_sum`.\n\n    Args:\n        ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Batch of weight values of `len(ws)` for each node in the trie\n    \"\"\"\n    return np.array([self.weight_sum(ws) for ws in ws])\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.TokenCharacterTrie.visualize","title":"<code>visualize(ws=None)</code>","text":"<p>Visualize the trie structure using Graphviz.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>ndarray | None</code> <p>Optional weight vector to display at each node.                 Should be of length <code>len(self.children)</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Digraph</code> <p>The generated graph object</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def visualize(self, ws=None):\n    \"\"\"Visualize the trie structure using Graphviz.\n\n    Args:\n        ws (np.ndarray|None): Optional weight vector to display at each node.\n                            Should be of length `len(self.children)`.\n\n    Returns:\n        (graphviz.Digraph): The generated graph object\n    \"\"\"\n    try:\n        import graphviz\n    except ImportError:\n        raise ImportError(\"Please install graphviz: pip install graphviz\")\n\n    if ws is not None and len(ws) != len(self.children):\n        raise ValueError(\n            f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n        )\n\n    dot = graphviz.Digraph(comment=\"Token Character Trie\")\n    dot.attr(rankdir=\"LR\")\n\n    # Create a subgraph for the legend\n    with dot.subgraph(name=\"cluster_legend\") as legend:\n        legend.attr(label=\"Legend\", fontsize=\"10\")\n        legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n        # Example internal node\n        legend.node(\n            \"legend_internal\",\n            \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n            shape=\"circle\",\n        )\n\n        # Example leaf node\n        legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n        legend.edge(\n            \"legend_internal\",\n            \"legend_leaf\",\n            label=\"Token item\",\n            fontsize=\"10\",\n        )\n\n        # Align legend horizontally\n        legend.attr(rankdir=\"TB\")\n        legend.attr(rank=\"same\")\n\n    # Add the main trie nodes and edges\n    for node_id in range(len(self.children)):\n        prefix = self.node2prefix[node_id]\n\n        if ws is not None:\n            label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n        else:\n            label = f\"{node_id}\\n'{prefix}'\"\n\n        # Color nodes based on mass if provided\n        if ws is not None:\n            max_ws = ws.max()\n            if max_ws &gt; 0:\n                intensity = int(255 * (1 - ws[node_id] / max_ws))\n                color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n            else:\n                color = \"#ffffff\"  # white for zero mass\n        else:\n            color = \"#ffffff\"  # default white\n\n        if node_id in self.leaf2word:\n            dot.node(\n                str(node_id),\n                label,\n                shape=\"doublecircle\",\n                style=\"filled\",\n                fillcolor=color,\n            )\n        else:\n            dot.node(\n                str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n            )\n\n    for node_id, children in enumerate(self.children):\n        for char, child_id in children.items():\n            if char is not None:\n                edge_label = str(char)\n            else:\n                edge_label = \"End-of-Token\"\n\n            dot.edge(str(node_id), str(child_id), label=edge_label)\n\n    return dot\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.TokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>","text":"<p>Compute weight max for each node in the trie.</p> <p>For each node in the trie, this computes the maximum weight among all leaf nodes (tokens) that are descendants of that node.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor | ndarray</code> <p>Token weights over the vocabulary of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Weight max values for each node in the trie.</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def weight_max(self, ws):\n    \"\"\"Compute weight max for each node in the trie.\n\n    For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n    that are descendants of that node.\n\n    Args:\n        ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Weight max values for each node in the trie.\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    node_ws = self._alloc_weights()\n    _update_trie_numba_max(\n        node_ws=node_ws,\n        ws=ws,\n        token_id_to_leaf=self.token_id_to_leaf,\n        jump=self.jump,\n        ordering=self.ordering,\n    )\n    return node_ws\n</code></pre>"},{"location":"reference/genlm_backend/trie/__init__/#genlm_backend.trie.TokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>","text":"<p>Compute weight sum for each node in the trie.</p> <p>For each node in the trie, this computes the sum of weights of all leaf nodes (tokens) that are descendants of that node.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor | ndarray</code> <p>Token weights over the vocabulary of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie.</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def weight_sum(self, ws):\n    \"\"\"Compute weight sum for each node in the trie.\n\n    For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n    that are descendants of that node.\n\n    Args:\n        ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Summed weights for each node in the trie.\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    node_ws = self._alloc_weights()\n    _update_trie_numba_sum(\n        node_ws=node_ws,\n        ws=ws,\n        token_id_to_leaf=self.token_id_to_leaf,\n        jump=self.jump,\n        ordering=self.ordering,\n    )\n    return node_ws\n</code></pre>"},{"location":"reference/genlm_backend/trie/async_impl/","title":"async_impl","text":""},{"location":"reference/genlm_backend/trie/async_impl/#genlm_backend.trie.async_impl.AsyncTokenCharacterTrie","title":"<code>AsyncTokenCharacterTrie</code>","text":"<p>An asynchronous wrapper for TokenCharacterTrie implementations that provides automatic request batching.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>class AsyncTokenCharacterTrie:\n    \"\"\"An asynchronous wrapper for TokenCharacterTrie implementations that provides automatic request batching.\"\"\"\n\n    def __init__(self, trie):\n        \"\"\"Initialize an `AsyncTokenCharacterTrie`.\n\n        Args:\n            trie (TokenCharacterTrie|ParallelTokenCharacterTrie): The underlying `TokenCharacterTrie` or `ParallelTokenCharacterTrie` instance\n        \"\"\"\n        self.trie = trie\n        self._queue = None\n        self._task = None\n\n    @classmethod\n    def from_vocab(cls, vocab, backend=\"parallel\", **kwargs):\n        \"\"\"Creates an `AsyncTokenCharacterTrie` from a vocabulary.\n\n        Args:\n            vocab (list): The vocabulary over which the trie will be defined.\n            backend (str, optional): The trie implementation to use - either 'sequential' or 'parallel'.\n                    Defaults to 'parallel' which uses GPU acceleration when available.\n            **kwargs: Additional arguments passed to the trie constructor\n\n        Returns:\n            (AsyncTokenCharacterTrie): The initialized asynchronous trie instance.\n        \"\"\"\n        if backend == \"sequential\":\n            trie = TokenCharacterTrie(decode=vocab, **kwargs)\n        elif backend == \"parallel\":\n            trie = ParallelTokenCharacterTrie(decode=vocab, **kwargs)\n        else:\n            raise ValueError(\n                f\"Unknown backend: {backend}. Must be one of ['sequential', 'parallel']\"\n            )\n        return cls(trie)\n\n    async def _queue_request(self, request, op):\n        if not self._task or self._task.done():\n            self.start()\n\n        future = asyncio.Future()\n        await self._queue.put((request, future, op))\n        return future\n\n    async def weight_sum(self, ws):\n        \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n        together.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n        Returns:\n            (np.ndarray): The calculated mass sums for the given distribution.\n        \"\"\"\n        future = await self._queue_request(ws, \"sum\")\n        result = await future\n        return result\n\n    async def weight_max(self, ws):\n        \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n        together.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n        Returns:\n            (np.ndarray): The calculated max weights for the given distribution.\n        \"\"\"\n        future = await self._queue_request(ws, \"max\")\n        result = await future\n        return result\n\n    def start(self):\n        \"\"\"Start the background processing task if not already running.\"\"\"\n        if not self._task or self._task.done():\n            self._queue = (\n                asyncio.Queue()\n            )  # Create a new queue so that it is bound to the current event loop\n            self._task = asyncio.create_task(self._background_loop())\n\n    def _do_weight_sums(self, batch_weights):\n        return self.trie.batch_weight_sum(batch_weights)\n\n    def _do_weight_maxs(self, batch_weights):\n        return self.trie.batch_weight_max(batch_weights)\n\n    async def _background_loop(self):\n        \"\"\"Background task that processes queued weight sum and max requests.\n\n        Continuously monitors the queue for new requests and processes them in batches\n        using the underlying trie implementation.\n\n        Raises:\n            Exception: If any error occurs during processing, it is propagated to all\n                      pending futures in the current batch.\n        \"\"\"\n        while True:\n            try:\n                op_groups = defaultdict(list)\n\n                request, future, op = await self._queue.get()\n                op_groups[op].append((request, future))\n\n                while not self._queue.empty():\n                    request, future, op = await self._queue.get()\n                    op_groups[op].append((request, future))\n\n                for op, group in op_groups.items():\n                    requests, futures = zip(*group)\n\n                    if op == \"sum\":\n                        logger.debug(f\"processing {len(requests)} sum requests\")\n                        results = self._do_weight_sums(requests)\n                    elif op == \"max\":\n                        logger.debug(f\"processing {len(requests)} max requests\")\n                        results = self._do_weight_maxs(requests)\n                    else:\n                        raise ValueError(f\"Unknown operation: {op}\")\n\n                    for future, result in zip(futures, results):\n                        future.set_result(result)\n\n            except Exception as e:\n                for group in op_groups.values():\n                    for _, future in group:\n                        if not future.done():\n                            future.set_exception(e)\n                raise\n\n    async def cleanup(self):\n        \"\"\"Async cleanup - preferred method\"\"\"\n        if self._task and not self._task.done():\n            self._task.cancel()\n            try:\n                await self._task\n            except asyncio.CancelledError:\n                pass\n            self._task = None\n\n    def shutdown(self):\n        \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n        if self._task is not None:\n            try:\n                self._task.cancel()\n            except RuntimeError:\n                # Ignore runtime errors that might occur if event loop is closed\n                pass\n            self._task = None\n\n    def __del__(self):\n        self.shutdown()\n</code></pre>"},{"location":"reference/genlm_backend/trie/async_impl/#genlm_backend.trie.async_impl.AsyncTokenCharacterTrie.__init__","title":"<code>__init__(trie)</code>","text":"<p>Initialize an <code>AsyncTokenCharacterTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>trie</code> <code>TokenCharacterTrie | ParallelTokenCharacterTrie</code> <p>The underlying <code>TokenCharacterTrie</code> or <code>ParallelTokenCharacterTrie</code> instance</p> required Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>def __init__(self, trie):\n    \"\"\"Initialize an `AsyncTokenCharacterTrie`.\n\n    Args:\n        trie (TokenCharacterTrie|ParallelTokenCharacterTrie): The underlying `TokenCharacterTrie` or `ParallelTokenCharacterTrie` instance\n    \"\"\"\n    self.trie = trie\n    self._queue = None\n    self._task = None\n</code></pre>"},{"location":"reference/genlm_backend/trie/async_impl/#genlm_backend.trie.async_impl.AsyncTokenCharacterTrie._background_loop","title":"<code>_background_loop()</code>  <code>async</code>","text":"<p>Background task that processes queued weight sum and max requests.</p> <p>Continuously monitors the queue for new requests and processes them in batches using the underlying trie implementation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during processing, it is propagated to all       pending futures in the current batch.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>async def _background_loop(self):\n    \"\"\"Background task that processes queued weight sum and max requests.\n\n    Continuously monitors the queue for new requests and processes them in batches\n    using the underlying trie implementation.\n\n    Raises:\n        Exception: If any error occurs during processing, it is propagated to all\n                  pending futures in the current batch.\n    \"\"\"\n    while True:\n        try:\n            op_groups = defaultdict(list)\n\n            request, future, op = await self._queue.get()\n            op_groups[op].append((request, future))\n\n            while not self._queue.empty():\n                request, future, op = await self._queue.get()\n                op_groups[op].append((request, future))\n\n            for op, group in op_groups.items():\n                requests, futures = zip(*group)\n\n                if op == \"sum\":\n                    logger.debug(f\"processing {len(requests)} sum requests\")\n                    results = self._do_weight_sums(requests)\n                elif op == \"max\":\n                    logger.debug(f\"processing {len(requests)} max requests\")\n                    results = self._do_weight_maxs(requests)\n                else:\n                    raise ValueError(f\"Unknown operation: {op}\")\n\n                for future, result in zip(futures, results):\n                    future.set_result(result)\n\n        except Exception as e:\n            for group in op_groups.values():\n                for _, future in group:\n                    if not future.done():\n                        future.set_exception(e)\n            raise\n</code></pre>"},{"location":"reference/genlm_backend/trie/async_impl/#genlm_backend.trie.async_impl.AsyncTokenCharacterTrie.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Async cleanup - preferred method</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Async cleanup - preferred method\"\"\"\n    if self._task and not self._task.done():\n        self._task.cancel()\n        try:\n            await self._task\n        except asyncio.CancelledError:\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm_backend/trie/async_impl/#genlm_backend.trie.async_impl.AsyncTokenCharacterTrie.from_vocab","title":"<code>from_vocab(vocab, backend='parallel', **kwargs)</code>  <code>classmethod</code>","text":"<p>Creates an <code>AsyncTokenCharacterTrie</code> from a vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>list</code> <p>The vocabulary over which the trie will be defined.</p> required <code>backend</code> <code>str</code> <p>The trie implementation to use - either 'sequential' or 'parallel'.     Defaults to 'parallel' which uses GPU acceleration when available.</p> <code>'parallel'</code> <code>**kwargs</code> <p>Additional arguments passed to the trie constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTokenCharacterTrie</code> <p>The initialized asynchronous trie instance.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>@classmethod\ndef from_vocab(cls, vocab, backend=\"parallel\", **kwargs):\n    \"\"\"Creates an `AsyncTokenCharacterTrie` from a vocabulary.\n\n    Args:\n        vocab (list): The vocabulary over which the trie will be defined.\n        backend (str, optional): The trie implementation to use - either 'sequential' or 'parallel'.\n                Defaults to 'parallel' which uses GPU acceleration when available.\n        **kwargs: Additional arguments passed to the trie constructor\n\n    Returns:\n        (AsyncTokenCharacterTrie): The initialized asynchronous trie instance.\n    \"\"\"\n    if backend == \"sequential\":\n        trie = TokenCharacterTrie(decode=vocab, **kwargs)\n    elif backend == \"parallel\":\n        trie = ParallelTokenCharacterTrie(decode=vocab, **kwargs)\n    else:\n        raise ValueError(\n            f\"Unknown backend: {backend}. Must be one of ['sequential', 'parallel']\"\n        )\n    return cls(trie)\n</code></pre>"},{"location":"reference/genlm_backend/trie/async_impl/#genlm_backend.trie.async_impl.AsyncTokenCharacterTrie.shutdown","title":"<code>shutdown()</code>","text":"<p>Stop the background processing task and cleanup resources.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>def shutdown(self):\n    \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n    if self._task is not None:\n        try:\n            self._task.cancel()\n        except RuntimeError:\n            # Ignore runtime errors that might occur if event loop is closed\n            pass\n        self._task = None\n</code></pre>"},{"location":"reference/genlm_backend/trie/async_impl/#genlm_backend.trie.async_impl.AsyncTokenCharacterTrie.start","title":"<code>start()</code>","text":"<p>Start the background processing task if not already running.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>def start(self):\n    \"\"\"Start the background processing task if not already running.\"\"\"\n    if not self._task or self._task.done():\n        self._queue = (\n            asyncio.Queue()\n        )  # Create a new queue so that it is bound to the current event loop\n        self._task = asyncio.create_task(self._background_loop())\n</code></pre>"},{"location":"reference/genlm_backend/trie/async_impl/#genlm_backend.trie.async_impl.AsyncTokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>  <code>async</code>","text":"<p>Queue a <code>weight_max</code> request. Multiple concurrent calls will be automatically batched together.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated max weights for the given distribution.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>async def weight_max(self, ws):\n    \"\"\"Queue a `weight_max` request. Multiple concurrent calls will be automatically batched\n    together.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n    Returns:\n        (np.ndarray): The calculated max weights for the given distribution.\n    \"\"\"\n    future = await self._queue_request(ws, \"max\")\n    result = await future\n    return result\n</code></pre>"},{"location":"reference/genlm_backend/trie/async_impl/#genlm_backend.trie.async_impl.AsyncTokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>  <code>async</code>","text":"<p>Queue a <code>weight_sum</code> request. Multiple concurrent calls will be automatically batched together.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.trie.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The calculated mass sums for the given distribution.</p> Source code in <code>genlm_backend/trie/async_impl.py</code> <pre><code>async def weight_sum(self, ws):\n    \"\"\"Queue a `weight_sum` request. Multiple concurrent calls will be automatically batched\n    together.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.trie.decode)`,).\n\n    Returns:\n        (np.ndarray): The calculated mass sums for the given distribution.\n    \"\"\"\n    future = await self._queue_request(ws, \"sum\")\n    result = await future\n    return result\n</code></pre>"},{"location":"reference/genlm_backend/trie/base/","title":"base","text":""},{"location":"reference/genlm_backend/trie/base/#genlm_backend.trie.base.TokenCharacterTrie","title":"<code>TokenCharacterTrie</code>","text":"<p>A trie data structure for efficient token-to-character mapping.</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>class TokenCharacterTrie:\n    \"\"\"A trie data structure for efficient token-to-character mapping.\"\"\"\n\n    def __init__(self, decode):\n        \"\"\"Initialize a `TokenCharacterTrie`.\n\n        Args:\n            decode (list): List representing the token vocabulary.\n                Each element of the list must be iterable.\n        \"\"\"\n        self.decode = decode\n        self.word2leaf = {}\n        self.children = [{}]  # First node is root\n        self.root = 0\n        self.token_id_to_leaf = []\n\n        for token_id, word in enumerate(self.decode):\n            curr = self.root\n            for letter in word:\n                if letter not in self.children[curr]:\n                    self.children[curr][letter] = len(self.children)\n                    self.children.append({})\n                curr = self.children[curr][letter]\n\n            self.children[curr][None] = last = len(self.children)\n            self.children.append({})\n            assert word not in self.word2leaf, (\n                \"Can't have duplicate words in vocabulary\"\n            )\n            self.word2leaf[word] = last\n\n            self.token_id_to_leaf.append((token_id, last))\n\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n        self.jump = List(\n            [np.array(sorted(x.values()), dtype=np.int32) for x in self.children]\n        )\n        self.ordering = np.array(list(self._order(self.root)), np.int32)\n\n        # Renumber the states of the trie so that they are named by a contiguous\n        # range of integers and those integers respect the are topologically\n        # ordering of the trie topology.  This improves the efficiency of the\n        # updating the trie as it improves memory locality.\n        ordering = {}\n        for i, x in enumerate(self._order_full(self.root)):\n            ordering[x] = i\n        self._rename(f=lambda x: ordering[x])\n\n        node2prefix = {self.root: []}\n        for x in reversed(range(len(self.children))):\n            for letter, y in self.children[x].items():\n                if letter is None:\n                    node2prefix[y] = node2prefix[x]\n                else:\n                    node2prefix[y] = node2prefix[x] + [letter]\n        self.node2prefix = node2prefix\n\n    def _rename(self, f):\n        \"\"\"Rename all node indices in the trie using the provided mapping function.\n\n        Args:\n            f (callable): Function that maps old node indices to new node indices\n        \"\"\"\n        N = len(self.children)\n\n        new_children = [{} for _ in range(N)]\n        nodes = range(N)\n\n        for x in nodes:\n            for letter, y in self.children[x].items():\n                new_children[f(x)][letter] = f(y)\n\n        self.root = f(self.root)\n        self.children = new_children\n        self.word2leaf = {w: f(x) for w, x in self.word2leaf.items()}\n        self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n\n        self.token_id_to_leaf = np.array(\n            [(i, f(x)) for i, x in self.token_id_to_leaf], dtype=np.int32\n        )\n\n        self.ordering = np.array([f(x) for x in self.ordering])\n        self.jump = List(\n            [np.array(sorted(x.values()), dtype=np.int32) for x in new_children]\n        )\n\n    def _alloc_weights(self):\n        \"\"\"Allocate an array to store weight values for all nodes.\n\n        Returns:\n            np.ndarray: Zero-initialized array for storing weight values\n        \"\"\"\n        return np.zeros(len(self.children), dtype=np.float64)\n\n    def _preprocess_ws(self, ws):\n        \"\"\"Preprocess the weight vector to ensure it is a numpy array and on the correct device.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Weight vector\n        \"\"\"\n        if isinstance(ws, torch.Tensor):\n            if ws.device.type != \"cpu\":\n                ws = ws.cpu()\n            ws = ws.numpy()\n        return ws\n\n    def weight_sum(self, ws):\n        \"\"\"Compute weight sum for each node in the trie.\n\n        For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n        that are descendants of that node.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Summed weights for each node in the trie.\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        node_ws = self._alloc_weights()\n        _update_trie_numba_sum(\n            node_ws=node_ws,\n            ws=ws,\n            token_id_to_leaf=self.token_id_to_leaf,\n            jump=self.jump,\n            ordering=self.ordering,\n        )\n        return node_ws\n\n    def weight_max(self, ws):\n        \"\"\"Compute weight max for each node in the trie.\n\n        For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n        that are descendants of that node.\n\n        Args:\n            ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Weight max values for each node in the trie.\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        node_ws = self._alloc_weights()\n        _update_trie_numba_max(\n            node_ws=node_ws,\n            ws=ws,\n            token_id_to_leaf=self.token_id_to_leaf,\n            jump=self.jump,\n            ordering=self.ordering,\n        )\n        return node_ws\n\n    def batch_weight_sum(self, ws):\n        \"\"\"Batched equivalent of `weight_sum`.\n\n        Args:\n            ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Batch of weight values of `len(ws)` for each node in the trie\n        \"\"\"\n        return np.array([self.weight_sum(ws) for ws in ws])\n\n    def batch_weight_max(self, ws):\n        \"\"\"Batched equivalent of `weight_max`.\n\n        Args:\n            ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n        Returns:\n            (np.ndarray): Batch of weight max values of `len(ws)` for each node in the trie\n        \"\"\"\n        return np.array([self.weight_max(ws) for ws in ws])\n\n    def _order(self, node):\n        \"\"\"Generate a topological ordering of nodes beneath the given node.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            int: Node indices in topological order\n        \"\"\"\n        for a in self.children[node]:\n            if a is None:\n                pass\n            else:\n                yield from self._order(self.children[node][a])\n        yield node\n\n    def _order_full(self, node):\n        \"\"\"Generate a complete topological ordering including all child nodes.\n\n        Args:\n            node (int): Starting node index\n\n        Yields:\n            (int): Node indices in complete topological order\n        \"\"\"\n        for a in self.children[node]:\n            yield from self._order_full(self.children[node][a])\n        yield node\n\n    def visualize(self, ws=None):\n        \"\"\"Visualize the trie structure using Graphviz.\n\n        Args:\n            ws (np.ndarray|None): Optional weight vector to display at each node.\n                                Should be of length `len(self.children)`.\n\n        Returns:\n            (graphviz.Digraph): The generated graph object\n        \"\"\"\n        try:\n            import graphviz\n        except ImportError:\n            raise ImportError(\"Please install graphviz: pip install graphviz\")\n\n        if ws is not None and len(ws) != len(self.children):\n            raise ValueError(\n                f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n            )\n\n        dot = graphviz.Digraph(comment=\"Token Character Trie\")\n        dot.attr(rankdir=\"LR\")\n\n        # Create a subgraph for the legend\n        with dot.subgraph(name=\"cluster_legend\") as legend:\n            legend.attr(label=\"Legend\", fontsize=\"10\")\n            legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n            # Example internal node\n            legend.node(\n                \"legend_internal\",\n                \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n                shape=\"circle\",\n            )\n\n            # Example leaf node\n            legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n            legend.edge(\n                \"legend_internal\",\n                \"legend_leaf\",\n                label=\"Token item\",\n                fontsize=\"10\",\n            )\n\n            # Align legend horizontally\n            legend.attr(rankdir=\"TB\")\n            legend.attr(rank=\"same\")\n\n        # Add the main trie nodes and edges\n        for node_id in range(len(self.children)):\n            prefix = self.node2prefix[node_id]\n\n            if ws is not None:\n                label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n            else:\n                label = f\"{node_id}\\n'{prefix}'\"\n\n            # Color nodes based on mass if provided\n            if ws is not None:\n                max_ws = ws.max()\n                if max_ws &gt; 0:\n                    intensity = int(255 * (1 - ws[node_id] / max_ws))\n                    color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n                else:\n                    color = \"#ffffff\"  # white for zero mass\n            else:\n                color = \"#ffffff\"  # default white\n\n            if node_id in self.leaf2word:\n                dot.node(\n                    str(node_id),\n                    label,\n                    shape=\"doublecircle\",\n                    style=\"filled\",\n                    fillcolor=color,\n                )\n            else:\n                dot.node(\n                    str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n                )\n\n        for node_id, children in enumerate(self.children):\n            for char, child_id in children.items():\n                if char is not None:\n                    edge_label = str(char)\n                else:\n                    edge_label = \"End-of-Token\"\n\n                dot.edge(str(node_id), str(child_id), label=edge_label)\n\n        return dot\n</code></pre>"},{"location":"reference/genlm_backend/trie/base/#genlm_backend.trie.base.TokenCharacterTrie.__init__","title":"<code>__init__(decode)</code>","text":"<p>Initialize a <code>TokenCharacterTrie</code>.</p> <p>Parameters:</p> Name Type Description Default <code>decode</code> <code>list</code> <p>List representing the token vocabulary. Each element of the list must be iterable.</p> required Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def __init__(self, decode):\n    \"\"\"Initialize a `TokenCharacterTrie`.\n\n    Args:\n        decode (list): List representing the token vocabulary.\n            Each element of the list must be iterable.\n    \"\"\"\n    self.decode = decode\n    self.word2leaf = {}\n    self.children = [{}]  # First node is root\n    self.root = 0\n    self.token_id_to_leaf = []\n\n    for token_id, word in enumerate(self.decode):\n        curr = self.root\n        for letter in word:\n            if letter not in self.children[curr]:\n                self.children[curr][letter] = len(self.children)\n                self.children.append({})\n            curr = self.children[curr][letter]\n\n        self.children[curr][None] = last = len(self.children)\n        self.children.append({})\n        assert word not in self.word2leaf, (\n            \"Can't have duplicate words in vocabulary\"\n        )\n        self.word2leaf[word] = last\n\n        self.token_id_to_leaf.append((token_id, last))\n\n    self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n    self.jump = List(\n        [np.array(sorted(x.values()), dtype=np.int32) for x in self.children]\n    )\n    self.ordering = np.array(list(self._order(self.root)), np.int32)\n\n    # Renumber the states of the trie so that they are named by a contiguous\n    # range of integers and those integers respect the are topologically\n    # ordering of the trie topology.  This improves the efficiency of the\n    # updating the trie as it improves memory locality.\n    ordering = {}\n    for i, x in enumerate(self._order_full(self.root)):\n        ordering[x] = i\n    self._rename(f=lambda x: ordering[x])\n\n    node2prefix = {self.root: []}\n    for x in reversed(range(len(self.children))):\n        for letter, y in self.children[x].items():\n            if letter is None:\n                node2prefix[y] = node2prefix[x]\n            else:\n                node2prefix[y] = node2prefix[x] + [letter]\n    self.node2prefix = node2prefix\n</code></pre>"},{"location":"reference/genlm_backend/trie/base/#genlm_backend.trie.base.TokenCharacterTrie._alloc_weights","title":"<code>_alloc_weights()</code>","text":"<p>Allocate an array to store weight values for all nodes.</p> <p>Returns:</p> Type Description <p>np.ndarray: Zero-initialized array for storing weight values</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def _alloc_weights(self):\n    \"\"\"Allocate an array to store weight values for all nodes.\n\n    Returns:\n        np.ndarray: Zero-initialized array for storing weight values\n    \"\"\"\n    return np.zeros(len(self.children), dtype=np.float64)\n</code></pre>"},{"location":"reference/genlm_backend/trie/base/#genlm_backend.trie.base.TokenCharacterTrie._order","title":"<code>_order(node)</code>","text":"<p>Generate a topological ordering of nodes beneath the given node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>int</code> <p>Starting node index</p> required <p>Yields:</p> Name Type Description <code>int</code> <p>Node indices in topological order</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def _order(self, node):\n    \"\"\"Generate a topological ordering of nodes beneath the given node.\n\n    Args:\n        node (int): Starting node index\n\n    Yields:\n        int: Node indices in topological order\n    \"\"\"\n    for a in self.children[node]:\n        if a is None:\n            pass\n        else:\n            yield from self._order(self.children[node][a])\n    yield node\n</code></pre>"},{"location":"reference/genlm_backend/trie/base/#genlm_backend.trie.base.TokenCharacterTrie._order_full","title":"<code>_order_full(node)</code>","text":"<p>Generate a complete topological ordering including all child nodes.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>int</code> <p>Starting node index</p> required <p>Yields:</p> Type Description <code>int</code> <p>Node indices in complete topological order</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def _order_full(self, node):\n    \"\"\"Generate a complete topological ordering including all child nodes.\n\n    Args:\n        node (int): Starting node index\n\n    Yields:\n        (int): Node indices in complete topological order\n    \"\"\"\n    for a in self.children[node]:\n        yield from self._order_full(self.children[node][a])\n    yield node\n</code></pre>"},{"location":"reference/genlm_backend/trie/base/#genlm_backend.trie.base.TokenCharacterTrie._preprocess_ws","title":"<code>_preprocess_ws(ws)</code>","text":"<p>Preprocess the weight vector to ensure it is a numpy array and on the correct device.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor | ndarray</code> <p>Token weights over the vocabulary of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Weight vector</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def _preprocess_ws(self, ws):\n    \"\"\"Preprocess the weight vector to ensure it is a numpy array and on the correct device.\n\n    Args:\n        ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Weight vector\n    \"\"\"\n    if isinstance(ws, torch.Tensor):\n        if ws.device.type != \"cpu\":\n            ws = ws.cpu()\n        ws = ws.numpy()\n    return ws\n</code></pre>"},{"location":"reference/genlm_backend/trie/base/#genlm_backend.trie.base.TokenCharacterTrie._rename","title":"<code>_rename(f)</code>","text":"<p>Rename all node indices in the trie using the provided mapping function.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>callable</code> <p>Function that maps old node indices to new node indices</p> required Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def _rename(self, f):\n    \"\"\"Rename all node indices in the trie using the provided mapping function.\n\n    Args:\n        f (callable): Function that maps old node indices to new node indices\n    \"\"\"\n    N = len(self.children)\n\n    new_children = [{} for _ in range(N)]\n    nodes = range(N)\n\n    for x in nodes:\n        for letter, y in self.children[x].items():\n            new_children[f(x)][letter] = f(y)\n\n    self.root = f(self.root)\n    self.children = new_children\n    self.word2leaf = {w: f(x) for w, x in self.word2leaf.items()}\n    self.leaf2word = dict(zip(self.word2leaf.values(), self.word2leaf.keys()))\n\n    self.token_id_to_leaf = np.array(\n        [(i, f(x)) for i, x in self.token_id_to_leaf], dtype=np.int32\n    )\n\n    self.ordering = np.array([f(x) for x in self.ordering])\n    self.jump = List(\n        [np.array(sorted(x.values()), dtype=np.int32) for x in new_children]\n    )\n</code></pre>"},{"location":"reference/genlm_backend/trie/base/#genlm_backend.trie.base.TokenCharacterTrie.batch_weight_max","title":"<code>batch_weight_max(ws)</code>","text":"<p>Batched equivalent of <code>weight_max</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>list[Tensor | ndarray]</code> <p>Batch of token weights, each of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Batch of weight max values of <code>len(ws)</code> for each node in the trie</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def batch_weight_max(self, ws):\n    \"\"\"Batched equivalent of `weight_max`.\n\n    Args:\n        ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Batch of weight max values of `len(ws)` for each node in the trie\n    \"\"\"\n    return np.array([self.weight_max(ws) for ws in ws])\n</code></pre>"},{"location":"reference/genlm_backend/trie/base/#genlm_backend.trie.base.TokenCharacterTrie.batch_weight_sum","title":"<code>batch_weight_sum(ws)</code>","text":"<p>Batched equivalent of <code>weight_sum</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>list[Tensor | ndarray]</code> <p>Batch of token weights, each of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Batch of weight values of <code>len(ws)</code> for each node in the trie</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def batch_weight_sum(self, ws):\n    \"\"\"Batched equivalent of `weight_sum`.\n\n    Args:\n        ws (list[torch.Tensor|np.ndarray]): Batch of token weights, each of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Batch of weight values of `len(ws)` for each node in the trie\n    \"\"\"\n    return np.array([self.weight_sum(ws) for ws in ws])\n</code></pre>"},{"location":"reference/genlm_backend/trie/base/#genlm_backend.trie.base.TokenCharacterTrie.visualize","title":"<code>visualize(ws=None)</code>","text":"<p>Visualize the trie structure using Graphviz.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>ndarray | None</code> <p>Optional weight vector to display at each node.                 Should be of length <code>len(self.children)</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Digraph</code> <p>The generated graph object</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def visualize(self, ws=None):\n    \"\"\"Visualize the trie structure using Graphviz.\n\n    Args:\n        ws (np.ndarray|None): Optional weight vector to display at each node.\n                            Should be of length `len(self.children)`.\n\n    Returns:\n        (graphviz.Digraph): The generated graph object\n    \"\"\"\n    try:\n        import graphviz\n    except ImportError:\n        raise ImportError(\"Please install graphviz: pip install graphviz\")\n\n    if ws is not None and len(ws) != len(self.children):\n        raise ValueError(\n            f\"Weight vector length ({len(ws)}) must match number of nodes ({len(self.children)})\"\n        )\n\n    dot = graphviz.Digraph(comment=\"Token Character Trie\")\n    dot.attr(rankdir=\"LR\")\n\n    # Create a subgraph for the legend\n    with dot.subgraph(name=\"cluster_legend\") as legend:\n        legend.attr(label=\"Legend\", fontsize=\"10\")\n        legend.attr(\"node\", fontsize=\"7\", width=\"0.1\", height=\"0.1\")\n\n        # Example internal node\n        legend.node(\n            \"legend_internal\",\n            \"Internal Node ID\\n'Prefix'\\nWeight (if provided)\",\n            shape=\"circle\",\n        )\n\n        # Example leaf node\n        legend.node(\"legend_leaf\", \"Complete Token\", shape=\"doublecircle\")\n\n        legend.edge(\n            \"legend_internal\",\n            \"legend_leaf\",\n            label=\"Token item\",\n            fontsize=\"10\",\n        )\n\n        # Align legend horizontally\n        legend.attr(rankdir=\"TB\")\n        legend.attr(rank=\"same\")\n\n    # Add the main trie nodes and edges\n    for node_id in range(len(self.children)):\n        prefix = self.node2prefix[node_id]\n\n        if ws is not None:\n            label = f\"{node_id}\\n'{prefix}'\\n{ws[node_id]:.4f}\"\n        else:\n            label = f\"{node_id}\\n'{prefix}'\"\n\n        # Color nodes based on mass if provided\n        if ws is not None:\n            max_ws = ws.max()\n            if max_ws &gt; 0:\n                intensity = int(255 * (1 - ws[node_id] / max_ws))\n                color = f\"#{intensity:02x}{255:02x}{intensity:02x}\"\n            else:\n                color = \"#ffffff\"  # white for zero mass\n        else:\n            color = \"#ffffff\"  # default white\n\n        if node_id in self.leaf2word:\n            dot.node(\n                str(node_id),\n                label,\n                shape=\"doublecircle\",\n                style=\"filled\",\n                fillcolor=color,\n            )\n        else:\n            dot.node(\n                str(node_id), label, shape=\"circle\", style=\"filled\", fillcolor=color\n            )\n\n    for node_id, children in enumerate(self.children):\n        for char, child_id in children.items():\n            if char is not None:\n                edge_label = str(char)\n            else:\n                edge_label = \"End-of-Token\"\n\n            dot.edge(str(node_id), str(child_id), label=edge_label)\n\n    return dot\n</code></pre>"},{"location":"reference/genlm_backend/trie/base/#genlm_backend.trie.base.TokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>","text":"<p>Compute weight max for each node in the trie.</p> <p>For each node in the trie, this computes the maximum weight among all leaf nodes (tokens) that are descendants of that node.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor | ndarray</code> <p>Token weights over the vocabulary of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Weight max values for each node in the trie.</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def weight_max(self, ws):\n    \"\"\"Compute weight max for each node in the trie.\n\n    For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n    that are descendants of that node.\n\n    Args:\n        ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Weight max values for each node in the trie.\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    node_ws = self._alloc_weights()\n    _update_trie_numba_max(\n        node_ws=node_ws,\n        ws=ws,\n        token_id_to_leaf=self.token_id_to_leaf,\n        jump=self.jump,\n        ordering=self.ordering,\n    )\n    return node_ws\n</code></pre>"},{"location":"reference/genlm_backend/trie/base/#genlm_backend.trie.base.TokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>","text":"<p>Compute weight sum for each node in the trie.</p> <p>For each node in the trie, this computes the sum of weights of all leaf nodes (tokens) that are descendants of that node.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor | ndarray</code> <p>Token weights over the vocabulary of shape <code>(len(self.decode),)</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie.</p> Source code in <code>genlm_backend/trie/base.py</code> <pre><code>def weight_sum(self, ws):\n    \"\"\"Compute weight sum for each node in the trie.\n\n    For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n    that are descendants of that node.\n\n    Args:\n        ws (torch.Tensor|np.ndarray): Token weights over the vocabulary of shape `(len(self.decode),)`\n\n    Returns:\n        (np.ndarray): Summed weights for each node in the trie.\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    node_ws = self._alloc_weights()\n    _update_trie_numba_sum(\n        node_ws=node_ws,\n        ws=ws,\n        token_id_to_leaf=self.token_id_to_leaf,\n        jump=self.jump,\n        ordering=self.ordering,\n    )\n    return node_ws\n</code></pre>"},{"location":"reference/genlm_backend/trie/parallel/","title":"parallel","text":""},{"location":"reference/genlm_backend/trie/parallel/#genlm_backend.trie.parallel.ParallelTokenCharacterTrie","title":"<code>ParallelTokenCharacterTrie</code>","text":"<p>               Bases: <code>TokenCharacterTrie</code></p> <p>A GPU-optimized version of <code>TokenCharacterTrie</code> that performs weight sum and max operations in parallel.</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>class ParallelTokenCharacterTrie(TokenCharacterTrie):\n    \"\"\"A GPU-optimized version of `TokenCharacterTrie` that performs weight sum and max operations in parallel.\"\"\"\n\n    def __init__(self, decode, device=None, **kwargs):\n        super().__init__(decode, **kwargs)\n\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        if self.device not in [\"cpu\", \"cuda\"]:\n            raise ValueError(f\"Invalid device: {device}. Must be 'cpu', 'cuda' or None\")\n\n        self._build_reachability_matrix()\n        self.token_ids = torch.tensor(\n            self.token_id_to_leaf[:, 0], dtype=torch.long, device=self.device\n        )\n\n    def _build_parent_map(self):\n        \"\"\"Builds a mapping from each node to its parent node in the trie.\n\n        Returns:\n            (dict): A dictionary where keys are child nodes and values are their parent nodes.\n        \"\"\"\n        parent = {}\n        for node in range(len(self.children)):\n            for child in self.jump[node]:\n                parent[child] = node\n        return parent\n\n    def _build_reachability_matrix(self):\n        \"\"\"Constructs a sparse reachability matrix for efficient weight propagation.\n\n        The matrix M is constructed such that M[i,j] = 1 if node j is either:\n        - The leaf node i itself (self-connection)\n        - An ancestor of leaf node i in the trie\n        \"\"\"\n        leaf_indices = self.token_id_to_leaf[:, 1]\n        parent = self._build_parent_map()\n\n        rows, cols = [], []\n        for i, node in enumerate(leaf_indices):\n            # self connections\n            rows.append(i)\n            cols.append(node)\n\n            current = node\n            while current in parent:  # Walk up to root\n                ancestor = parent[current]\n                rows.append(i)\n                cols.append(ancestor)\n                current = ancestor\n\n        self.src_indices = torch.tensor(rows, dtype=torch.long, device=self.device)\n        self.dst_indices = torch.tensor(cols, dtype=torch.long, device=self.device)\n\n        indices = torch.tensor([rows, cols], dtype=torch.long, device=self.device)\n        values = torch.ones(len(rows), device=self.device)\n\n        self.M = torch.sparse_coo_tensor(\n            indices, values, (len(leaf_indices), len(self.children))\n        ).to_sparse_csr()\n\n    def _preprocess_ws(self, batch_ws):\n        processed_batch_ws = []\n        for ws in batch_ws:\n            if not isinstance(ws, torch.Tensor):\n                ws = torch.tensor(ws, device=self.device, dtype=torch.float32)\n            elif ws.device != self.device or ws.dtype != torch.float32:\n                ws = ws.to(device=self.device, dtype=torch.float32)\n            assert ws.shape[0] == len(self.decode), [ws.shape[0], len(self.decode)]\n            processed_batch_ws.append(ws)\n        return torch.stack(processed_batch_ws)\n\n    def weight_sum(self, ws):\n        \"\"\"Computes weight sums given token weights.\n\n        For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n        that are descendants of that node. This is efficiently implemented using sparse matrix multiplication\n        with a pre-computed reachability matrix.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n        Returns:\n            (numpy.ndarray): Summed weights for each node in the trie, shape (`len(self.decode)`,).\n        \"\"\"\n        return self.batch_weight_sum(self._preprocess_ws([ws]))[0]\n\n    def batch_weight_sum(self, ws):\n        \"\"\"Batch version of `weight_sum`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n        Returns:\n            numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n        masses = torch.sparse.mm(ws[:, self.token_ids], self.M)\n        return masses.cpu().numpy()\n\n    def weight_max(self, ws):\n        \"\"\"Computes the max weights given the token weights.\n\n        For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n        that are descendants of that node. This is efficiently implemented using parallel scatter_reduce\n        operations on GPU.\n\n        Args:\n            ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (`len(self.decode)`,).\n        \"\"\"\n        return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n\n    def batch_weight_max(self, ws):\n        \"\"\"Batch version of `weight_max`.\n\n        Args:\n            ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n        Returns:\n            (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n        \"\"\"\n        ws = self._preprocess_ws(ws)\n\n        # Get leaf weights\n        leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n        batch_size = leaf_weights.shape[0]\n\n        # Use scatter_reduce to propagate maximum values in parallel\n        result = torch.zeros((batch_size, len(self.children)), device=self.device)\n        result.scatter_reduce_(\n            dim=1,\n            index=self.dst_indices.expand(batch_size, -1),\n            src=leaf_weights[:, self.src_indices],\n            reduce=\"amax\",\n            include_self=False,\n        )\n\n        return result.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm_backend/trie/parallel/#genlm_backend.trie.parallel.ParallelTokenCharacterTrie._build_parent_map","title":"<code>_build_parent_map()</code>","text":"<p>Builds a mapping from each node to its parent node in the trie.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where keys are child nodes and values are their parent nodes.</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>def _build_parent_map(self):\n    \"\"\"Builds a mapping from each node to its parent node in the trie.\n\n    Returns:\n        (dict): A dictionary where keys are child nodes and values are their parent nodes.\n    \"\"\"\n    parent = {}\n    for node in range(len(self.children)):\n        for child in self.jump[node]:\n            parent[child] = node\n    return parent\n</code></pre>"},{"location":"reference/genlm_backend/trie/parallel/#genlm_backend.trie.parallel.ParallelTokenCharacterTrie._build_reachability_matrix","title":"<code>_build_reachability_matrix()</code>","text":"<p>Constructs a sparse reachability matrix for efficient weight propagation.</p> <p>The matrix M is constructed such that M[i,j] = 1 if node j is either: - The leaf node i itself (self-connection) - An ancestor of leaf node i in the trie</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>def _build_reachability_matrix(self):\n    \"\"\"Constructs a sparse reachability matrix for efficient weight propagation.\n\n    The matrix M is constructed such that M[i,j] = 1 if node j is either:\n    - The leaf node i itself (self-connection)\n    - An ancestor of leaf node i in the trie\n    \"\"\"\n    leaf_indices = self.token_id_to_leaf[:, 1]\n    parent = self._build_parent_map()\n\n    rows, cols = [], []\n    for i, node in enumerate(leaf_indices):\n        # self connections\n        rows.append(i)\n        cols.append(node)\n\n        current = node\n        while current in parent:  # Walk up to root\n            ancestor = parent[current]\n            rows.append(i)\n            cols.append(ancestor)\n            current = ancestor\n\n    self.src_indices = torch.tensor(rows, dtype=torch.long, device=self.device)\n    self.dst_indices = torch.tensor(cols, dtype=torch.long, device=self.device)\n\n    indices = torch.tensor([rows, cols], dtype=torch.long, device=self.device)\n    values = torch.ones(len(rows), device=self.device)\n\n    self.M = torch.sparse_coo_tensor(\n        indices, values, (len(leaf_indices), len(self.children))\n    ).to_sparse_csr()\n</code></pre>"},{"location":"reference/genlm_backend/trie/parallel/#genlm_backend.trie.parallel.ParallelTokenCharacterTrie.batch_weight_max","title":"<code>batch_weight_max(ws)</code>","text":"<p>Batch version of <code>weight_max</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>def batch_weight_max(self, ws):\n    \"\"\"Batch version of `weight_max`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n\n    # Get leaf weights\n    leaf_weights = ws[:, self.token_ids]  # shape: (batch_size \u00d7 num_leafs)\n    batch_size = leaf_weights.shape[0]\n\n    # Use scatter_reduce to propagate maximum values in parallel\n    result = torch.zeros((batch_size, len(self.children)), device=self.device)\n    result.scatter_reduce_(\n        dim=1,\n        index=self.dst_indices.expand(batch_size, -1),\n        src=leaf_weights[:, self.src_indices],\n        reduce=\"amax\",\n        include_self=False,\n    )\n\n    return result.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm_backend/trie/parallel/#genlm_backend.trie.parallel.ParallelTokenCharacterTrie.batch_weight_sum","title":"<code>batch_weight_sum(ws)</code>","text":"<p>Batch version of <code>weight_sum</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Batch of token weights, shape (batch_size \u00d7 <code>len(self.decode)</code>).</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>def batch_weight_sum(self, ws):\n    \"\"\"Batch version of `weight_sum`.\n\n    Args:\n        ws (torch.Tensor): Batch of token weights, shape (batch_size \u00d7 `len(self.decode)`).\n\n    Returns:\n        numpy.ndarray: Summed weights for each node in the trie, shape (batch_size \u00d7 num_nodes).\n    \"\"\"\n    ws = self._preprocess_ws(ws)\n    masses = torch.sparse.mm(ws[:, self.token_ids], self.M)\n    return masses.cpu().numpy()\n</code></pre>"},{"location":"reference/genlm_backend/trie/parallel/#genlm_backend.trie.parallel.ParallelTokenCharacterTrie.weight_max","title":"<code>weight_max(ws)</code>","text":"<p>Computes the max weights given the token weights.</p> <p>For each node in the trie, this computes the maximum weight among all leaf nodes (tokens) that are descendants of that node. This is efficiently implemented using parallel scatter_reduce operations on GPU.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Maximum weights for each node in the trie, shape (<code>len(self.decode)</code>,).</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>def weight_max(self, ws):\n    \"\"\"Computes the max weights given the token weights.\n\n    For each node in the trie, this computes the maximum weight among all leaf nodes (tokens)\n    that are descendants of that node. This is efficiently implemented using parallel scatter_reduce\n    operations on GPU.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n    Returns:\n        (numpy.ndarray): Maximum weights for each node in the trie, shape (`len(self.decode)`,).\n    \"\"\"\n    return self.batch_weight_max(self._preprocess_ws([ws]))[0]\n</code></pre>"},{"location":"reference/genlm_backend/trie/parallel/#genlm_backend.trie.parallel.ParallelTokenCharacterTrie.weight_sum","title":"<code>weight_sum(ws)</code>","text":"<p>Computes weight sums given token weights.</p> <p>For each node in the trie, this computes the sum of weights of all leaf nodes (tokens) that are descendants of that node. This is efficiently implemented using sparse matrix multiplication with a pre-computed reachability matrix.</p> <p>Parameters:</p> Name Type Description Default <code>ws</code> <code>Tensor</code> <p>Token weights, shape (<code>len(self.decode)</code>,).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Summed weights for each node in the trie, shape (<code>len(self.decode)</code>,).</p> Source code in <code>genlm_backend/trie/parallel.py</code> <pre><code>def weight_sum(self, ws):\n    \"\"\"Computes weight sums given token weights.\n\n    For each node in the trie, this computes the sum of weights of all leaf nodes (tokens)\n    that are descendants of that node. This is efficiently implemented using sparse matrix multiplication\n    with a pre-computed reachability matrix.\n\n    Args:\n        ws (torch.Tensor): Token weights, shape (`len(self.decode)`,).\n\n    Returns:\n        (numpy.ndarray): Summed weights for each node in the trie, shape (`len(self.decode)`,).\n    \"\"\"\n    return self.batch_weight_sum(self._preprocess_ws([ws]))[0]\n</code></pre>"}]}